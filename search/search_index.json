{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"configuration/","text":"Configuration \u00b6 Core Properties \u00b6 Name Type Default Description dgs.graphql.path String \"/graphql\" Path to the endpoint that will serve GraphQL requests. dgs.graphql.introspection.enabled Boolean true Enables graphql introspection functionality. dgs.graphql.schema-json.enabled Boolean true Enables schema-json endpoint functionality. dgs.graphql.schema-json.path String \"/schema.json\" Path to the schema-json endpoint without trailing slash. dgs.graphql.schema-locations [String] \"classpath*:schema/**/*.graphql*\" Location of the GraphQL schema files. dgs.graphql.graphiql.enabled Boolean true Enables GraphiQL functionality. dgs.graphql.graphiql.path String \"/graphiql\" Path to the GraphiQL endpoint without trailing slash. Example: Configure the location of the GraphQL Schema Files \u00b6 You can configure the location of your GraphQL schema files via the dgs.graphql.schema-locations property. By default it will attempt to load them from the schema directory via the Classpath , i.e. using classpath*:schema/**/*.graphql* . Let's go through an example, let's say you want to change the directory from being schema to graphql-schemas , you would define your configuration as follows: dgs : graphql : schema-locations : - classpath*:graphql-schemas/**/*.graphql* Now, if you want to add additional locations to look for the GraphQL Schema files you an add them to the list. For example, let's say we want to also look into your graphql-experimental-schemas : dgs : graphql : schema-locations : - classpath*:graphql-schemas/**/*.graphql* - classpath*:graphql-experimental-schemas/**/*.graphql* DGS Extended Scalars: graphql-dgs-extended-scalars \u00b6 Name Type Default Description dgs.graphql.extensions.scalars.enabled Boolean true Registered the Scalar Extensions available in graphql-java-extended-scalars for the DGS Framework. dgs.graphql.extensions.scalars.chars.enabled Boolean true Will register the GraphQLChar extension. dgs.graphql.extensions.scalars.numbers.enabled Boolean true Will register all numeric scalar extensions such as PositiveInt, NegativeInt, etc. dgs.graphql.extensions.scalars.objects.enabled Boolean true Will register the Object, Json, Url, and Locale scalar extensions. dgs.graphql.extensions.scalars.time-dates.enabled Boolean true Will register the DateTime, Date, and Time scalar extensions. DGS Extended Validation: graphql-dgs-extended-validation \u00b6 Name Type Default Description dgs.graphql.extensions.validation.enabled Boolean true Registered the Validation Schema Directive Extensions available in graphql-java-extended-validation for the DGS Framework. DGS Metrics: graphql-dgs-spring-boot-micrometer \u00b6 Name Type Default Description management.metrics.dgs-graphql.enabled Boolean true Enables DGS' GraphQL metrics, via micrometer. management.metrics.dgs-graphql.instrumentation.enabled Boolean true Enables DGS' GraphQL's base instrumentation; emits gql.query , gql.resolver , and gql.error meters. management.metrics.dgs-graphql.data-loader-instrumentation.enabled Boolean true Enables DGS' instrumentation for DataLoader; emits gql.dataLoader meters. management.metrics.dgs-graphql.tag-customizers.outcome.enabled Boolean true Enables DGS' GraphQL Outcome tag customizer. This adds an OUTCOME tag that is ether SUCCESS or ERROR to the emitted gql meters. management.metrics.dgs-graphql.query-signature.enabled Boolean true Enables DGS' QuerySignatureRepository ; if available metrics will be tagged with the gql.query.sig.hash . management.metrics.dgs-graphql.query-signature.caching.enabled Boolean true Enables DGS' QuerySignature caching; if set to false the signature will always be calculated on each request. management.metrics.dgs-graphql.tags.limiter.limit Integer 100 The limit that will apply for this tag. The interpretation of this limit depends on the cardinality limiter itself. management.metrics.dgs-graphql.autotime.percentiles [Double] [] DGS Micrometer Timers percentiles, e.g. [0.95, 0.99, 0.50] . 1 management.metrics.dgs-graphql.autotime.percentiles-histogram Boolean false Enables publishing percentile histograms for the DGS Micrometer Timers. 1 Hint You can configure percentiles, and enable percentile histograms, directly via the per-meter customizations available out of the box in Spring Boot. For example, to enable percentile histograms for all gql.* meters you can set the following property: management.metrics.distribution.percentiles-histogram.gql=true For more information please refer to Spring Boot's Per Meter Properties . Spring Boot's Per Meter Properties can be used to configure percentiles, and histograms, out of the box. \u21a9 \u21a9","title":"Configuration"},{"location":"configuration/#configuration","text":"","title":"Configuration"},{"location":"configuration/#core-properties","text":"Name Type Default Description dgs.graphql.path String \"/graphql\" Path to the endpoint that will serve GraphQL requests. dgs.graphql.introspection.enabled Boolean true Enables graphql introspection functionality. dgs.graphql.schema-json.enabled Boolean true Enables schema-json endpoint functionality. dgs.graphql.schema-json.path String \"/schema.json\" Path to the schema-json endpoint without trailing slash. dgs.graphql.schema-locations [String] \"classpath*:schema/**/*.graphql*\" Location of the GraphQL schema files. dgs.graphql.graphiql.enabled Boolean true Enables GraphiQL functionality. dgs.graphql.graphiql.path String \"/graphiql\" Path to the GraphiQL endpoint without trailing slash.","title":"Core Properties"},{"location":"configuration/#example-configure-the-location-of-the-graphql-schema-files","text":"You can configure the location of your GraphQL schema files via the dgs.graphql.schema-locations property. By default it will attempt to load them from the schema directory via the Classpath , i.e. using classpath*:schema/**/*.graphql* . Let's go through an example, let's say you want to change the directory from being schema to graphql-schemas , you would define your configuration as follows: dgs : graphql : schema-locations : - classpath*:graphql-schemas/**/*.graphql* Now, if you want to add additional locations to look for the GraphQL Schema files you an add them to the list. For example, let's say we want to also look into your graphql-experimental-schemas : dgs : graphql : schema-locations : - classpath*:graphql-schemas/**/*.graphql* - classpath*:graphql-experimental-schemas/**/*.graphql*","title":"Example: Configure the location of the GraphQL Schema Files"},{"location":"configuration/#dgs-extended-scalars-graphql-dgs-extended-scalars","text":"Name Type Default Description dgs.graphql.extensions.scalars.enabled Boolean true Registered the Scalar Extensions available in graphql-java-extended-scalars for the DGS Framework. dgs.graphql.extensions.scalars.chars.enabled Boolean true Will register the GraphQLChar extension. dgs.graphql.extensions.scalars.numbers.enabled Boolean true Will register all numeric scalar extensions such as PositiveInt, NegativeInt, etc. dgs.graphql.extensions.scalars.objects.enabled Boolean true Will register the Object, Json, Url, and Locale scalar extensions. dgs.graphql.extensions.scalars.time-dates.enabled Boolean true Will register the DateTime, Date, and Time scalar extensions.","title":"DGS Extended Scalars: graphql-dgs-extended-scalars"},{"location":"configuration/#dgs-extended-validation-graphql-dgs-extended-validation","text":"Name Type Default Description dgs.graphql.extensions.validation.enabled Boolean true Registered the Validation Schema Directive Extensions available in graphql-java-extended-validation for the DGS Framework.","title":"DGS Extended Validation: graphql-dgs-extended-validation"},{"location":"configuration/#dgs-metrics-graphql-dgs-spring-boot-micrometer","text":"Name Type Default Description management.metrics.dgs-graphql.enabled Boolean true Enables DGS' GraphQL metrics, via micrometer. management.metrics.dgs-graphql.instrumentation.enabled Boolean true Enables DGS' GraphQL's base instrumentation; emits gql.query , gql.resolver , and gql.error meters. management.metrics.dgs-graphql.data-loader-instrumentation.enabled Boolean true Enables DGS' instrumentation for DataLoader; emits gql.dataLoader meters. management.metrics.dgs-graphql.tag-customizers.outcome.enabled Boolean true Enables DGS' GraphQL Outcome tag customizer. This adds an OUTCOME tag that is ether SUCCESS or ERROR to the emitted gql meters. management.metrics.dgs-graphql.query-signature.enabled Boolean true Enables DGS' QuerySignatureRepository ; if available metrics will be tagged with the gql.query.sig.hash . management.metrics.dgs-graphql.query-signature.caching.enabled Boolean true Enables DGS' QuerySignature caching; if set to false the signature will always be calculated on each request. management.metrics.dgs-graphql.tags.limiter.limit Integer 100 The limit that will apply for this tag. The interpretation of this limit depends on the cardinality limiter itself. management.metrics.dgs-graphql.autotime.percentiles [Double] [] DGS Micrometer Timers percentiles, e.g. [0.95, 0.99, 0.50] . 1 management.metrics.dgs-graphql.autotime.percentiles-histogram Boolean false Enables publishing percentile histograms for the DGS Micrometer Timers. 1 Hint You can configure percentiles, and enable percentile histograms, directly via the per-meter customizations available out of the box in Spring Boot. For example, to enable percentile histograms for all gql.* meters you can set the following property: management.metrics.distribution.percentiles-histogram.gql=true For more information please refer to Spring Boot's Per Meter Properties . Spring Boot's Per Meter Properties can be used to configure percentiles, and histograms, out of the box. \u21a9 \u21a9","title":"DGS Metrics: graphql-dgs-spring-boot-micrometer"},{"location":"data-loaders/","text":"Data loaders solve the N+1 problem while loading data. The N+1 Problem Explained \u00b6 Say you query for a list of movies, and each movie includes some data about the director of the movie. Also assume that the Movie and Director entities are owned by two different services. In a na\u00efve implementation, to load 50 movies, you would have to call the Director service 50 times: once for each movie. This totals 51 queries: one query to get the list of movies, and 50 queries to get the director data for each movie. This obviously wouldn\u2019t perform very well. It would be much more efficient to create a list of directors to load, and load all of them at once in a single call. This first of all must be supported by the Director service , because that service needs to provide a way to load a list of Directors. The data fetchers in the Movie service need to be smart as well, to take care of batching the requests to the Directors service. This is where data loaders come in. What If My Service Doesn\u2019t Support Loading in Batches? \u00b6 What if (in this example) DirectorServiceClient doesn\u2019t provide a method to load a list of directors? What if it only provides a method to load a single director by ID? The same problem applies to REST services as well: what if there\u2019s no endpoint to load multiple directors? Similarly, to load from a database directly, you must write a query to load multiple directors. If such methods are unavailable, the providing service needs to fix this! Implementing a Data Loader \u00b6 The easiest way for you to register a data loader is for you to create a class that implements the org.dataloader.BatchLoader or org.dataloader.MappedBatchLoader interface. This interface is parameterized; it requires a type for the key and result of the BatchLoader . For example, if the identifiers for a Director are of type String , you could have a org.dataloader.BatchLoader<String, Director> . You must annotate the class with @DgsDataLoader so that the framework will register the data loader it represents. In order to implement the BatchLoader interface you must implement a CompletionStage<List<V>> load(List<K> keys) method. The following example is a data loader that loads data from an imaginary Director service: package com.netflix.graphql.dgs.example.dataLoader ; import com.netflix.graphql.dgs.DgsDataLoader ; import org.dataloader.BatchLoader ; import java.util.List ; import java.util.concurrent.CompletableFuture ; import java.util.concurrent.CompletionStage ; import java.util.stream.Collectors ; @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements BatchLoader < String , Director > { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < List < Director >> load ( List < String > keys ) { return CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } } The data loader is responsible for loading data for a given list of keys. In this example, it just passes on the list of keys to the backend that owns Director (this could for example be a [gRPC] service). However, you might also write such a service so that it loads data from a database. Although this example registers a data loader, nobody will use that data loader until you implement a data fetcher that uses it. Implementing a Data Loader With Try \u00b6 If you want to handle exceptions during fetching of partial results, you can return a list of Try objects from the loader. The query result will contain partial results for the successful calls and an error for the exception case. package com.netflix.graphql.dgs.example.dataLoader ; import com.netflix.graphql.dgs.DgsDataLoader ; import org.dataloader.BatchLoader ; import java.util.List ; import java.util.concurrent.CompletableFuture ; import java.util.concurrent.CompletionStage ; import java.util.stream.Collectors ; @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements BatchLoader < String , Try < Director >> { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < List < Try < Director >>> load ( List < String > keys ) { return CompletableFuture . supplyAsync (() -> keys . stream () . map ( key -> Try . tryCall (() -> directorServiceClient . loadDirectors ( keys ))) . collect ( Collectors . toList ())); } } } Provide as Lambda \u00b6 Because BatchLoader is a functional interface (an interface with only a single method), you can also provide it as a lambda expression. Technically this is exactly the same as providing a class; it\u2019s really just another way of writing it: @DgsComponent public class ExampleBatchLoaderFromField { @DgsDataLoader ( name = \"directors\" ) public BatchLoader < String , Director > directorBatchLoader = keys -> CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } MappedBatchLoader \u00b6 The BatchLoader interface creates a List of values for a List of keys. You can also use the MappedBatchLoader which creates a Map of key/values for a Set of values. The latter is a better choice if you do not expect all keys to have a value. You register a MappedBatchLoader in the same way as you register a BatchLoader : @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements MappedBatchLoader < String , Director > { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < Map < String , Director >> load ( Set < String > keys ) { return CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } } Using a Data Loader \u00b6 The following is an example of a data fetcher that uses a data loader: @DgsComponent public class DirectorDataFetcher { @DgsData ( parentType = \"Movie\" , field = \"director\" ) public CompletableFuture < Director > director ( DataFetchingEnvironment dfe ) { DataLoader < String , Director > dataLoader = dfe . getDataLoader ( \"directors\" ); String id = dfe . getArgument ( \"directorId\" ); return dataLoader . load ( id ); } } The code above is mostly just a regular data fetcher. However, instead of actually loading the data from another service or database, it uses the data loader to do so. You can retrieve a data loader from the DataFetchingEnvironment with its getDataLoader() method. This requires you to pass the name of the data loader as a string. The other change to the data fetcher is that it returns a CompletableFuture instead of the actual type you\u2019re loading. This enables the framework to do work asynchronously, and is a requirement for batching . Using the DgsDataFetchingEnvironment \u00b6 You can also get the data loader in a type-safe way by using our custom DgsDataFetchingEnvironment , which is an enhanced version of DataFetchingEnvironment in graphql-java , and provides getDataLoader() using the classname. @DgsComponent public class DirectorDataFetcher { @DgsData ( parentType = \"Movie\" , field = \"director\" ) public CompletableFuture < Director > director ( DgsDataFetchingEnvironment dfe ) { DataLoader < String , Director > dataLoader = dfe . getDataLoader ( DirectorsDataLoader . class ); String id = dfe . getArgument ( \"directorId\" ); return dataLoader . load ( id ); } } The same works if you have @DgsDataLoader defined as a lambda instead of on a class as shown here . If you have multiple @DgsDataLoader lambdas defined as fields in the same class, you won't be able to use this feature. It is recommended that you use getDataLoader() with the loader name passed as a string in such cases. Note that there is no logic present about how batching works exactly; this is all handled by the framework! The framework will recognize that many directors need to be loaded when many movies are loaded, batch up all the calls to the data loader, and call the data loader with a list of IDs instead of a single ID. The data loader implemented above already knows how to handle a list of IDs, and that way it avoids the N+1 problem. Using Spring Features such as SecurityContextHolder inside a CompletableFuture \u00b6 When you write async data fetchers, the code will run on worker threads. Spring internally stores some context, for example to make the SecurityContextHolder work, on the thread context however. This context wouldn\u2019t be available inside code running on a different thread, which makes fetching the Principal associated with the request not work. Spring Boot has a solution for this: it manages a thread pool that does have this context carry over. You can inject this solution in the following way: @Autowired @DefaultExecutor private Executor executor ; You must pass in the executor as the second argument of the supplyAsync() method which is typically used to make data fetchers asynchronous. @DgsData ( parentType = \"Query\" , field = \"list_things\" ) public CompletableFuture < List < Thing >> resolve ( DataFetchingEnvironment environment ) { return CompletableFuture . supplyAsync (() -> { return myService . getThings (); }, executor ); Caching \u00b6 Batching is the most important aspect of preventing N+1 problems. Data loaders also support caching, however. If the same key is loaded multiple times, it will only be loaded once. For example, if a list of movies is loaded , and some movies are directed by the same director, the director data will only be retrieved once. Caching is Disabled by Default in DGS 1 Version 1 of the DGS framework disables caching by default, but you can switch it on in the @DgsDataLoader annotation: @DgsDataLoader ( name = \"directors\" , caching = true ) class DirectorsBatchLoader implements BatchLoader < String , Director > {} You do not need to make this change in version 2 of the DGS framework, because that version enables caching by default. Batch Size \u00b6 Sometimes it\u2019s possible to load multiple items at once, but to a certain limit. When loading from a database for example, an IN query could be used , but maybe with the limitation of a maximum number of IDs to provide. The @DgsDataLoader has a maxBatchSize annotation that you can use to configure this behavior. By default it does not specify a maximum batch size. Data Loader Scope \u00b6 Data loaders are wired up to only span a single request. This is what most use cases require. Spanning multiple requests can introduce difficult-to-debug issues.","title":"Async Data Fetching"},{"location":"data-loaders/#the-n1-problem-explained","text":"Say you query for a list of movies, and each movie includes some data about the director of the movie. Also assume that the Movie and Director entities are owned by two different services. In a na\u00efve implementation, to load 50 movies, you would have to call the Director service 50 times: once for each movie. This totals 51 queries: one query to get the list of movies, and 50 queries to get the director data for each movie. This obviously wouldn\u2019t perform very well. It would be much more efficient to create a list of directors to load, and load all of them at once in a single call. This first of all must be supported by the Director service , because that service needs to provide a way to load a list of Directors. The data fetchers in the Movie service need to be smart as well, to take care of batching the requests to the Directors service. This is where data loaders come in.","title":"The N+1 Problem Explained"},{"location":"data-loaders/#what-if-my-service-doesnt-support-loading-in-batches","text":"What if (in this example) DirectorServiceClient doesn\u2019t provide a method to load a list of directors? What if it only provides a method to load a single director by ID? The same problem applies to REST services as well: what if there\u2019s no endpoint to load multiple directors? Similarly, to load from a database directly, you must write a query to load multiple directors. If such methods are unavailable, the providing service needs to fix this!","title":"What If My Service Doesn\u2019t Support Loading in Batches?"},{"location":"data-loaders/#implementing-a-data-loader","text":"The easiest way for you to register a data loader is for you to create a class that implements the org.dataloader.BatchLoader or org.dataloader.MappedBatchLoader interface. This interface is parameterized; it requires a type for the key and result of the BatchLoader . For example, if the identifiers for a Director are of type String , you could have a org.dataloader.BatchLoader<String, Director> . You must annotate the class with @DgsDataLoader so that the framework will register the data loader it represents. In order to implement the BatchLoader interface you must implement a CompletionStage<List<V>> load(List<K> keys) method. The following example is a data loader that loads data from an imaginary Director service: package com.netflix.graphql.dgs.example.dataLoader ; import com.netflix.graphql.dgs.DgsDataLoader ; import org.dataloader.BatchLoader ; import java.util.List ; import java.util.concurrent.CompletableFuture ; import java.util.concurrent.CompletionStage ; import java.util.stream.Collectors ; @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements BatchLoader < String , Director > { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < List < Director >> load ( List < String > keys ) { return CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } } The data loader is responsible for loading data for a given list of keys. In this example, it just passes on the list of keys to the backend that owns Director (this could for example be a [gRPC] service). However, you might also write such a service so that it loads data from a database. Although this example registers a data loader, nobody will use that data loader until you implement a data fetcher that uses it.","title":"Implementing a Data Loader"},{"location":"data-loaders/#implementing-a-data-loader-with-try","text":"If you want to handle exceptions during fetching of partial results, you can return a list of Try objects from the loader. The query result will contain partial results for the successful calls and an error for the exception case. package com.netflix.graphql.dgs.example.dataLoader ; import com.netflix.graphql.dgs.DgsDataLoader ; import org.dataloader.BatchLoader ; import java.util.List ; import java.util.concurrent.CompletableFuture ; import java.util.concurrent.CompletionStage ; import java.util.stream.Collectors ; @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements BatchLoader < String , Try < Director >> { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < List < Try < Director >>> load ( List < String > keys ) { return CompletableFuture . supplyAsync (() -> keys . stream () . map ( key -> Try . tryCall (() -> directorServiceClient . loadDirectors ( keys ))) . collect ( Collectors . toList ())); } } }","title":"Implementing a Data Loader With Try"},{"location":"data-loaders/#provide-as-lambda","text":"Because BatchLoader is a functional interface (an interface with only a single method), you can also provide it as a lambda expression. Technically this is exactly the same as providing a class; it\u2019s really just another way of writing it: @DgsComponent public class ExampleBatchLoaderFromField { @DgsDataLoader ( name = \"directors\" ) public BatchLoader < String , Director > directorBatchLoader = keys -> CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); }","title":"Provide as Lambda"},{"location":"data-loaders/#mappedbatchloader","text":"The BatchLoader interface creates a List of values for a List of keys. You can also use the MappedBatchLoader which creates a Map of key/values for a Set of values. The latter is a better choice if you do not expect all keys to have a value. You register a MappedBatchLoader in the same way as you register a BatchLoader : @DgsDataLoader ( name = \"directors\" ) public class DirectorsDataLoader implements MappedBatchLoader < String , Director > { @Autowired DirectorServiceClient directorServiceClient ; @Override public CompletionStage < Map < String , Director >> load ( Set < String > keys ) { return CompletableFuture . supplyAsync (() -> directorServiceClient . loadDirectors ( keys )); } }","title":"MappedBatchLoader"},{"location":"data-loaders/#using-a-data-loader","text":"The following is an example of a data fetcher that uses a data loader: @DgsComponent public class DirectorDataFetcher { @DgsData ( parentType = \"Movie\" , field = \"director\" ) public CompletableFuture < Director > director ( DataFetchingEnvironment dfe ) { DataLoader < String , Director > dataLoader = dfe . getDataLoader ( \"directors\" ); String id = dfe . getArgument ( \"directorId\" ); return dataLoader . load ( id ); } } The code above is mostly just a regular data fetcher. However, instead of actually loading the data from another service or database, it uses the data loader to do so. You can retrieve a data loader from the DataFetchingEnvironment with its getDataLoader() method. This requires you to pass the name of the data loader as a string. The other change to the data fetcher is that it returns a CompletableFuture instead of the actual type you\u2019re loading. This enables the framework to do work asynchronously, and is a requirement for batching .","title":"Using a Data Loader"},{"location":"data-loaders/#using-the-dgsdatafetchingenvironment","text":"You can also get the data loader in a type-safe way by using our custom DgsDataFetchingEnvironment , which is an enhanced version of DataFetchingEnvironment in graphql-java , and provides getDataLoader() using the classname. @DgsComponent public class DirectorDataFetcher { @DgsData ( parentType = \"Movie\" , field = \"director\" ) public CompletableFuture < Director > director ( DgsDataFetchingEnvironment dfe ) { DataLoader < String , Director > dataLoader = dfe . getDataLoader ( DirectorsDataLoader . class ); String id = dfe . getArgument ( \"directorId\" ); return dataLoader . load ( id ); } } The same works if you have @DgsDataLoader defined as a lambda instead of on a class as shown here . If you have multiple @DgsDataLoader lambdas defined as fields in the same class, you won't be able to use this feature. It is recommended that you use getDataLoader() with the loader name passed as a string in such cases. Note that there is no logic present about how batching works exactly; this is all handled by the framework! The framework will recognize that many directors need to be loaded when many movies are loaded, batch up all the calls to the data loader, and call the data loader with a list of IDs instead of a single ID. The data loader implemented above already knows how to handle a list of IDs, and that way it avoids the N+1 problem.","title":"Using the DgsDataFetchingEnvironment"},{"location":"data-loaders/#using-spring-features-such-as-securitycontextholder-inside-a-completablefuture","text":"When you write async data fetchers, the code will run on worker threads. Spring internally stores some context, for example to make the SecurityContextHolder work, on the thread context however. This context wouldn\u2019t be available inside code running on a different thread, which makes fetching the Principal associated with the request not work. Spring Boot has a solution for this: it manages a thread pool that does have this context carry over. You can inject this solution in the following way: @Autowired @DefaultExecutor private Executor executor ; You must pass in the executor as the second argument of the supplyAsync() method which is typically used to make data fetchers asynchronous. @DgsData ( parentType = \"Query\" , field = \"list_things\" ) public CompletableFuture < List < Thing >> resolve ( DataFetchingEnvironment environment ) { return CompletableFuture . supplyAsync (() -> { return myService . getThings (); }, executor );","title":"Using Spring Features such as SecurityContextHolder inside a CompletableFuture"},{"location":"data-loaders/#caching","text":"Batching is the most important aspect of preventing N+1 problems. Data loaders also support caching, however. If the same key is loaded multiple times, it will only be loaded once. For example, if a list of movies is loaded , and some movies are directed by the same director, the director data will only be retrieved once. Caching is Disabled by Default in DGS 1 Version 1 of the DGS framework disables caching by default, but you can switch it on in the @DgsDataLoader annotation: @DgsDataLoader ( name = \"directors\" , caching = true ) class DirectorsBatchLoader implements BatchLoader < String , Director > {} You do not need to make this change in version 2 of the DGS framework, because that version enables caching by default.","title":"Caching"},{"location":"data-loaders/#batch-size","text":"Sometimes it\u2019s possible to load multiple items at once, but to a certain limit. When loading from a database for example, an IN query could be used , but maybe with the limitation of a maximum number of IDs to provide. The @DgsDataLoader has a maxBatchSize annotation that you can use to configure this behavior. By default it does not specify a maximum batch size.","title":"Batch Size"},{"location":"data-loaders/#data-loader-scope","text":"Data loaders are wired up to only span a single request. This is what most use cases require. Spanning multiple requests can introduce difficult-to-debug issues.","title":"Data Loader Scope"},{"location":"datafetching/","text":"In the getting started guide we introduced the @DgsData annotation, which you use to create a data fetcher. In this section, we look at some of the finer details of datafetchers. The @DgsData, @DgsQuery, @DgsMutation and @DgsSubscription Annotations \u00b6 You use the @DgsData annotation on a Java/Kotlin method to make that method a datafetcher. The method must be in a @DgsComponent class. The @DgsData annotation has two parameters: Parameter Description parentType This is the type that contains the field. field The field that the datafetcher is responsible for For example, we have the following schema. type Query { shows: [Show] } type Show { title: String actors: [Actor] } We can implement this schema with a single datafetcher. @DgsComponent public class ShowDataFetcher { @DgsData ( parentType = \"Query\" , field = \"shows\" ) public List < Show > shows () { //Load shows from a database and return the list of Show objects return shows ; } } If the field parameter is not set, the method name will be used as the field name. The @DgsQuery , @DgsMutation and @DgsSubscription annotations are shorthands to define datafetchers on the Query , Mutation and Subscription types. The following definitions are all equivalent. @DgsData ( parentType = \"Query\" , field = \"shows\" ) public List < Show > shows () { .... } // The \"field\" argument is omitted. It uses the method name as the field name. @DgsData ( parentType = \"Query\" ) public List < Show > shows () { .... } // The parentType is \"Query\", the field name is derived from the method name. @DgsQuery public List < Show > shows () { .... } // The parentType is \"Query\", the field name is explicitly specified. @DgsQuery ( field = \"shows\" ) public List < Show > shows () { .... } Notice how a datafetcher can return complex objects or lists of objects. You don't have to create a separate datafetcher for each field. The framework will take care of only returning the fields that are specified in the query. For example, if a user queries: { shows { title } } Although we're returning Show objects, which in the example contains both a title and an actors field, the actors field gets stripped off before the response gets sent back. Child Datafetchers \u00b6 The previous example assumed that you could load a list of Show objects from your database with a single query. It wouldn't matter which fields the user included in the GraphQL query; the cost of loading the shows would be the same. What if there is an extra cost to specific fields? For example, what if loading actors for a show requires an extra query? It would be wasteful to run the extra query to load actors if the actors field doesn't get returned to the user. In such scenarios, it's better to create a separate datafetcher for the expensive field. @DgsQuery public List < Show > shows () { //Load shows, which doesn't include \"actors\" return shows ; } @DgsData ( parentType = \"Show\" , field = \"actors\" ) public List < Actor > actors ( DgsDataFetchingEnvironment dfe ) { Show show = dfe . getSource (); actorsService . forShow ( show . getId ()); return actors ; } The actors datafetcher only gets executed when the actors field is included in the query. The actors datafetcher also introduces a new concept; the DgsDataFetchingEnvironment . The DgsDataFetchingEnvironment gives access to the context , the query itself, data loaders, and the source object. The source object is the object that contains the field. For this example, the source is the Show object, which you can use to get the show's identifier to use in the query for actors. Do note that the shows datafetcher is returning a list of Show , while the actors datafetcher fetches the actors for a single show. The framework executes the actors datafetcher for each Show returned by the shows datafetcher. If the actors get loaded from a database, this would now cause an N+1 problem. To solve the N+1 problem, you use data loaders . Note: There are more complex scenarios with nested datafetchers, and ways to pass context between related datafetchers. See the nested datafetchers guide for more advanced use-cases. Multiple @DgsData Annotations Via @DgsData.List \u00b6 Since v4.6.0 , methods can be annotated with multiple @DgsData annotations. Effectively you can resolve multiple GraphQL Type fields via the same method implementation. To do so you will have to leverage the @DgsData.List annotation, for example: Java @DgsData.List ({ @DgsData ( parentType = \"Query\" , field = \"movies\" ), @DgsData ( parentType = \"Query\" , field = \"shows\" ) }) Kotlin @DgsData.List ( DgsData ( parentType = \"Query\" , field = \"movies\" ), DgsData ( parentType = \"Query\" , field = \"shows\" ) ) Tip Both @DgsQuery and @DgsMutation can't be defined multiple times in a single method. Please use @DgsData instead and explicitly define the parentType to match either Query or Mutation . Using @InputArgument \u00b6 It's very common for GraphQL queries to have one or more input arguments. According to the GraphQL specification, an input argument can be: An input type A scalar An enum Other types, such as output types, unions and interfaces, are not allowed as input arguments. You can get input arguments as method arguments in a datafetcher method using the @InputArgument annotation. type Query { shows(title: String, filter: ShowFilter): [Show] } input ShowFilter { director: String genre: ShowGenre } enum ShowGenre { commedy, action, horror } We can write a datafetcher with the following signature: @DgsQuery public List < Show > shows ( @InputArgument String title , @InputArgument ShowFilter filter ) The @InputArgument annotation will use the name of the method argument to match it with the name of an input argument sent in the query. Optionally we can specify the name argument in the @InputArgument annotation, if the argument name doesn't match the method argument name. The framework converts input arguments to Java/Kotlin types. The first step for converting input arguments is graphql-java using scalar implementations to convert raw string input into whatever type the scalar represents. A GraphQL Int becomes an Integer in Java, a formatted date string becomes a LocalDateTime (depending on the scalars you're using!), lists become an instance of java.util.ArrayList . Input objects are represented as a Map<String, Object> in graphql-java . The next step is the DGS Framework converting the Map<String, Object> to the Java/Kotlin classes that you use for the @InputArgument . For Java classes, the framework creates a new instance of the class using the no-arg constructor. This implies that a no-arg constructor is required. It then sets each field of the instance to the input argument values. For Kotlin Data classes, the instance can only be created by passing in all arguments in the constructor. This means you have to make sure to make fields optional in the data class when the fields are optional in the GraphQL schema! If you're using the Codegen plugin (you really should!), the input types will work perfectly out of the box. Input argument conversion isn't JSON It's easy to confuse the conversion described above with JSON deserialization as you are familiar with in libraries such as Jackson. Although it looks similar, the mechanisms are completely unrelated. Input arguments aren't JSON, and the Scalar mechanism is really the core of how conversion works. This also means that Jackson annotations on Java/Kotlin types are not used at all. Defining scalars, and scalars in codegen @InputArgument is designed to work well with scalars. More information about defining custom scalars in the framework can be found here . For a scalar you typically either create a class representing the value, or use an existing type. Such types need to be mapped in Codegen configuration so that they don't (incorrectly) get generated. Nullability in Kotlin for Input Arguments \u00b6 If you're using Kotlin you must consider if an input type is nullable. If the schema defines an input argument as nullable, the code must reflect this by using a nullable type. If a non-nullable type receives a null value, Kotlin will throw an exception. For example: # name is a nullable input argument hello(name: String): String You must write the datafetcher function as: fun hello ( @InputArgument hello : String? ) In Java you don't have to worry about this, types can always be null. You do need to null check in your datafetching code! Using @InputArgument with lists \u00b6 An input argument can also be a list. If the list type is an input type, you must specify the type explicitly in the @InputArgument annotation. type Query { hello(people:[Person]): String } public String hello ( @InputArgument ( collectionType = Person . class ) List < Person > people ) Using Optional with @InputArgument \u00b6 Input arguments are often defined as optional in schemas. Your datafetcher code needs to null-check arguments to check if they were provided. Instead of null-checks you can wrap an input argument in an Optional. public List < Show > shows ( @InputArgument ( collectionType = ShowFilter . class ) Optional < ShowFilter > filter ) You do need to provide the type in the collectionType argument when using complex types, similar to using lists. If the argument is not provided, the value will be Optional.empty() . It's a matter of preference to use Optional or not. Codegen Constants \u00b6 In the examples of @DgsData so far, we used string values for the parentType and field arguments. If you are using code generation you can instead use generated constants. Codegen creates a DgsConstants class with constants for each type and field in your schema. Using this we can write a datafetcher as follows: type Query { shows: [Show] } @DgsData ( parentType = DgsConstants . QUERY_TYPE , field = DgsConstants . QUERY . Shows ) public List < Show > shows () {} The benefit of using constants is that you can detect issues between your schema and datafetchers at compile time. @RequestHeader, @RequestParam and @CookieValue \u00b6 Sometimes you need to evaluate HTTP headers, or other elements of the request, in a datafetcher. You can easily get a HTTP header value by using the @RequestHeader annotation. The @RequestHeader annotation is the same annotation as used in Spring WebMVC. @DgsQuery public String hello ( @RequestHeader String host ) Technically, headers are lists of values. If multiple values are set, you can retrieve them as a list by using a List as your argument type. Otherwise, the values are concatenated to a single String. Similar to @InputArgument it's possible to wrap a header or parameter in an Optional . Similarly, you can get request parameters using @RequestParam . Both @RequestHeader and @RequestParam support a defaultValue and required argument. If a @RequestHeader or @RequestParam is required , doesn't have a defaultValue and isn't provided, a DgsInvalidInputArgumentException is thrown. To easily get access to cookie values you can use Spring's @CookieValue annotation. @DgsQuery public String usingCookieWithDefault ( @CookieValue ( defaultValue = \"defaultvalue\" ) myCookie : String ) { return myCookie } @CookieValue supports a defaultValue and the required argument. You can also use an Optional<String> for a @CookieValue if it's not required. Using DgsRequestData to get access to the request object \u00b6 You can get access to the request object, representing the HTTP request itself, as well. It's stored on the DgsContext object in the DgsDataFetchingEnvironment . Because Spring WebMVC and Spring Webflux use different types to represent the request, the DgsRequestData is different depending on what environment (WebMVC/WebFlux) you're running in. The DgsRequestData interface only gives access to the request headers and the extensions . To get the actual request object, you need to cast the DgsRequestData to the correct implementation. This is either DgsWebMvcRequestData or DgsReactiveRequestData . Let's use this in an example to set a cookie, which is done through the response object. Let's look at a WebMVC example first. From the DgsWebMvcRequestData you can get the WebRequest , which can be further cast to a ServletWebRequest . @DgsQuery @DgsMutation public String updateCookie ( @InputArgument String value , DgsDataFetchingEnvironment dfe ) { DgsWebMvcRequestData requestData = ( DgsWebMvcRequestData ) dfe . getDgsContext (). getRequestData (); ServletWebRequest webRequest = ( ServletWebRequest ) requestData . getWebRequest (); javax . servlet . http . Cookie cookie = new javax . servlet . http . Cookie ( \"mydgscookie\" , value ); webRequest . getResponse (). addCookie ( cookie ); return value ; } Now let's try the same with WebFlux. DgsRequestData is now an instance of DgsReactiveRequestData , which gives access to the ServerRequest . @DgsMutation public String updateCookie ( @InputArgument String value , DgsDataFetchingEnvironment dfe ) { DgsReactiveRequestData requestData = ( DgsReactiveRequestData ) dfe . getDgsContext (). getRequestData (); ServerRequest serverRequest = requestData . getServerRequest (); serverRequest . exchange (). getResponse () . addCookie ( ResponseCookie . from ( \"mydgscookie\" , \"webfluxupdated\" ). build ()); return value ; } Using data fetcher context \u00b6 The DgsRequestData object described in the previous section is part of the data fetching context . The DGS Framework adds the DgsRequestData to the data fetching context. You can also add your own data to the context, for use in data fetchers. The context is initialized per request, before query execution starts. You can customize the context and add your own data by creating a DgsCustomContextBuilder . @Component public class MyContextBuilder implements DgsCustomContextBuilder < MyContext > { @Override public MyContext build () { return new MyContext (); } } public class MyContext { private final String customState = \"Custom state!\" ; public String getCustomState () { return customState ; } } If you require access to the request, e.g. to read HTTP headers, you can implement the DgsCustomContextBuilderWithRequest interface instead. @Component public class MyContextBuilder implements DgsCustomContextBuilderWithRequest < MyContext > { @Override public MyContext build ( Map < String , Object > extensions , HttpHeaders headers , WebRequest webRequest ) { //e.g. you can now read headers to set up context return new MyContext (); } } A data fetcher can now retrieve the context by calling the getCustomContext() method: @DgsData ( parentType = \"Query\" , field = \"withContext\" ) public String withContext ( DataFetchingEnvironment dfe ) { MyContext customContext = DgsContext . getCustomContext ( dfe ); return customContext . getCustomState (); } Similarly, custom context can be used in a DataLoader. @DgsDataLoader ( name = \"exampleLoaderWithContext\" ) public class ExampleLoaderWithContext implements BatchLoaderWithContext < String , String > { @Override public CompletionStage < List < String >> load ( List < String > keys , BatchLoaderEnvironment environment ) { MyContext context = DgsContext . getCustomContext ( environment ); return CompletableFuture . supplyAsync (() -> keys . stream (). map ( key -> context . getCustomState () + \" \" + key ). collect ( Collectors . toList ())); } }","title":"Data fetching"},{"location":"datafetching/#the-dgsdata-dgsquery-dgsmutation-and-dgssubscription-annotations","text":"You use the @DgsData annotation on a Java/Kotlin method to make that method a datafetcher. The method must be in a @DgsComponent class. The @DgsData annotation has two parameters: Parameter Description parentType This is the type that contains the field. field The field that the datafetcher is responsible for For example, we have the following schema. type Query { shows: [Show] } type Show { title: String actors: [Actor] } We can implement this schema with a single datafetcher. @DgsComponent public class ShowDataFetcher { @DgsData ( parentType = \"Query\" , field = \"shows\" ) public List < Show > shows () { //Load shows from a database and return the list of Show objects return shows ; } } If the field parameter is not set, the method name will be used as the field name. The @DgsQuery , @DgsMutation and @DgsSubscription annotations are shorthands to define datafetchers on the Query , Mutation and Subscription types. The following definitions are all equivalent. @DgsData ( parentType = \"Query\" , field = \"shows\" ) public List < Show > shows () { .... } // The \"field\" argument is omitted. It uses the method name as the field name. @DgsData ( parentType = \"Query\" ) public List < Show > shows () { .... } // The parentType is \"Query\", the field name is derived from the method name. @DgsQuery public List < Show > shows () { .... } // The parentType is \"Query\", the field name is explicitly specified. @DgsQuery ( field = \"shows\" ) public List < Show > shows () { .... } Notice how a datafetcher can return complex objects or lists of objects. You don't have to create a separate datafetcher for each field. The framework will take care of only returning the fields that are specified in the query. For example, if a user queries: { shows { title } } Although we're returning Show objects, which in the example contains both a title and an actors field, the actors field gets stripped off before the response gets sent back.","title":"The @DgsData, @DgsQuery, @DgsMutation and @DgsSubscription Annotations"},{"location":"datafetching/#child-datafetchers","text":"The previous example assumed that you could load a list of Show objects from your database with a single query. It wouldn't matter which fields the user included in the GraphQL query; the cost of loading the shows would be the same. What if there is an extra cost to specific fields? For example, what if loading actors for a show requires an extra query? It would be wasteful to run the extra query to load actors if the actors field doesn't get returned to the user. In such scenarios, it's better to create a separate datafetcher for the expensive field. @DgsQuery public List < Show > shows () { //Load shows, which doesn't include \"actors\" return shows ; } @DgsData ( parentType = \"Show\" , field = \"actors\" ) public List < Actor > actors ( DgsDataFetchingEnvironment dfe ) { Show show = dfe . getSource (); actorsService . forShow ( show . getId ()); return actors ; } The actors datafetcher only gets executed when the actors field is included in the query. The actors datafetcher also introduces a new concept; the DgsDataFetchingEnvironment . The DgsDataFetchingEnvironment gives access to the context , the query itself, data loaders, and the source object. The source object is the object that contains the field. For this example, the source is the Show object, which you can use to get the show's identifier to use in the query for actors. Do note that the shows datafetcher is returning a list of Show , while the actors datafetcher fetches the actors for a single show. The framework executes the actors datafetcher for each Show returned by the shows datafetcher. If the actors get loaded from a database, this would now cause an N+1 problem. To solve the N+1 problem, you use data loaders . Note: There are more complex scenarios with nested datafetchers, and ways to pass context between related datafetchers. See the nested datafetchers guide for more advanced use-cases.","title":"Child Datafetchers"},{"location":"datafetching/#multiple-dgsdata-annotations-via-dgsdatalist","text":"Since v4.6.0 , methods can be annotated with multiple @DgsData annotations. Effectively you can resolve multiple GraphQL Type fields via the same method implementation. To do so you will have to leverage the @DgsData.List annotation, for example: Java @DgsData.List ({ @DgsData ( parentType = \"Query\" , field = \"movies\" ), @DgsData ( parentType = \"Query\" , field = \"shows\" ) }) Kotlin @DgsData.List ( DgsData ( parentType = \"Query\" , field = \"movies\" ), DgsData ( parentType = \"Query\" , field = \"shows\" ) ) Tip Both @DgsQuery and @DgsMutation can't be defined multiple times in a single method. Please use @DgsData instead and explicitly define the parentType to match either Query or Mutation .","title":"Multiple @DgsData Annotations Via @DgsData.List"},{"location":"datafetching/#using-inputargument","text":"It's very common for GraphQL queries to have one or more input arguments. According to the GraphQL specification, an input argument can be: An input type A scalar An enum Other types, such as output types, unions and interfaces, are not allowed as input arguments. You can get input arguments as method arguments in a datafetcher method using the @InputArgument annotation. type Query { shows(title: String, filter: ShowFilter): [Show] } input ShowFilter { director: String genre: ShowGenre } enum ShowGenre { commedy, action, horror } We can write a datafetcher with the following signature: @DgsQuery public List < Show > shows ( @InputArgument String title , @InputArgument ShowFilter filter ) The @InputArgument annotation will use the name of the method argument to match it with the name of an input argument sent in the query. Optionally we can specify the name argument in the @InputArgument annotation, if the argument name doesn't match the method argument name. The framework converts input arguments to Java/Kotlin types. The first step for converting input arguments is graphql-java using scalar implementations to convert raw string input into whatever type the scalar represents. A GraphQL Int becomes an Integer in Java, a formatted date string becomes a LocalDateTime (depending on the scalars you're using!), lists become an instance of java.util.ArrayList . Input objects are represented as a Map<String, Object> in graphql-java . The next step is the DGS Framework converting the Map<String, Object> to the Java/Kotlin classes that you use for the @InputArgument . For Java classes, the framework creates a new instance of the class using the no-arg constructor. This implies that a no-arg constructor is required. It then sets each field of the instance to the input argument values. For Kotlin Data classes, the instance can only be created by passing in all arguments in the constructor. This means you have to make sure to make fields optional in the data class when the fields are optional in the GraphQL schema! If you're using the Codegen plugin (you really should!), the input types will work perfectly out of the box. Input argument conversion isn't JSON It's easy to confuse the conversion described above with JSON deserialization as you are familiar with in libraries such as Jackson. Although it looks similar, the mechanisms are completely unrelated. Input arguments aren't JSON, and the Scalar mechanism is really the core of how conversion works. This also means that Jackson annotations on Java/Kotlin types are not used at all. Defining scalars, and scalars in codegen @InputArgument is designed to work well with scalars. More information about defining custom scalars in the framework can be found here . For a scalar you typically either create a class representing the value, or use an existing type. Such types need to be mapped in Codegen configuration so that they don't (incorrectly) get generated.","title":"Using @InputArgument"},{"location":"datafetching/#nullability-in-kotlin-for-input-arguments","text":"If you're using Kotlin you must consider if an input type is nullable. If the schema defines an input argument as nullable, the code must reflect this by using a nullable type. If a non-nullable type receives a null value, Kotlin will throw an exception. For example: # name is a nullable input argument hello(name: String): String You must write the datafetcher function as: fun hello ( @InputArgument hello : String? ) In Java you don't have to worry about this, types can always be null. You do need to null check in your datafetching code!","title":"Nullability in Kotlin for Input Arguments"},{"location":"datafetching/#using-inputargument-with-lists","text":"An input argument can also be a list. If the list type is an input type, you must specify the type explicitly in the @InputArgument annotation. type Query { hello(people:[Person]): String } public String hello ( @InputArgument ( collectionType = Person . class ) List < Person > people )","title":"Using @InputArgument with lists"},{"location":"datafetching/#using-optional-with-inputargument","text":"Input arguments are often defined as optional in schemas. Your datafetcher code needs to null-check arguments to check if they were provided. Instead of null-checks you can wrap an input argument in an Optional. public List < Show > shows ( @InputArgument ( collectionType = ShowFilter . class ) Optional < ShowFilter > filter ) You do need to provide the type in the collectionType argument when using complex types, similar to using lists. If the argument is not provided, the value will be Optional.empty() . It's a matter of preference to use Optional or not.","title":"Using Optional with @InputArgument"},{"location":"datafetching/#codegen-constants","text":"In the examples of @DgsData so far, we used string values for the parentType and field arguments. If you are using code generation you can instead use generated constants. Codegen creates a DgsConstants class with constants for each type and field in your schema. Using this we can write a datafetcher as follows: type Query { shows: [Show] } @DgsData ( parentType = DgsConstants . QUERY_TYPE , field = DgsConstants . QUERY . Shows ) public List < Show > shows () {} The benefit of using constants is that you can detect issues between your schema and datafetchers at compile time.","title":"Codegen Constants"},{"location":"datafetching/#requestheader-requestparam-and-cookievalue","text":"Sometimes you need to evaluate HTTP headers, or other elements of the request, in a datafetcher. You can easily get a HTTP header value by using the @RequestHeader annotation. The @RequestHeader annotation is the same annotation as used in Spring WebMVC. @DgsQuery public String hello ( @RequestHeader String host ) Technically, headers are lists of values. If multiple values are set, you can retrieve them as a list by using a List as your argument type. Otherwise, the values are concatenated to a single String. Similar to @InputArgument it's possible to wrap a header or parameter in an Optional . Similarly, you can get request parameters using @RequestParam . Both @RequestHeader and @RequestParam support a defaultValue and required argument. If a @RequestHeader or @RequestParam is required , doesn't have a defaultValue and isn't provided, a DgsInvalidInputArgumentException is thrown. To easily get access to cookie values you can use Spring's @CookieValue annotation. @DgsQuery public String usingCookieWithDefault ( @CookieValue ( defaultValue = \"defaultvalue\" ) myCookie : String ) { return myCookie } @CookieValue supports a defaultValue and the required argument. You can also use an Optional<String> for a @CookieValue if it's not required.","title":"@RequestHeader, @RequestParam and @CookieValue"},{"location":"datafetching/#using-dgsrequestdata-to-get-access-to-the-request-object","text":"You can get access to the request object, representing the HTTP request itself, as well. It's stored on the DgsContext object in the DgsDataFetchingEnvironment . Because Spring WebMVC and Spring Webflux use different types to represent the request, the DgsRequestData is different depending on what environment (WebMVC/WebFlux) you're running in. The DgsRequestData interface only gives access to the request headers and the extensions . To get the actual request object, you need to cast the DgsRequestData to the correct implementation. This is either DgsWebMvcRequestData or DgsReactiveRequestData . Let's use this in an example to set a cookie, which is done through the response object. Let's look at a WebMVC example first. From the DgsWebMvcRequestData you can get the WebRequest , which can be further cast to a ServletWebRequest . @DgsQuery @DgsMutation public String updateCookie ( @InputArgument String value , DgsDataFetchingEnvironment dfe ) { DgsWebMvcRequestData requestData = ( DgsWebMvcRequestData ) dfe . getDgsContext (). getRequestData (); ServletWebRequest webRequest = ( ServletWebRequest ) requestData . getWebRequest (); javax . servlet . http . Cookie cookie = new javax . servlet . http . Cookie ( \"mydgscookie\" , value ); webRequest . getResponse (). addCookie ( cookie ); return value ; } Now let's try the same with WebFlux. DgsRequestData is now an instance of DgsReactiveRequestData , which gives access to the ServerRequest . @DgsMutation public String updateCookie ( @InputArgument String value , DgsDataFetchingEnvironment dfe ) { DgsReactiveRequestData requestData = ( DgsReactiveRequestData ) dfe . getDgsContext (). getRequestData (); ServerRequest serverRequest = requestData . getServerRequest (); serverRequest . exchange (). getResponse () . addCookie ( ResponseCookie . from ( \"mydgscookie\" , \"webfluxupdated\" ). build ()); return value ; }","title":"Using DgsRequestData to get access to the request object"},{"location":"datafetching/#using-data-fetcher-context","text":"The DgsRequestData object described in the previous section is part of the data fetching context . The DGS Framework adds the DgsRequestData to the data fetching context. You can also add your own data to the context, for use in data fetchers. The context is initialized per request, before query execution starts. You can customize the context and add your own data by creating a DgsCustomContextBuilder . @Component public class MyContextBuilder implements DgsCustomContextBuilder < MyContext > { @Override public MyContext build () { return new MyContext (); } } public class MyContext { private final String customState = \"Custom state!\" ; public String getCustomState () { return customState ; } } If you require access to the request, e.g. to read HTTP headers, you can implement the DgsCustomContextBuilderWithRequest interface instead. @Component public class MyContextBuilder implements DgsCustomContextBuilderWithRequest < MyContext > { @Override public MyContext build ( Map < String , Object > extensions , HttpHeaders headers , WebRequest webRequest ) { //e.g. you can now read headers to set up context return new MyContext (); } } A data fetcher can now retrieve the context by calling the getCustomContext() method: @DgsData ( parentType = \"Query\" , field = \"withContext\" ) public String withContext ( DataFetchingEnvironment dfe ) { MyContext customContext = DgsContext . getCustomContext ( dfe ); return customContext . getCustomState (); } Similarly, custom context can be used in a DataLoader. @DgsDataLoader ( name = \"exampleLoaderWithContext\" ) public class ExampleLoaderWithContext implements BatchLoaderWithContext < String , String > { @Override public CompletionStage < List < String >> load ( List < String > keys , BatchLoaderEnvironment environment ) { MyContext context = DgsContext . getCustomContext ( environment ); return CompletableFuture . supplyAsync (() -> keys . stream (). map ( key -> context . getCustomState () + \" \" + key ). collect ( Collectors . toList ())); } }","title":"Using data fetcher context"},{"location":"error-handling/","text":"It is common in GraphQL to support error reporting by adding an errors block to a response. Responses can contain both data and errors, for example when some fields where resolved successfully, but other fields had errors. A field with an error is set to null, and an error is added to the errors block. The DGS framework has an exception handler out-of-the-box that works according to the specification described in the Error Specification section on this page. This exception handler handles exceptions from data fetchers. Any RuntimeException is translated to a GraphQLError of type INTERNAL . For some specific exception types, a more specific GraphQL error type is used. Exception type GraphQL error type description AccessDeniedException PERMISSION_DENIED When a @Secured check fails DgsEntityNotFoundException NOT_FOUND Thrown by the developer when a requested entity (e.g. based on query parameters) isn't found Mapping custom exceptions \u00b6 It can be useful to map application specific exceptions to meaningful exceptions back to the client. You can do this by registering a DataFetcherExceptionHandler . Make sure to delegate to the DefaultDataFetcherExceptionHandler class, this is the default exception handler of the framework. If you don't delegate to this class, you lose the framework's built-in exception handler. The following is an example of a custom exception handler implementation. @Component public class CustomDataFetchingExceptionHandler implements DataFetcherExceptionHandler { private final DefaultDataFetcherExceptionHandler defaultHandler = new DefaultDataFetcherExceptionHandler (); @Override public DataFetcherExceptionHandlerResult onException ( DataFetcherExceptionHandlerParameters handlerParameters ) { if ( handlerParameters . getException () instanceof MyException ) { Map < String , Object > debugInfo = new HashMap <> (); debugInfo . put ( \"somefield\" , \"somevalue\" ); GraphQLError graphqlError = TypedGraphQLError . INTERNAL . message ( \"This custom thing went wrong!\" ) . debugInfo ( debugInfo ) . path ( handlerParameters . getPath ()). build (); return DataFetcherExceptionHandlerResult . newResult () . error ( graphqlError ) . build (); } else { return defaultHandler . onException ( handlerParameters ); } } } The following data fetcher throws MyException . @DgsComponent public class HelloDataFetcher { @DgsData ( parentType = \"Query\" , field = \"hello\" ) @DgsEnableDataFetcherInstrumentation ( false ) public String hello ( DataFetchingEnvironment dfe ) { throw new MyException (); } } Querying the hello field results in the following response. { \"errors\" : [ { \"message\" : \"This custom thing went wrong!\" , \"locations\" : [], \"path\" : [ \"hello\" ], \"extensions\" : { \"errorType\" : \"INTERNAL\" , \"debugInfo\" : { \"somefield\" : \"somevalue\" } } } ], \"data\" : { \"hello\" : null } } Error specification \u00b6 There are two families of errors we typically encounter for GraphQL: Comprehensive Errors. These are unexpected errors and do not represent a condition that the end user can be expected to fix. Errors of this sort are generally applicable to many types and fields. Such errors appear in the errors array in the GraphQL response. Errors as Data. These are errors that are informative to the end user (for example: \u201cthis title is not available in your country\u201d or \u201cyour account has been suspended\u201d). Errors of this sort are typically specific to a particular use case and apply only to certain fields or to a certain subset of fields. These errors are part of the GraphQL schema. The GraphQLError Interface \u00b6 The GraphQL specification provides minimal guidance on the structure of an error. The only required field is a message String, which has no defined format. In Studio Edge we would like to have a stronger, more expressive contract. Here is the definition we are using: field type description message (non-nullable) String! a string description of the error intended for the developer as a guide to understand and correct the error locations [Location] an array of code locations, where each location is a map with the keys line and column , both natural numbers starting from 1 that describe the beginning of an associated syntax element path [String | Int] if the error is associated with one or more particular fields in the response, this field of the error details the paths of those response fields that experienced the error (this allows clients to identify whether a null result is intentional or caused by a runtime error) extensions [TypedError] see \u201cThe TypedError Interface\u201d below \"\"\" Error format as defined in GraphQL Spec \"\"\" interface GraphQLError { message: String! // Required by GraphQL Spec locations: [Location] // See GraphQL Spec path: [String | Int] // See GraphQL Spec extensions: TypedError } See the GraphQL specification: Errors for more information. The TypedError Interface \u00b6 Studio Edge defines TypedError as follows: field type description errorType (non-nullable) ErrorType! an enumerated error code that is meant as a fairly coarse characterization of an error, sufficient for client-side branching logic errorDetail ErrorDetail an enumeration that provides more detail about the error, including its specific cause (the elements of this enumeration are subject to change and are not documented here) origin String the name of the source that issued the error (for instance the name of a backend service, DGS, gateway, client library, or client app) debugInfo DebugInfo if the request included a flag indicating that it wanted debug information, this field contains that additional information (such as a stack trace or additional reporting from an upstream service) debugUri String the URI of a page that contains additional information that may be helpful in debugging the error (this could be a generic page for errors of this sort, or a specific page about the particular error instance) interface TypedError { \"\"\" An error code from the ErrorType enumeration. An errorType is a fairly coarse characterization of an error that should be sufficient for client side branching logic. \"\"\" errorType: ErrorType! \"\"\" The ErrorDetail is an optional field which will provide more fine grained information on the error condition. This allows the ErrorType enumeration to be small and mostly static so that application branching logic can depend on it. The ErrorDetail provides a more specific cause for the error. This enumeration will be much larger and likely change/grow over time. \"\"\" errorDetail: ErrorDetail \"\"\" Indicates the source that issued the error. For example, could be a backend service name, a domain graph service name, or a gateway. In the case of client code throwing the error, this may be a client library name, or the client app name. \"\"\" origin: String \"\"\" Optionally provided based on request flag Could include e.g. stacktrace or info from upstream service \"\"\" debugInfo: DebugInfo \"\"\" Http URI to a page detailing additional information that could be used to debug the error. This information may be general to the class of error or specific to this particular instance of the error. \"\"\" debugUri: String } The ErrorType Enumeration \u00b6 The following table shows the available ErrorType enum values: type description HTTP analog BAD_REQUEST This indicates a problem with the request. Retrying the same request is not likely to succeed. An example would be a query or argument that cannot be deserialized. 400 Bad Request FAILED_PRECONDITION The operation was rejected because the system is not in a state required for the operation\u2019s execution. For example, the directory to be deleted is non-empty, an rmdir operation is applied to a non-directory, etc. Use UNAVAILABLE instead if the client can retry just the failing call without waiting for the system state to be explicitly fixed. 400 Bad Request, or 500 Internal Server Error INTERNAL This indicates that an unexpected internal error was encountered: some invariants expected by the underlying system have been broken. This error code is reserved for serious errors. 500 Internal Server Error NOT_FOUND This could apply to a resource that has never existed (e.g. bad resource id), or a resource that no longer exists (e.g. cache expired). Note to server developers: if a request is denied for an entire class of users, such as gradual feature rollout or undocumented allowlist, NOT_FOUND may be used. If a request is denied for some users within a class of users, such as user-based access control, PERMISSION_DENIED must be used. 404 Not Found PERMISSION_DENIED This indicates that the requester does not have permission to execute the specified operation. PERMISSION_DENIED must not be used for rejections caused by exhausting some resource or quota. PERMISSION_DENIED must not be used if the caller cannot be identified (use UNAUTHENTICATED instead for those errors). This error does not imply that the request is valid or the requested entity exists or satisfies other pre-conditions. 403 Forbidden UNAUTHENTICATED This indicates that the request does not have valid authentication credentials but the route requires authentication. 401 Unauthorized UNAVAILABLE This indicates that the service is currently unavailable. This is most likely a transient condition, which can be corrected by retrying with a backoff. 503 Unavailable UNKNOWN This error may be returned, for example, when an error code received from another address space belongs to an error space that is not known in this address space. Errors raised by APIs that do not return enough error information may also be converted to this error. If a client sees an errorType that is not known to it, it will be interpreted as UNKNOWN . Unknown errors must not trigger any special behavior. They may be treated by an implementation as being equivalent to INTERNAL . 520 Unknown Error The HTTP analogs are only rough mappings that are given here to provide a quick conceptual explanation of the semantics of the error by showing their analogs in the HTTP specification.","title":"Error Handling"},{"location":"error-handling/#mapping-custom-exceptions","text":"It can be useful to map application specific exceptions to meaningful exceptions back to the client. You can do this by registering a DataFetcherExceptionHandler . Make sure to delegate to the DefaultDataFetcherExceptionHandler class, this is the default exception handler of the framework. If you don't delegate to this class, you lose the framework's built-in exception handler. The following is an example of a custom exception handler implementation. @Component public class CustomDataFetchingExceptionHandler implements DataFetcherExceptionHandler { private final DefaultDataFetcherExceptionHandler defaultHandler = new DefaultDataFetcherExceptionHandler (); @Override public DataFetcherExceptionHandlerResult onException ( DataFetcherExceptionHandlerParameters handlerParameters ) { if ( handlerParameters . getException () instanceof MyException ) { Map < String , Object > debugInfo = new HashMap <> (); debugInfo . put ( \"somefield\" , \"somevalue\" ); GraphQLError graphqlError = TypedGraphQLError . INTERNAL . message ( \"This custom thing went wrong!\" ) . debugInfo ( debugInfo ) . path ( handlerParameters . getPath ()). build (); return DataFetcherExceptionHandlerResult . newResult () . error ( graphqlError ) . build (); } else { return defaultHandler . onException ( handlerParameters ); } } } The following data fetcher throws MyException . @DgsComponent public class HelloDataFetcher { @DgsData ( parentType = \"Query\" , field = \"hello\" ) @DgsEnableDataFetcherInstrumentation ( false ) public String hello ( DataFetchingEnvironment dfe ) { throw new MyException (); } } Querying the hello field results in the following response. { \"errors\" : [ { \"message\" : \"This custom thing went wrong!\" , \"locations\" : [], \"path\" : [ \"hello\" ], \"extensions\" : { \"errorType\" : \"INTERNAL\" , \"debugInfo\" : { \"somefield\" : \"somevalue\" } } } ], \"data\" : { \"hello\" : null } }","title":"Mapping custom exceptions"},{"location":"error-handling/#error-specification","text":"There are two families of errors we typically encounter for GraphQL: Comprehensive Errors. These are unexpected errors and do not represent a condition that the end user can be expected to fix. Errors of this sort are generally applicable to many types and fields. Such errors appear in the errors array in the GraphQL response. Errors as Data. These are errors that are informative to the end user (for example: \u201cthis title is not available in your country\u201d or \u201cyour account has been suspended\u201d). Errors of this sort are typically specific to a particular use case and apply only to certain fields or to a certain subset of fields. These errors are part of the GraphQL schema.","title":"Error specification"},{"location":"error-handling/#the-graphqlerror-interface","text":"The GraphQL specification provides minimal guidance on the structure of an error. The only required field is a message String, which has no defined format. In Studio Edge we would like to have a stronger, more expressive contract. Here is the definition we are using: field type description message (non-nullable) String! a string description of the error intended for the developer as a guide to understand and correct the error locations [Location] an array of code locations, where each location is a map with the keys line and column , both natural numbers starting from 1 that describe the beginning of an associated syntax element path [String | Int] if the error is associated with one or more particular fields in the response, this field of the error details the paths of those response fields that experienced the error (this allows clients to identify whether a null result is intentional or caused by a runtime error) extensions [TypedError] see \u201cThe TypedError Interface\u201d below \"\"\" Error format as defined in GraphQL Spec \"\"\" interface GraphQLError { message: String! // Required by GraphQL Spec locations: [Location] // See GraphQL Spec path: [String | Int] // See GraphQL Spec extensions: TypedError } See the GraphQL specification: Errors for more information.","title":"The GraphQLError Interface"},{"location":"error-handling/#the-typederror-interface","text":"Studio Edge defines TypedError as follows: field type description errorType (non-nullable) ErrorType! an enumerated error code that is meant as a fairly coarse characterization of an error, sufficient for client-side branching logic errorDetail ErrorDetail an enumeration that provides more detail about the error, including its specific cause (the elements of this enumeration are subject to change and are not documented here) origin String the name of the source that issued the error (for instance the name of a backend service, DGS, gateway, client library, or client app) debugInfo DebugInfo if the request included a flag indicating that it wanted debug information, this field contains that additional information (such as a stack trace or additional reporting from an upstream service) debugUri String the URI of a page that contains additional information that may be helpful in debugging the error (this could be a generic page for errors of this sort, or a specific page about the particular error instance) interface TypedError { \"\"\" An error code from the ErrorType enumeration. An errorType is a fairly coarse characterization of an error that should be sufficient for client side branching logic. \"\"\" errorType: ErrorType! \"\"\" The ErrorDetail is an optional field which will provide more fine grained information on the error condition. This allows the ErrorType enumeration to be small and mostly static so that application branching logic can depend on it. The ErrorDetail provides a more specific cause for the error. This enumeration will be much larger and likely change/grow over time. \"\"\" errorDetail: ErrorDetail \"\"\" Indicates the source that issued the error. For example, could be a backend service name, a domain graph service name, or a gateway. In the case of client code throwing the error, this may be a client library name, or the client app name. \"\"\" origin: String \"\"\" Optionally provided based on request flag Could include e.g. stacktrace or info from upstream service \"\"\" debugInfo: DebugInfo \"\"\" Http URI to a page detailing additional information that could be used to debug the error. This information may be general to the class of error or specific to this particular instance of the error. \"\"\" debugUri: String }","title":"The TypedError Interface"},{"location":"error-handling/#the-errortype-enumeration","text":"The following table shows the available ErrorType enum values: type description HTTP analog BAD_REQUEST This indicates a problem with the request. Retrying the same request is not likely to succeed. An example would be a query or argument that cannot be deserialized. 400 Bad Request FAILED_PRECONDITION The operation was rejected because the system is not in a state required for the operation\u2019s execution. For example, the directory to be deleted is non-empty, an rmdir operation is applied to a non-directory, etc. Use UNAVAILABLE instead if the client can retry just the failing call without waiting for the system state to be explicitly fixed. 400 Bad Request, or 500 Internal Server Error INTERNAL This indicates that an unexpected internal error was encountered: some invariants expected by the underlying system have been broken. This error code is reserved for serious errors. 500 Internal Server Error NOT_FOUND This could apply to a resource that has never existed (e.g. bad resource id), or a resource that no longer exists (e.g. cache expired). Note to server developers: if a request is denied for an entire class of users, such as gradual feature rollout or undocumented allowlist, NOT_FOUND may be used. If a request is denied for some users within a class of users, such as user-based access control, PERMISSION_DENIED must be used. 404 Not Found PERMISSION_DENIED This indicates that the requester does not have permission to execute the specified operation. PERMISSION_DENIED must not be used for rejections caused by exhausting some resource or quota. PERMISSION_DENIED must not be used if the caller cannot be identified (use UNAUTHENTICATED instead for those errors). This error does not imply that the request is valid or the requested entity exists or satisfies other pre-conditions. 403 Forbidden UNAUTHENTICATED This indicates that the request does not have valid authentication credentials but the route requires authentication. 401 Unauthorized UNAVAILABLE This indicates that the service is currently unavailable. This is most likely a transient condition, which can be corrected by retrying with a backoff. 503 Unavailable UNKNOWN This error may be returned, for example, when an error code received from another address space belongs to an error space that is not known in this address space. Errors raised by APIs that do not return enough error information may also be converted to this error. If a client sees an errorType that is not known to it, it will be interpreted as UNKNOWN . Unknown errors must not trigger any special behavior. They may be treated by an implementation as being equivalent to INTERNAL . 520 Unknown Error The HTTP analogs are only rough mappings that are given here to provide a quick conceptual explanation of the semantics of the error by showing their analogs in the HTTP specification.","title":"The ErrorType Enumeration"},{"location":"examples/","text":"Common Patterns \u00b6 We have several example applications that demonstrate the use of the DGS framework in a fully working example. Each example has detailed documentation in the corresponding Github repository, and is further explained in code comments. Java DGS example : An implementation of a typical GraphQL service Kotlin DGS example : The same example as above, but implemented in Kotlin Federation examples : A Federated GraphQL example, using Apollo Gateway Community Contributions \u00b6 We welcome contributions from the community. Check out some of these examples for more patterns. Authorization using custom directives : Authz using a custom @secured directive","title":"Examples"},{"location":"examples/#common-patterns","text":"We have several example applications that demonstrate the use of the DGS framework in a fully working example. Each example has detailed documentation in the corresponding Github repository, and is further explained in code comments. Java DGS example : An implementation of a typical GraphQL service Kotlin DGS example : The same example as above, but implemented in Kotlin Federation examples : A Federated GraphQL example, using Apollo Gateway","title":"Common Patterns"},{"location":"examples/#community-contributions","text":"We welcome contributions from the community. Check out some of these examples for more patterns. Authorization using custom directives : Authz using a custom @secured directive","title":"Community Contributions"},{"location":"federation/","text":"Federation is based on the Federation spec . A DGS is federation-compatible out of the box with the ability to reference and extend federated types. There is more federation documentation available Read the Federation Spec . Check out Federated Testing to learn how to write tests for federated queries. Federation Example DGS \u00b6 This is a DGS example that demonstrates how to implement a federated type, and test federated queries. The source code in this guide comes from the Federation example app . We highly recommend cloning the project and use the IDE while following this guide. The example project has the following set up: A federated gateway is set up using Apollo's federation gateway libraries. The Shows DGS defines and owns the Show type. The Reviews DGS adds a reviews field to the Show type. Info If you are completely new to the DGS framework, please take a look at the DGS Getting Started guide, which also contains an introduction video. The remainder of the guide on this page assumes basic GraphQL and DGS knowledge, and focuses on more advanced use cases. Defining a federated type \u00b6 The Shows DGS defines the Show type with fields id, title and releaseYear. Note that the id field is marked as the key. The example has one key, but you can have multiple kets as well @key(fields:\"fieldA fieldB\" This indicates to the gateway that the id field will be used for identifying the corresponding Show in the Shows DGS and must be specified for federated types. type Query { shows(titleFilter: String): [Show] } type Show @key(fields: \"id\") { id: ID title: String releaseYear: Int } Extending a federated Type \u00b6 To extend a type you redefine the type in your own schema, using directive @extends to instruct that it's a type extension. @key is required to indicate the field that the gateway will use to identify the original Show for a query. In this case, the key is the id field. type Show @key(fields: \"id\") @extends { id: ID @external reviews: [Review] } type Review { starRating: Int } When redefining a type, only the id field, and the fields you're adding need to be listed. Other fields, such as title for Show type are provided by the Shows DGS and do not need to be specified unless you are using it in the schema. Federation makes sure the fields provided by all DGSs are combined into a single type for returning the results of a query. Info Don't forget to use the @external directive if you define a field that doesn't belong to your DGS, but you need to reference it. Implementing a Federated Type \u00b6 The very first step to get started is to generate Java types that represent the schema. This is configured in build.gradle as described in the manual . When running ./gradlew build the Java types are generated into the build/generated folder, which are then automatically added to the classpath. Provide an Entity Fetcher \u00b6 Let's go through an example of the following query sent to the gateway: query { shows { title reviews { starRating } } } The gateway first fetches the list of all the shows from the Shows DGS containing the title and id fields. query { shows { __typename id title } } Next, the gateway sends the following _entities query to the Reviews DGS using the list of id s from the first query: query($representations: [_Any!]!) { _entities(representations: $representations) { ... on Show { reviews { starRating } } } } This query comes with the following variables: { \"representations\" : [ { \"__typename\" : \"Show\" , \"id\" : 1 }, , { \"__typename\" : \"Show\" , \"id\" : 2 }, { \"__typename\" : \"Show\" , \"id\" : 3 }, { \"__typename\" : \"Show\" , \"id\" : 4 }, { \"__typename\" : \"Show\" , \"id\" : 5 } ] } The Reviews DGS needs to implement an entity fetcher to handle this query. An entity fetcher is responsible for creating an instance of a Show based on the representation in the _entities query above. The DGS framework does most of the heavy lifting, and all we have to do is provide the following: Full code @DgsEntityFetcher ( name = \"Show\" ) public Show show ( Map < String , Object > values ) { return new Show (( String ) values . get ( \"id\" ), null ); } Tip Remember that the Show Java type here is generated by codegen. It's generated from the schema, so it only has the fields our schema specifies. Providing Data with a Data Fetcher \u00b6 Now the DGS knows how to create a Show instance when an _entities query is received, we can specify how to hydrate data for the reviews field. Full code @DgsData ( parentType = \"Show\" , field = \"reviews\" ) public List < Review > reviews ( DgsDataFetchingEnvironment dataFetchingEnvironment ) { Show show = dataFetchingEnvironment . getSource (); return reviews . get ( show . getId ()); } Testing a Federated Query \u00b6 You can always manually test federated queries by running the gateway and your DGS locally. You can also manually test a federated query against just your DGS, without the gateway, using the _entities query to replicate the call made to your DGS by the gateway. For automated tests, the QueryExecutor gives a way to run queries from unit tests, with very little startup overhead (in the order of 500ms). We can capture (or manually write) the _entities query that the gateway sends to the DGS. When running the query through the (locally running) gateway, the DGS will log the query that it receives. Simply copy this query in a QueryExecutor test, and that verifies the DGS in isolation. @SpringBootTest ( classes = { DgsAutoConfiguration . class , ReviewsDatafetcher . class }) class ReviewssDatafetcherTest { @Autowired DgsQueryExecutor dgsQueryExecutor ; @Test void shows () { Map < String , Object > representation = new HashMap <> (); representation . put ( \"__typename\" , \"Show\" ); representation . put ( \"id\" , \"1\" ); List < Map < String , Object >> representationsList = new ArrayList <> (); representationsList . add ( representation ); Map < String , Object > variables = new HashMap <> (); variables . put ( \"representations\" , representationsList ); List < Review > reviewsList = dgsQueryExecutor . executeAndExtractJsonPathAsObject ( \"query ($representations:[_Any!]!) {\" + \"_entities(representations:$representations) {\" + \"... on Show {\" + \" reviews {\" + \" starRating\" + \"}}}}\" , \"data['_entities'][0].reviews\" , variables , new TypeRef <> () {}); assertThat ( reviewsList ) . isNotNull () . hasSize ( 3 ); } } To help build the federated _entities query, you can also use the EntitiesGraphQLQuery available in graphql-dgs-client package along with code generation. Here is an example of the same test that uses the builder API: @SpringBootTest ( classes = { DgsAutoConfiguration . class , ReviewsDatafetcher . class }) class ReviewssDatafetcherTest { @Autowired DgsQueryExecutor dgsQueryExecutor ; @Test void showsWithEntitiesQueryBuilder () { EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder (). addRepresentationAsVariable ( ShowRepresentation . newBuilder (). id ( \"1\" ). build ()). build (); GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onShow (). reviews (). starRating ()); List < Review > reviewsList = dgsQueryExecutor . executeAndExtractJsonPathAsObject ( request . serialize (), \"data['_entities'][0].reviews\" , entitiesQuery . getVariables (), new TypeRef <> () { }); assertThat ( reviewsList ). isNotNull (); assertThat ( reviewsList . size ()). isEqualTo ( 3 ); } } For more details on the API and how to set it up for tests, please refer to our documentation here . Customizing the Default Federation Resolver \u00b6 In the example above the GraphQL Show type name maps to the Java Show type. There are also cases where the GraphQL and Java type names don't match, specially when working with existing code. If any of your class names do not match your schema type names, you need to provide this class with a way to map between them. To do this, return a map from the typeMapping() method in your own implementation of the DefaultDgsFederationResolver . In the following example we map the GraphQL Show type to a ShowId Java type. @DgsComponent public class FederationResolver extends DefaultDgsFederationResolver { private final Map < Class <?> , String > types = new HashMap <> (); @PostConstruct public void init () { //The Show type is represented by the ShowId class. types . put ( ShowId . class , \"Show\" ); } @Override public Map < Class <?> , String > typeMapping () { return types ; } }","title":"Federation"},{"location":"federation/#federation-example-dgs","text":"This is a DGS example that demonstrates how to implement a federated type, and test federated queries. The source code in this guide comes from the Federation example app . We highly recommend cloning the project and use the IDE while following this guide. The example project has the following set up: A federated gateway is set up using Apollo's federation gateway libraries. The Shows DGS defines and owns the Show type. The Reviews DGS adds a reviews field to the Show type. Info If you are completely new to the DGS framework, please take a look at the DGS Getting Started guide, which also contains an introduction video. The remainder of the guide on this page assumes basic GraphQL and DGS knowledge, and focuses on more advanced use cases.","title":"Federation Example DGS"},{"location":"federation/#defining-a-federated-type","text":"The Shows DGS defines the Show type with fields id, title and releaseYear. Note that the id field is marked as the key. The example has one key, but you can have multiple kets as well @key(fields:\"fieldA fieldB\" This indicates to the gateway that the id field will be used for identifying the corresponding Show in the Shows DGS and must be specified for federated types. type Query { shows(titleFilter: String): [Show] } type Show @key(fields: \"id\") { id: ID title: String releaseYear: Int }","title":"Defining a federated type"},{"location":"federation/#extending-a-federated-type","text":"To extend a type you redefine the type in your own schema, using directive @extends to instruct that it's a type extension. @key is required to indicate the field that the gateway will use to identify the original Show for a query. In this case, the key is the id field. type Show @key(fields: \"id\") @extends { id: ID @external reviews: [Review] } type Review { starRating: Int } When redefining a type, only the id field, and the fields you're adding need to be listed. Other fields, such as title for Show type are provided by the Shows DGS and do not need to be specified unless you are using it in the schema. Federation makes sure the fields provided by all DGSs are combined into a single type for returning the results of a query. Info Don't forget to use the @external directive if you define a field that doesn't belong to your DGS, but you need to reference it.","title":"Extending a federated Type"},{"location":"federation/#implementing-a-federated-type","text":"The very first step to get started is to generate Java types that represent the schema. This is configured in build.gradle as described in the manual . When running ./gradlew build the Java types are generated into the build/generated folder, which are then automatically added to the classpath.","title":"Implementing a Federated Type"},{"location":"federation/#provide-an-entity-fetcher","text":"Let's go through an example of the following query sent to the gateway: query { shows { title reviews { starRating } } } The gateway first fetches the list of all the shows from the Shows DGS containing the title and id fields. query { shows { __typename id title } } Next, the gateway sends the following _entities query to the Reviews DGS using the list of id s from the first query: query($representations: [_Any!]!) { _entities(representations: $representations) { ... on Show { reviews { starRating } } } } This query comes with the following variables: { \"representations\" : [ { \"__typename\" : \"Show\" , \"id\" : 1 }, , { \"__typename\" : \"Show\" , \"id\" : 2 }, { \"__typename\" : \"Show\" , \"id\" : 3 }, { \"__typename\" : \"Show\" , \"id\" : 4 }, { \"__typename\" : \"Show\" , \"id\" : 5 } ] } The Reviews DGS needs to implement an entity fetcher to handle this query. An entity fetcher is responsible for creating an instance of a Show based on the representation in the _entities query above. The DGS framework does most of the heavy lifting, and all we have to do is provide the following: Full code @DgsEntityFetcher ( name = \"Show\" ) public Show show ( Map < String , Object > values ) { return new Show (( String ) values . get ( \"id\" ), null ); } Tip Remember that the Show Java type here is generated by codegen. It's generated from the schema, so it only has the fields our schema specifies.","title":"Provide an Entity Fetcher"},{"location":"federation/#providing-data-with-a-data-fetcher","text":"Now the DGS knows how to create a Show instance when an _entities query is received, we can specify how to hydrate data for the reviews field. Full code @DgsData ( parentType = \"Show\" , field = \"reviews\" ) public List < Review > reviews ( DgsDataFetchingEnvironment dataFetchingEnvironment ) { Show show = dataFetchingEnvironment . getSource (); return reviews . get ( show . getId ()); }","title":"Providing Data with a Data Fetcher"},{"location":"federation/#testing-a-federated-query","text":"You can always manually test federated queries by running the gateway and your DGS locally. You can also manually test a federated query against just your DGS, without the gateway, using the _entities query to replicate the call made to your DGS by the gateway. For automated tests, the QueryExecutor gives a way to run queries from unit tests, with very little startup overhead (in the order of 500ms). We can capture (or manually write) the _entities query that the gateway sends to the DGS. When running the query through the (locally running) gateway, the DGS will log the query that it receives. Simply copy this query in a QueryExecutor test, and that verifies the DGS in isolation. @SpringBootTest ( classes = { DgsAutoConfiguration . class , ReviewsDatafetcher . class }) class ReviewssDatafetcherTest { @Autowired DgsQueryExecutor dgsQueryExecutor ; @Test void shows () { Map < String , Object > representation = new HashMap <> (); representation . put ( \"__typename\" , \"Show\" ); representation . put ( \"id\" , \"1\" ); List < Map < String , Object >> representationsList = new ArrayList <> (); representationsList . add ( representation ); Map < String , Object > variables = new HashMap <> (); variables . put ( \"representations\" , representationsList ); List < Review > reviewsList = dgsQueryExecutor . executeAndExtractJsonPathAsObject ( \"query ($representations:[_Any!]!) {\" + \"_entities(representations:$representations) {\" + \"... on Show {\" + \" reviews {\" + \" starRating\" + \"}}}}\" , \"data['_entities'][0].reviews\" , variables , new TypeRef <> () {}); assertThat ( reviewsList ) . isNotNull () . hasSize ( 3 ); } } To help build the federated _entities query, you can also use the EntitiesGraphQLQuery available in graphql-dgs-client package along with code generation. Here is an example of the same test that uses the builder API: @SpringBootTest ( classes = { DgsAutoConfiguration . class , ReviewsDatafetcher . class }) class ReviewssDatafetcherTest { @Autowired DgsQueryExecutor dgsQueryExecutor ; @Test void showsWithEntitiesQueryBuilder () { EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder (). addRepresentationAsVariable ( ShowRepresentation . newBuilder (). id ( \"1\" ). build ()). build (); GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onShow (). reviews (). starRating ()); List < Review > reviewsList = dgsQueryExecutor . executeAndExtractJsonPathAsObject ( request . serialize (), \"data['_entities'][0].reviews\" , entitiesQuery . getVariables (), new TypeRef <> () { }); assertThat ( reviewsList ). isNotNull (); assertThat ( reviewsList . size ()). isEqualTo ( 3 ); } } For more details on the API and how to set it up for tests, please refer to our documentation here .","title":"Testing a Federated Query"},{"location":"federation/#customizing-the-default-federation-resolver","text":"In the example above the GraphQL Show type name maps to the Java Show type. There are also cases where the GraphQL and Java type names don't match, specially when working with existing code. If any of your class names do not match your schema type names, you need to provide this class with a way to map between them. To do this, return a map from the typeMapping() method in your own implementation of the DefaultDgsFederationResolver . In the following example we map the GraphQL Show type to a ShowId Java type. @DgsComponent public class FederationResolver extends DefaultDgsFederationResolver { private final Map < Class <?> , String > types = new HashMap <> (); @PostConstruct public void init () { //The Show type is represented by the ShowId class. types . put ( ShowId . class , \"Show\" ); } @Override public Map < Class <?> , String > typeMapping () { return types ; } }","title":"Customizing the Default Federation Resolver"},{"location":"generating-code-from-schema/","text":"The DGS Code Generation plugin generates code during your project\u2019s build process based on your Domain Graph Service\u2019s GraphQL schema file. The plugin generates the following: Data types for types, input types, enums and interfaces. A DgsConstants class containing the names of types and fields Example data fetchers A type safe query API that represents your queries Quick Start \u00b6 Code generation is typically integrated in the build. A Gradle plugin has always been available, and recently a Maven plugin was made available by the community. To apply the plugin, update your project\u2019s build.gradle file to include the following: // Using plugins DSL plugins { id \"com.netflix.dgs.codegen\" version \"[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]\" } Alternatively, you can set up classpath dependencies in your buildscript: buildscript { dependencies { classpath 'com.netflix.graphql.dgs.codegen:graphql-dgs-codegen-gradle:[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]' } } apply plugin: 'com.netflix.dgs.codegen' Next, you need to add the task configuration as shown here: generateJava { schemaPaths = [ \"${projectDir}/src/main/resources/schema\" ] // List of directories containing schema files packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true // Enable generating the type safe query API } NOTE: Please use the latest version of the plugin, available here The plugin adds a generateJava Gradle task that runs as part of your project\u2019s build. generateJava generates the code in the project\u2019s build/generated directory. Note that on a Kotlin project, the generateJava task generates Kotlin code by default (yes the name is confusing). This folder is automatically added to the project's classpath. Types are available as part of the package specified by the packageName .types , where you specify the value of packageName as a configuration in your build.gradle file. Please ensure that your project\u2019s sources refer to the generated code using the specified package name. generateJava generates the data fetchers and places them in build/generated-examples . NOTE: generateJava does NOT add the data fetchers that it generates to your project\u2019s sources. These fetchers serve mainly as a basic boilerplate code that require further implementation from you. You can exclude parts of the schema from code-generation by placing them in a different schema directory that is not specified as part of the schemaPaths for the plugin. Fixing the \"Could not initialize class graphql.parser.antlr.GraphqlLexer\" problem \u00b6 Gradle's plugin system uses a flat classpath for all plugins, which makes it very easy to run into classpath conflicts. One of the dependencies of the Codegen plugin is ANTLR, which is unfortuanatly used by some other plugins as well. If you see an error such as Could not initialize class graphql.parser.antlr.GraphqlLexer this typically indicates a classpath conflict. If this happens, please change the ordering of the plugins in your build script. ANTLR is typically backwards, but not forwards, compatible. For multi-module projects means you need to declare the Codegen plugin in the root build file, without applying it: plugins { id ( \"com.netflix.dgs.codegen\" ) version \"[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]\" apply false //other plugins } In the module where the plugin should be applied, you specify the plugin in the plugins block again, but without the version. plugins { id ( \"com.netflix.dgs.codegen\" ) } If you're using the old buildscript syntax, you add the plugin dependency to the root buildscript , but only apply in the module. Mapping existing types \u00b6 Codegen tries to generate a type for each type it finds in the schema, with a few exceptions. Basic scalar types - are mapped to corresponding Java/Kotlin types (String, Integer etc.) Date and time types - are mapped to corresponding java.time classes PageInfo and RelayPageInfo - are mapped to graphql.relay classes Types mapped with a typeMapping configuration When you have existing classes that you want to use instead of generating a class for a certain type, you can configure the plugin to do so using a typeMapping . The typeMapping configuration is a Map where each key is a GraphQL type and each value is a fully qualified Java/Kotlin type. generateJava { typeMapping = [ \"MyGraphQLType\" : \"com.mypackage.MyJavaType\" ] } Generating Client APIs \u00b6 The code generator can also create client API classes. You can use these classes to query data from a GraphQL endpoint using Java, or in unit tests using the QueryExecutor . The Java GraphQL Client is useful for server-to-server communication. A GraphQL Java Client is available as part of the framework. Code generation creates a field-name GraphQLQuery for each Query and Mutation field. The *GraphQLQuery query class contains fields for each parameter of the field. For each type returned by a Query or Mutation, code generation creates a *ProjectionRoot . A projection is a builder class that specifies which fields get returned. The following is an example usage of a generated API: GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new TicksGraphQLQuery . Builder () . first ( first ) . after ( after ) . build (), new TicksConnectionProjection () . edges () . node () . date () . route () . name () . votes () . starRating () . parent () . grade ()); This API was generated based on the following schema. The edges and node types are because the schema uses pagination. The API allows for a fluent style of writing queries, with almost the same feel of writing the query as a String, but with the added benefit of code completion and type safety. type Query @extends { ticks(first: Int, after: Int, allowCached: Boolean): TicksConnection } type Tick { id: ID route: Route date: LocalDate userStars: Int userRating: String leadStyle: LeadStyle comments: String } type Votes { starRating: Float nrOfVotes: Int } type Route { routeId: ID name: String grade: String style: Style pitches: Int votes: Votes location: [String] } type TicksConnection { edges: [TickEdge] } type TickEdge { cursor: String! node: Tick } Generating Query APIs for external services \u00b6 Generating a Query API like above is very useful for testing your own DGS. The same type of API can also be useful when interacting with another GraphQL service, where your code is a client of that service. This is typically done using the DGS Client . When you use code generation both for your own schema, and an internal schema, you might want different code generation configuration for both. The recommendation is to create a separate module in your project containing the schema of the external service and the codegen configuration to just generate a Query API. The following is example configuration that only generates a Query API. generateJava { schemaPaths = [ \"${projectDir}/composed-schema.graphqls\" ] packageName = \"some.other.service\" generateClient = true generateDataTypes = false skipEntityQueries = true includeQueries = [ \"hello\" ] includeMutations = [ \"\" ] shortProjectionNames = true maxProjectionDepth = 2 } Limiting generated code for Client API \u00b6 If your schema is large or has a lot of cycles, it is not ideal to generate client APIs for the entire schema, since you will end up with a large number of projections. This can cause code generation to slow down significantly, or run out of memory depending on your schema. We have a few configuration parameters that help tune this so you can limit the generation of client API to only what is required. generateJava { ... generateClient = true skipEntityQueries = true includeQueries = [ \"hello\" ] includeMutations = [ \"\" ] includeSubscriptions = [ \"\" ] maxProjectionDepth = 2 } Firstly, you can specify exactly which queries/mutation/subscriptions to generate for via includeQueries , includeMutations , and includeSubscriptions . skipEntityQueries is only used if you are constructing federated _entities queries for testing purposes, so you can also set that to restrict the amount of generated code. Finally, maxProjectionDepth will instruct codegen to stop generating beyond 2 levels of the graph from the query root. The default is 10. This will help further limit the number of projections as well. Configuring code generation \u00b6 Code generation has many configuration switches. The following table shows the Gradle configuration options, but the same options are available command line and in Maven as well. Configuration property Description Default Value schemaPaths List of files/directories containing schemas src/main/resources/schema packageName Base package name of generated code subPackageNameClient Sub package name for generated Query API client subPackageNameDatafetchers Sub package name for generated data fetchers datafetchers subPackageNameTypes Sub package name for generated data types types language Either java or kotlin Autodetected from project typeMapping A Map where each key is a GraphQL type, and the value the FQN of a Java class generateBoxedTypes Always use boxed types for primitives false (boxed types are used only for nullable fields) generateClient Generate a Query API false generateDataTypes Generate data types. Useful for only generating a Query API. Input types are still generated when generateClient is true. true generateInterfaces Generate interfaces for data classes. This is useful if you would like to extend the generated POJOs for more context and use interfaces instead of the data classes in your data fetchers. false generatedSourcesDir Build directory for Gradle build outputDir Sub directory of the generatedSourcesDir to generate into generated exampleOutputDir Directory to generate datafetcher example code to generated-examples includeQueries Generate Query API only for the given list of Query fields All queries defined in schema includeMutations Generate Query API only for the given list of Mutation fields All mutations defined in schema includeSubscriptions Generate Query API only for the given list of Subscription fields All subscriptions defined in schema skipEntityQueries Disable generating Entity queries for federated types false shortProjectionNames Shorten class names of projection types. These types are not visible to the developer. false maxProjectionDepth Maximum projection depth to generate. Useful for (federated) schemas with very deep nesting 10","title":"Code Generation"},{"location":"generating-code-from-schema/#quick-start","text":"Code generation is typically integrated in the build. A Gradle plugin has always been available, and recently a Maven plugin was made available by the community. To apply the plugin, update your project\u2019s build.gradle file to include the following: // Using plugins DSL plugins { id \"com.netflix.dgs.codegen\" version \"[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]\" } Alternatively, you can set up classpath dependencies in your buildscript: buildscript { dependencies { classpath 'com.netflix.graphql.dgs.codegen:graphql-dgs-codegen-gradle:[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]' } } apply plugin: 'com.netflix.dgs.codegen' Next, you need to add the task configuration as shown here: generateJava { schemaPaths = [ \"${projectDir}/src/main/resources/schema\" ] // List of directories containing schema files packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true // Enable generating the type safe query API } NOTE: Please use the latest version of the plugin, available here The plugin adds a generateJava Gradle task that runs as part of your project\u2019s build. generateJava generates the code in the project\u2019s build/generated directory. Note that on a Kotlin project, the generateJava task generates Kotlin code by default (yes the name is confusing). This folder is automatically added to the project's classpath. Types are available as part of the package specified by the packageName .types , where you specify the value of packageName as a configuration in your build.gradle file. Please ensure that your project\u2019s sources refer to the generated code using the specified package name. generateJava generates the data fetchers and places them in build/generated-examples . NOTE: generateJava does NOT add the data fetchers that it generates to your project\u2019s sources. These fetchers serve mainly as a basic boilerplate code that require further implementation from you. You can exclude parts of the schema from code-generation by placing them in a different schema directory that is not specified as part of the schemaPaths for the plugin.","title":"Quick Start"},{"location":"generating-code-from-schema/#fixing-the-could-not-initialize-class-graphqlparserantlrgraphqllexer-problem","text":"Gradle's plugin system uses a flat classpath for all plugins, which makes it very easy to run into classpath conflicts. One of the dependencies of the Codegen plugin is ANTLR, which is unfortuanatly used by some other plugins as well. If you see an error such as Could not initialize class graphql.parser.antlr.GraphqlLexer this typically indicates a classpath conflict. If this happens, please change the ordering of the plugins in your build script. ANTLR is typically backwards, but not forwards, compatible. For multi-module projects means you need to declare the Codegen plugin in the root build file, without applying it: plugins { id ( \"com.netflix.dgs.codegen\" ) version \"[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]\" apply false //other plugins } In the module where the plugin should be applied, you specify the plugin in the plugins block again, but without the version. plugins { id ( \"com.netflix.dgs.codegen\" ) } If you're using the old buildscript syntax, you add the plugin dependency to the root buildscript , but only apply in the module.","title":"Fixing the \"Could not initialize class graphql.parser.antlr.GraphqlLexer\" problem"},{"location":"generating-code-from-schema/#mapping-existing-types","text":"Codegen tries to generate a type for each type it finds in the schema, with a few exceptions. Basic scalar types - are mapped to corresponding Java/Kotlin types (String, Integer etc.) Date and time types - are mapped to corresponding java.time classes PageInfo and RelayPageInfo - are mapped to graphql.relay classes Types mapped with a typeMapping configuration When you have existing classes that you want to use instead of generating a class for a certain type, you can configure the plugin to do so using a typeMapping . The typeMapping configuration is a Map where each key is a GraphQL type and each value is a fully qualified Java/Kotlin type. generateJava { typeMapping = [ \"MyGraphQLType\" : \"com.mypackage.MyJavaType\" ] }","title":"Mapping existing types"},{"location":"generating-code-from-schema/#generating-client-apis","text":"The code generator can also create client API classes. You can use these classes to query data from a GraphQL endpoint using Java, or in unit tests using the QueryExecutor . The Java GraphQL Client is useful for server-to-server communication. A GraphQL Java Client is available as part of the framework. Code generation creates a field-name GraphQLQuery for each Query and Mutation field. The *GraphQLQuery query class contains fields for each parameter of the field. For each type returned by a Query or Mutation, code generation creates a *ProjectionRoot . A projection is a builder class that specifies which fields get returned. The following is an example usage of a generated API: GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new TicksGraphQLQuery . Builder () . first ( first ) . after ( after ) . build (), new TicksConnectionProjection () . edges () . node () . date () . route () . name () . votes () . starRating () . parent () . grade ()); This API was generated based on the following schema. The edges and node types are because the schema uses pagination. The API allows for a fluent style of writing queries, with almost the same feel of writing the query as a String, but with the added benefit of code completion and type safety. type Query @extends { ticks(first: Int, after: Int, allowCached: Boolean): TicksConnection } type Tick { id: ID route: Route date: LocalDate userStars: Int userRating: String leadStyle: LeadStyle comments: String } type Votes { starRating: Float nrOfVotes: Int } type Route { routeId: ID name: String grade: String style: Style pitches: Int votes: Votes location: [String] } type TicksConnection { edges: [TickEdge] } type TickEdge { cursor: String! node: Tick }","title":"Generating Client APIs"},{"location":"generating-code-from-schema/#generating-query-apis-for-external-services","text":"Generating a Query API like above is very useful for testing your own DGS. The same type of API can also be useful when interacting with another GraphQL service, where your code is a client of that service. This is typically done using the DGS Client . When you use code generation both for your own schema, and an internal schema, you might want different code generation configuration for both. The recommendation is to create a separate module in your project containing the schema of the external service and the codegen configuration to just generate a Query API. The following is example configuration that only generates a Query API. generateJava { schemaPaths = [ \"${projectDir}/composed-schema.graphqls\" ] packageName = \"some.other.service\" generateClient = true generateDataTypes = false skipEntityQueries = true includeQueries = [ \"hello\" ] includeMutations = [ \"\" ] shortProjectionNames = true maxProjectionDepth = 2 }","title":"Generating Query APIs for external services"},{"location":"generating-code-from-schema/#limiting-generated-code-for-client-api","text":"If your schema is large or has a lot of cycles, it is not ideal to generate client APIs for the entire schema, since you will end up with a large number of projections. This can cause code generation to slow down significantly, or run out of memory depending on your schema. We have a few configuration parameters that help tune this so you can limit the generation of client API to only what is required. generateJava { ... generateClient = true skipEntityQueries = true includeQueries = [ \"hello\" ] includeMutations = [ \"\" ] includeSubscriptions = [ \"\" ] maxProjectionDepth = 2 } Firstly, you can specify exactly which queries/mutation/subscriptions to generate for via includeQueries , includeMutations , and includeSubscriptions . skipEntityQueries is only used if you are constructing federated _entities queries for testing purposes, so you can also set that to restrict the amount of generated code. Finally, maxProjectionDepth will instruct codegen to stop generating beyond 2 levels of the graph from the query root. The default is 10. This will help further limit the number of projections as well.","title":"Limiting generated code for Client API"},{"location":"generating-code-from-schema/#configuring-code-generation","text":"Code generation has many configuration switches. The following table shows the Gradle configuration options, but the same options are available command line and in Maven as well. Configuration property Description Default Value schemaPaths List of files/directories containing schemas src/main/resources/schema packageName Base package name of generated code subPackageNameClient Sub package name for generated Query API client subPackageNameDatafetchers Sub package name for generated data fetchers datafetchers subPackageNameTypes Sub package name for generated data types types language Either java or kotlin Autodetected from project typeMapping A Map where each key is a GraphQL type, and the value the FQN of a Java class generateBoxedTypes Always use boxed types for primitives false (boxed types are used only for nullable fields) generateClient Generate a Query API false generateDataTypes Generate data types. Useful for only generating a Query API. Input types are still generated when generateClient is true. true generateInterfaces Generate interfaces for data classes. This is useful if you would like to extend the generated POJOs for more context and use interfaces instead of the data classes in your data fetchers. false generatedSourcesDir Build directory for Gradle build outputDir Sub directory of the generatedSourcesDir to generate into generated exampleOutputDir Directory to generate datafetcher example code to generated-examples includeQueries Generate Query API only for the given list of Query fields All queries defined in schema includeMutations Generate Query API only for the given list of Mutation fields All mutations defined in schema includeSubscriptions Generate Query API only for the given list of Subscription fields All subscriptions defined in schema skipEntityQueries Disable generating Entity queries for federated types false shortProjectionNames Shorten class names of projection types. These types are not visible to the developer. false maxProjectionDepth Maximum projection depth to generate. Useful for (federated) schemas with very deep nesting 10","title":"Configuring code generation"},{"location":"getting-started/","text":"Create a new Spring Boot application \u00b6 The DGS framework is based on Spring Boot, so get started by creating a new Spring Boot application if you don't have one already. The Spring Initializr is an easy way to do so. You can use either Gradle or Maven, Java 8 or newer or use Kotlin. We do recommend Gradle because we have a really cool code generation plugin for it! The only Spring dependency needed is Spring Web. Open the project in an IDE (Intellij recommended). Adding the DGS Framework Dependency \u00b6 Add the platform dependencies to your Gradle or Maven configuration. The com.netflix.graphql.dgs:graphql-dgs-platform-dependencies dependency is a platform/BOM dependency , which aligns the versions of the individual modules and transitive dependencies of the framework. The com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter is a Spring Boot starter that includes everything you need to get started building a DGS. If you're building on top of WebFlux , use com.netflix.graphql.dgs:graphql-dgs-webflux-starter instead. Gradle repositories { mavenCentral () } dependencies { implementation ( platform ( \"com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:latest.release\" )) implementation \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter\" } Gradle Kotlin repositories { mavenCentral () } dependencies { implementation ( platform ( \"com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:latest.release\" )) implementation ( \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter\" ) } Maven <dependencyManagement> <dependencies> <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-platform-dependencies </artifactId> <!-- The DGS BOM/platform dependency. This is the only place you set version of DGS --> <version> 4.9.16 </version> <type> pom </type> <scope> import </scope> </dependency> </dependencies> </dependencyManagement> <dependencies> <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-web </artifactId> </dependency> <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-spring-boot-starter </artifactId> </dependency> <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-test </artifactId> <scope> test </scope> </dependency> </dependencies> Important The DGS framework uses Kotlin 1.5. If you use Spring Boot Gradle Plugin 2.3 , you will have to be explicit on the Kotlin version that will be available. This plugin will downgrade the transitive 1.5 Kotlin version to 1.3 . You can be explicit by setting it via Gradle's extensions as follows: Gradle ext [ 'kotlin.version' ] = '1.4.31' Gradle Kotlin extra [ \"kotlin.version\" ] = \"1.4.31\" Creating a Schema \u00b6 The DGS framework is designed for schema first development. The framework picks up any schema files in the src/main/resources/schema folder. Create a schema file in: src/main/resources/schema/schema.graphqls . type Query { shows(titleFilter: String): [Show] } type Show { title: String releaseYear: Int } This schema allows querying for a list of shows, optionally filtering by title. Implement a Data Fetcher \u00b6 Data fetchers are responsible for returning data for a query. Create two new classes example.ShowsDataFetcher and Show and add the following code. Note that we have a Codegen plugin that can do this automatically, but in this guide we'll manually write the classes. Java @DgsComponent public class ShowsDatafetcher { private final List < Show > shows = List . of ( new Show ( \"Stranger Things\" , 2016 ), new Show ( \"Ozark\" , 2017 ), new Show ( \"The Crown\" , 2016 ), new Show ( \"Dead to Me\" , 2019 ), new Show ( \"Orange is the New Black\" , 2013 ) ); @DgsQuery public List < Show > shows ( @InputArgument String titleFilter ) { if ( titleFilter == null ) { return shows ; } return shows . stream (). filter ( s -> s . getTitle (). contains ( titleFilter )). collect ( Collectors . toList ()); } } public class Show { private final String title ; private final Integer releaseYear ; public Show ( String title , Integer releaseYear ) { this . title = title ; this . releaseYear = releaseYear ; } public String getTitle () { return title ; } public Integer getReleaseYear () { return releaseYear ; } } Kotlin @DgsComponent class ShowsDataFetcher { private val shows = listOf ( Show ( \"Stranger Things\" , 2016 ), Show ( \"Ozark\" , 2017 ), Show ( \"The Crown\" , 2016 ), Show ( \"Dead to Me\" , 2019 ), Show ( \"Orange is the New Black\" , 2013 )) @DgsQuery fun shows ( @InputArgument titleFilter : String? ): List < Show > { return if ( titleFilter != null ) { shows . filter { it . title . contains ( titleFilter ) } } else { shows } } data class Show ( val title : String , val releaseYear : Int ) } That's all the code needed, the application is ready to be tested! Test the app with GraphiQL \u00b6 Start the application and open a browser to http://localhost:8080/graphiql. GraphiQL is a query editor that comes out of the box with the DGS framework. Write the following query and tests the result. { shows { title releaseYear } } Note that unlike with REST, you have to specifically list which fields you want to get returned from your query. This is where a lot of the power from GraphQL comes from, but a surprise to many developers new to GraphQL. The GraphiQL editor is really just a UI that uses the /graphql endpoint of your service. You could now connect a UI to your backend as well, for example using React and the Apollo Client . Install the Intellij plugin \u00b6 If you are a Intellij user, there is a plugin available for DGS. The plugin supports navigation between schema files and code and many hints and quick fixes. You can install the plugin from the Jetbrains plugin repository here . Next steps \u00b6 Now that you have a first GraphQL service running, we recommend improving this further by doing the following: Use the DGS Platform BOM to align DGS Framework dependencies. Learn more about datafetchers Use the Gradle CodeGen plugin - this will generate the data types for you. Write query tests in JUnit Look at example projects","title":"Getting Started"},{"location":"getting-started/#create-a-new-spring-boot-application","text":"The DGS framework is based on Spring Boot, so get started by creating a new Spring Boot application if you don't have one already. The Spring Initializr is an easy way to do so. You can use either Gradle or Maven, Java 8 or newer or use Kotlin. We do recommend Gradle because we have a really cool code generation plugin for it! The only Spring dependency needed is Spring Web. Open the project in an IDE (Intellij recommended).","title":"Create a new Spring Boot application"},{"location":"getting-started/#adding-the-dgs-framework-dependency","text":"Add the platform dependencies to your Gradle or Maven configuration. The com.netflix.graphql.dgs:graphql-dgs-platform-dependencies dependency is a platform/BOM dependency , which aligns the versions of the individual modules and transitive dependencies of the framework. The com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter is a Spring Boot starter that includes everything you need to get started building a DGS. If you're building on top of WebFlux , use com.netflix.graphql.dgs:graphql-dgs-webflux-starter instead. Gradle repositories { mavenCentral () } dependencies { implementation ( platform ( \"com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:latest.release\" )) implementation \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter\" } Gradle Kotlin repositories { mavenCentral () } dependencies { implementation ( platform ( \"com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:latest.release\" )) implementation ( \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter\" ) } Maven <dependencyManagement> <dependencies> <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-platform-dependencies </artifactId> <!-- The DGS BOM/platform dependency. This is the only place you set version of DGS --> <version> 4.9.16 </version> <type> pom </type> <scope> import </scope> </dependency> </dependencies> </dependencyManagement> <dependencies> <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-web </artifactId> </dependency> <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-spring-boot-starter </artifactId> </dependency> <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-test </artifactId> <scope> test </scope> </dependency> </dependencies> Important The DGS framework uses Kotlin 1.5. If you use Spring Boot Gradle Plugin 2.3 , you will have to be explicit on the Kotlin version that will be available. This plugin will downgrade the transitive 1.5 Kotlin version to 1.3 . You can be explicit by setting it via Gradle's extensions as follows: Gradle ext [ 'kotlin.version' ] = '1.4.31' Gradle Kotlin extra [ \"kotlin.version\" ] = \"1.4.31\"","title":"Adding the DGS Framework Dependency"},{"location":"getting-started/#creating-a-schema","text":"The DGS framework is designed for schema first development. The framework picks up any schema files in the src/main/resources/schema folder. Create a schema file in: src/main/resources/schema/schema.graphqls . type Query { shows(titleFilter: String): [Show] } type Show { title: String releaseYear: Int } This schema allows querying for a list of shows, optionally filtering by title.","title":"Creating a Schema"},{"location":"getting-started/#implement-a-data-fetcher","text":"Data fetchers are responsible for returning data for a query. Create two new classes example.ShowsDataFetcher and Show and add the following code. Note that we have a Codegen plugin that can do this automatically, but in this guide we'll manually write the classes. Java @DgsComponent public class ShowsDatafetcher { private final List < Show > shows = List . of ( new Show ( \"Stranger Things\" , 2016 ), new Show ( \"Ozark\" , 2017 ), new Show ( \"The Crown\" , 2016 ), new Show ( \"Dead to Me\" , 2019 ), new Show ( \"Orange is the New Black\" , 2013 ) ); @DgsQuery public List < Show > shows ( @InputArgument String titleFilter ) { if ( titleFilter == null ) { return shows ; } return shows . stream (). filter ( s -> s . getTitle (). contains ( titleFilter )). collect ( Collectors . toList ()); } } public class Show { private final String title ; private final Integer releaseYear ; public Show ( String title , Integer releaseYear ) { this . title = title ; this . releaseYear = releaseYear ; } public String getTitle () { return title ; } public Integer getReleaseYear () { return releaseYear ; } } Kotlin @DgsComponent class ShowsDataFetcher { private val shows = listOf ( Show ( \"Stranger Things\" , 2016 ), Show ( \"Ozark\" , 2017 ), Show ( \"The Crown\" , 2016 ), Show ( \"Dead to Me\" , 2019 ), Show ( \"Orange is the New Black\" , 2013 )) @DgsQuery fun shows ( @InputArgument titleFilter : String? ): List < Show > { return if ( titleFilter != null ) { shows . filter { it . title . contains ( titleFilter ) } } else { shows } } data class Show ( val title : String , val releaseYear : Int ) } That's all the code needed, the application is ready to be tested!","title":"Implement a Data Fetcher"},{"location":"getting-started/#test-the-app-with-graphiql","text":"Start the application and open a browser to http://localhost:8080/graphiql. GraphiQL is a query editor that comes out of the box with the DGS framework. Write the following query and tests the result. { shows { title releaseYear } } Note that unlike with REST, you have to specifically list which fields you want to get returned from your query. This is where a lot of the power from GraphQL comes from, but a surprise to many developers new to GraphQL. The GraphiQL editor is really just a UI that uses the /graphql endpoint of your service. You could now connect a UI to your backend as well, for example using React and the Apollo Client .","title":"Test the app with GraphiQL"},{"location":"getting-started/#install-the-intellij-plugin","text":"If you are a Intellij user, there is a plugin available for DGS. The plugin supports navigation between schema files and code and many hints and quick fixes. You can install the plugin from the Jetbrains plugin repository here .","title":"Install the Intellij plugin"},{"location":"getting-started/#next-steps","text":"Now that you have a first GraphQL service running, we recommend improving this further by doing the following: Use the DGS Platform BOM to align DGS Framework dependencies. Learn more about datafetchers Use the Gradle CodeGen plugin - this will generate the data types for you. Write query tests in JUnit Look at example projects","title":"Next steps"},{"location":"mutations/","text":"The DGS framework supports Mutations with the same constructs as data fetchers, using the @DgsData annotation. The following is a simple example of a mutation: type Mutation { addRating(title: String, stars: Int):Rating } type Rating { avgStars: Float } @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( DataFetchingEnvironment dataFetchingEnvironment ) { int stars = dataFetchingEnvironment . getArgument ( \"stars\" ); if ( stars < 1 ) { throw new IllegalArgumentException ( \"Stars must be 1-5\" ); } String title = dataFetchingEnvironment . getArgument ( \"title\" ); System . out . println ( \"Rated \" + title + \" with \" + stars + \" stars\" ) ; return new Rating ( stars ); } } Note that the code above retrieves the input data for the Mutation by calling the DataFetchingEnvironment.getArgument method, just as data fetchers do for their arguments. Input Types \u00b6 In the example above the input was two standard scalar types. You can also use complex types, and you should define these as input types in your schema. An input type is almost the same as a type in GraphQL, but with some extra rules . According to the GraphQL specification an input type should always be passed to the data fetcher as a Map . This means the DataFetchingEnvironment.getArgument for an input type is a Map , and not the Java/Kotlin representation that you might have. The framework has a convenience mechanism around this, which will be discussed next. Let's first look at an example that uses DataFetchingEnvironment directly. type Mutation { addRating(input: RatingInput):Rating } input RatingInput { title: String, stars: Int } type Rating { avgStars: Float } @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( DataFetchingEnvironment dataFetchingEnvironment ) { Map < String , Object > input = dataFetchingEnvironment . getArgument ( \"input\" ); RatingInput ratingInput = new ObjectMapper (). convertValue ( input , RatingInput . class ); System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; return new Rating ( ratingInput . getStars ()); } } class RatingInput { private String title ; private int stars ; public String getTitle () { return title ; } public void setTitle ( String title ) { this . title = title ; } public int getStars () { return stars ; } public void setStars ( int stars ) { this . stars = stars ; } } Input arguments as data fetcher method parameters \u00b6 The framework makes it easier to get input arguments. You can specify arguments as method parameters of a data fetcher. @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( @InputArgument ( \"input\" ) RatingInput ratingInput ) { //No need for custom parsing anymore! System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; return new Rating ( ratingInput . getStars ()); } } The @InputArgument annotation is important to specify the name of the input argument, because arguments can be specified in any order. If no annotation is present, the framework tries to use the parameter name, but this is only possible if the code is compiled with specific compiler settings . Input type parameters can be combined with a DataFetchingEnvironment parameter. @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( @InputArgument ( \"input\" ) RatingInput ratingInput , DataFetchingEnvironment dfe ) { //No need for custom parsing anymore! System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; System . out . println ( \"DataFetchingEnvironment: \" + dfe . getArgument ( ratingInput )); return new Rating ( ratingInput . getStars ()); } }","title":"Mutations"},{"location":"mutations/#input-types","text":"In the example above the input was two standard scalar types. You can also use complex types, and you should define these as input types in your schema. An input type is almost the same as a type in GraphQL, but with some extra rules . According to the GraphQL specification an input type should always be passed to the data fetcher as a Map . This means the DataFetchingEnvironment.getArgument for an input type is a Map , and not the Java/Kotlin representation that you might have. The framework has a convenience mechanism around this, which will be discussed next. Let's first look at an example that uses DataFetchingEnvironment directly. type Mutation { addRating(input: RatingInput):Rating } input RatingInput { title: String, stars: Int } type Rating { avgStars: Float } @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( DataFetchingEnvironment dataFetchingEnvironment ) { Map < String , Object > input = dataFetchingEnvironment . getArgument ( \"input\" ); RatingInput ratingInput = new ObjectMapper (). convertValue ( input , RatingInput . class ); System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; return new Rating ( ratingInput . getStars ()); } } class RatingInput { private String title ; private int stars ; public String getTitle () { return title ; } public void setTitle ( String title ) { this . title = title ; } public int getStars () { return stars ; } public void setStars ( int stars ) { this . stars = stars ; } }","title":"Input Types"},{"location":"mutations/#input-arguments-as-data-fetcher-method-parameters","text":"The framework makes it easier to get input arguments. You can specify arguments as method parameters of a data fetcher. @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( @InputArgument ( \"input\" ) RatingInput ratingInput ) { //No need for custom parsing anymore! System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; return new Rating ( ratingInput . getStars ()); } } The @InputArgument annotation is important to specify the name of the input argument, because arguments can be specified in any order. If no annotation is present, the framework tries to use the parameter name, but this is only possible if the code is compiled with specific compiler settings . Input type parameters can be combined with a DataFetchingEnvironment parameter. @DgsComponent public class RatingMutation { @DgsData ( parentType = \"Mutation\" , field = \"addRating\" ) public Rating addRating ( @InputArgument ( \"input\" ) RatingInput ratingInput , DataFetchingEnvironment dfe ) { //No need for custom parsing anymore! System . out . println ( \"Rated \" + ratingInput . getTitle () + \" with \" + ratingInput . getStars () + \" stars\" ) ; System . out . println ( \"DataFetchingEnvironment: \" + dfe . getArgument ( ratingInput )); return new Rating ( ratingInput . getStars ()); } }","title":"Input arguments as data fetcher method parameters"},{"location":"query-execution-testing/","text":"The DGS framework allows you to write lightweight tests that partially bootstrap the framework, just enough to run queries. Example \u00b6 Before writing tests, make sure that JUnit is enabled. If you created a project with Spring Initializr this configuration should already be there. Gradle dependencies { testImplementation 'org.springframework.boot:spring-boot-starter-test' } test { useJUnitPlatform () } Gradle Kotlin tasks . withType < Test > { useJUnitPlatform () } Maven <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-test </artifactId> <scope> test </scope> </dependency> Create a test class with the following contents to test the ShowsDatafetcher from the getting started example. Java import com.netflix.graphql.dgs.DgsQueryExecutor ; import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration ; import org.junit.jupiter.api.Test ; import org.springframework.beans.factory.annotation.Autowired ; import org.springframework.boot.test.context.SpringBootTest ; import java.util.List ; import static org.assertj.core.api.Assertions.assertThat ; @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDatafetcher . class }) class ShowsDatafetcherTest { @Autowired DgsQueryExecutor dgsQueryExecutor ; @Test void shows () { List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( \" { shows { title releaseYear }}\" , \"data.shows[*].title\" ); assertThat ( titles ). contains ( \"Ozark\" ); } } Kotlin import com.netflix.graphql.dgs.DgsQueryExecutor import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration import org.assertj.core.api.Assertions.assertThat import org.junit.jupiter.api.Test import org.springframework.beans.factory.annotation.Autowired import org.springframework.boot.test.context.SpringBootTest @SpringBootTest ( classes = [ DgsAutoConfiguration :: class , ShowsDataFetcher :: class ] ) class ShowsDataFetcherTest { @Autowired lateinit var dgsQueryExecutor : DgsQueryExecutor @Test fun shows () { val titles : List < String > = dgsQueryExecutor . executeAndExtractJsonPath ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent (), \"data.shows[*].title\" ) assertThat ( titles ). contains ( \"Ozark\" ) } } The @SpringBootTest annotation makes this a Spring test. If you do not specify classes explicitly, Spring will start all components on the classpath. For a small application this is fine, but for applications with components that are \"expensive\" to start we can speed up the test by only adding the classes we need for the test. In this case we need to include the DGS framework itself using the DgsAutoConfiguration class, and the ShowsDatafetcher . To execute queries, inject DgsQueryExecutor in the test. This interface has several methods to execute a query and get back the result. It executes the exact same code as a query on the /graphql endpoint would, but you won\u2019t have to deal with HTTP in your tests. The DgsQueryExecutor methods accept JSON paths, so that the methods can easily extract just the data from the response that you\u2019re interested in. The DgsQueryExecutor also includes methods (e.g. executeAndExtractJsonPathAsObject ) to deserialize the result to a Java class, which uses Jackson under the hood. The JSON paths are supported by the open source JsonPath library . Write a few more tests, for example to verify the behavior with using the titleFilter of ShowsDatafetcher . You can run the tests from the IDE, or from Gradle/Maven, just like any JUnit test. Building GraphQL Queries for Tests \u00b6 In the examples shown previously, we handcrafted the query string. This is simple enough for queries that are small and straightforward. However, constructing longer query strings can be tedious, specially in Java without support for multi-line Strings. For this, we can use the GraphQLQueryRequest to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . Now we can write a test that uses GraphQLQueryRequest to build the query and extract the response using GraphQLResponse . Java @Test public void showsWithQueryApi () { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ShowsGraphQLQuery . Builder (). titleFilter ( \"Oz\" ). build (), new ShowsProjectionRoot (). title () ); List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ); assertThat ( titles ). containsExactly ( \"Ozark\" ); } Kotlin @Test fun showsWithQueryApi () { val graphQLQueryRequest = GraphQLQueryRequest ( ShowsGraphQLQuery . Builder () . titleFilter ( \"Oz\" ) . build (), ShowsProjectionRoot (). title ()) val titles = dgsQueryExecutor . executeAndExtractJsonPath < List < String >> ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ) assertThat ( titles ). containsExactly ( \"Ozark\" ) } The GraphQLQueryRequest is available as part of the graphql-client module and is used to build the query string, and wrap the response respectively. You can also refer to the GraphQLClient JavaDoc for more details on the list of supported methods. Mocking External Service Calls in Tests \u00b6 It\u2019s not uncommon for a data fetcher to talk to external systems such as a database or a gRPC service. If it does so within a test, this adds two problems: It adds latency; your tests are going to run slower when they make a lot of external calls. It adds flakiness: Did your code introduce a bug, or did something go wrong in the external system? In many cases it\u2019s better to mock these external services. Spring already has good support for doing so with the @Mockbean annotation, which you can leverage in your DGS tests. Example \u00b6 Let's update the Shows example to load shows from an external data source, instead of just returning a fixed list. For the sake of the example we'll just move the fixed list of shows to a new class that we'll annotate @Service . The data fetcher is updated to use the injected ShowsService . Java public interface ShowsService { List < Show > shows (); } @Service public class ShowsServiceImpl implements ShowsService { @Override public List < Show > shows () { return List . of ( new Show ( \"Stranger Things\" , 2016 ), new Show ( \"Ozark\" , 2017 ), new Show ( \"The Crown\" , 2016 ), new Show ( \"Dead to Me\" , 2019 ), new Show ( \"Orange is the New Black\" , 2013 ) ); } } Kotlin interface ShowsService { fun shows (): List < ShowsDataFetcher . Show > } @Service class BasicShowsService : ShowsService { override fun shows (): List < ShowsDataFetcher . Show > { return listOf ( ShowsDataFetcher . Show ( \"Stranger Things\" , 2016 ), ShowsDataFetcher . Show ( \"Ozark\" , 2017 ), ShowsDataFetcher . Show ( \"The Crown\" , 2016 ), ShowsDataFetcher . Show ( \"Dead to Me\" , 2019 ), ShowsDataFetcher . Show ( \"Orange is the New Black\" , 2013 ) ) } } @DgsComponent class ShowsDataFetcher { @Autowired lateinit var showsService : ShowsService @DgsData ( parentType = \"Query\" , field = \"shows\" ) fun shows ( @InputArgument ( \"titleFilter\" ) titleFilter : String? ): List < Show > { return if ( titleFilter != null ) { showsService . shows (). filter { it . title . contains ( titleFilter ) } } else { showsService . shows () } } } For the sake of the example the shows are still in-memory, imagine that the service would actually call out to an external data store. Let's try to mock this service in the test! Java @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDataFetcher . class }) public class ShowsDataFetcherTests { @Autowired DgsQueryExecutor dgsQueryExecutor ; @MockBean ShowsService showsService ; @BeforeEach public void before () { Mockito . when ( showsService . shows ()). thenAnswer ( invocation -> List . of ( new Show ( \"mock title\" , 2020 ))); } @Test public void showsWithQueryApi () { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ShowsGraphQLQuery . Builder (). build (), new ShowsProjectionRoot (). title () ); List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ); assertThat ( titles ). containsExactly ( \"mock title\" ); } } Kotlin @SpringBootTest ( classes = [ DgsAutoConfiguration :: class , ShowsDataFetcher :: class ] ) class ShowsDataFetcherTest { @Autowired lateinit var dgsQueryExecutor : DgsQueryExecutor @MockBean lateinit var showsService : ShowsService @BeforeEach fun before () { Mockito . `when` ( showsService . shows ()). thenAnswer { listOf ( ShowsDataFetcher . Show ( \"mock title\" , 2020 )) } } @Test fun shows () { val titles : List < String > = dgsQueryExecutor . executeAndExtractJsonPath ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent (), \"data.shows[*].title\" ) assertThat ( titles ). contains ( \"mock title\" ) } } Testing Exceptions \u00b6 The tests you wrote so far are mostly happy paths. Failure scenarios are also easy to test. We use the mocked example from above to force an exception. Java @Test void showsWithException () { Mockito . when ( showsService . shows ()). thenThrow ( new RuntimeException ( \"nothing to see here\" )); ExecutionResult result = dgsQueryExecutor . execute ( \" { shows { title releaseYear }}\" ); assertThat ( result . getErrors ()). isNotEmpty (); assertThat ( result . getErrors (). get ( 0 ). getMessage ()). isEqualTo ( \"java.lang.RuntimeException: nothing to see here\" ); } Kotlin @Test fun showsWithException () { Mockito . `when` ( showsService . shows ()). thenThrow ( RuntimeException ( \"nothing to see here\" )) val result = dgsQueryExecutor . execute ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent ()) assertThat ( result . errors ). isNotEmpty assertThat ( result . errors [ 0 ] . message ). isEqualTo ( \"java.lang.RuntimeException: nothing to see here\" ) } When an error happens while executing the query, the errors are wrapped in a QueryException . This allows you to easily inspect the error. The message of the QueryException is the concatenation of all the errors. The getErrors() method gives access to the individual errors for further inspection. Testing with Client \u00b6 If you are interested in testing the web layer as well, you can use the Java GraphQL Client . Following is a simple example: Java import static org.junit.jupiter.api.Assertions.assertTrue ; import java.util.HashMap ; import java.util.List ; import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration ; import com.netflix.graphql.dgs.client.DefaultGraphQLClient ; import com.netflix.graphql.dgs.client.GraphQLClient ; import com.netflix.graphql.dgs.client.GraphQLResponse ; import com.netflix.graphql.dgs.client.HttpResponse ; import org.junit.jupiter.api.Test ; import org.springframework.boot.test.context.SpringBootTest ; import org.springframework.boot.test.context.SpringBootTest.WebEnvironment ; import org.springframework.boot.web.server.LocalServerPort ; import org.springframework.http.HttpEntity ; import org.springframework.http.HttpHeaders ; import org.springframework.http.HttpMethod ; import org.springframework.http.ResponseEntity ; import org.springframework.web.client.RestTemplate ; @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDatafetcher . class }, webEnvironment = WebEnvironment . RANDOM_PORT ) class ShowsDatafetcherTest { private GraphQLClient client ; private RestTemplate restTemplate ; public ShowsDatafetcherTest ( @LocalServerPort Integer port ) { this . client = new DefaultGraphQLClient ( \"http://localhost:\" + port . toString () + \"/graphql\" ); this . restTemplate = new RestTemplate (); } @Test void shows () { String query = \"{ shows { title releaseYear }}\" ; // Read more about executeQuery() at https://netflix.github.io/dgs/advanced/java-client/ GraphQLResponse response = this . client . executeQuery ( query , new HashMap <> (), ( url , headers , body ) -> { HttpHeaders requestHeaders = new HttpHeaders (); headers . forEach ( requestHeaders :: put ); ResponseEntity < String > exchange = this . restTemplate . exchange ( url , HttpMethod . POST , new HttpEntity < String > ( body , requestHeaders ), String . class ); return new HttpResponse ( exchange . getStatusCodeValue (), exchange . getBody ()); }); List <?> titles = response . extractValueAsObject ( \"shows[*].title\" , List . class ); assertTrue ( titles . contains ( \"Ozark\" )); } }","title":"Testing"},{"location":"query-execution-testing/#example","text":"Before writing tests, make sure that JUnit is enabled. If you created a project with Spring Initializr this configuration should already be there. Gradle dependencies { testImplementation 'org.springframework.boot:spring-boot-starter-test' } test { useJUnitPlatform () } Gradle Kotlin tasks . withType < Test > { useJUnitPlatform () } Maven <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-test </artifactId> <scope> test </scope> </dependency> Create a test class with the following contents to test the ShowsDatafetcher from the getting started example. Java import com.netflix.graphql.dgs.DgsQueryExecutor ; import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration ; import org.junit.jupiter.api.Test ; import org.springframework.beans.factory.annotation.Autowired ; import org.springframework.boot.test.context.SpringBootTest ; import java.util.List ; import static org.assertj.core.api.Assertions.assertThat ; @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDatafetcher . class }) class ShowsDatafetcherTest { @Autowired DgsQueryExecutor dgsQueryExecutor ; @Test void shows () { List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( \" { shows { title releaseYear }}\" , \"data.shows[*].title\" ); assertThat ( titles ). contains ( \"Ozark\" ); } } Kotlin import com.netflix.graphql.dgs.DgsQueryExecutor import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration import org.assertj.core.api.Assertions.assertThat import org.junit.jupiter.api.Test import org.springframework.beans.factory.annotation.Autowired import org.springframework.boot.test.context.SpringBootTest @SpringBootTest ( classes = [ DgsAutoConfiguration :: class , ShowsDataFetcher :: class ] ) class ShowsDataFetcherTest { @Autowired lateinit var dgsQueryExecutor : DgsQueryExecutor @Test fun shows () { val titles : List < String > = dgsQueryExecutor . executeAndExtractJsonPath ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent (), \"data.shows[*].title\" ) assertThat ( titles ). contains ( \"Ozark\" ) } } The @SpringBootTest annotation makes this a Spring test. If you do not specify classes explicitly, Spring will start all components on the classpath. For a small application this is fine, but for applications with components that are \"expensive\" to start we can speed up the test by only adding the classes we need for the test. In this case we need to include the DGS framework itself using the DgsAutoConfiguration class, and the ShowsDatafetcher . To execute queries, inject DgsQueryExecutor in the test. This interface has several methods to execute a query and get back the result. It executes the exact same code as a query on the /graphql endpoint would, but you won\u2019t have to deal with HTTP in your tests. The DgsQueryExecutor methods accept JSON paths, so that the methods can easily extract just the data from the response that you\u2019re interested in. The DgsQueryExecutor also includes methods (e.g. executeAndExtractJsonPathAsObject ) to deserialize the result to a Java class, which uses Jackson under the hood. The JSON paths are supported by the open source JsonPath library . Write a few more tests, for example to verify the behavior with using the titleFilter of ShowsDatafetcher . You can run the tests from the IDE, or from Gradle/Maven, just like any JUnit test.","title":"Example"},{"location":"query-execution-testing/#building-graphql-queries-for-tests","text":"In the examples shown previously, we handcrafted the query string. This is simple enough for queries that are small and straightforward. However, constructing longer query strings can be tedious, specially in Java without support for multi-line Strings. For this, we can use the GraphQLQueryRequest to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . Now we can write a test that uses GraphQLQueryRequest to build the query and extract the response using GraphQLResponse . Java @Test public void showsWithQueryApi () { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ShowsGraphQLQuery . Builder (). titleFilter ( \"Oz\" ). build (), new ShowsProjectionRoot (). title () ); List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ); assertThat ( titles ). containsExactly ( \"Ozark\" ); } Kotlin @Test fun showsWithQueryApi () { val graphQLQueryRequest = GraphQLQueryRequest ( ShowsGraphQLQuery . Builder () . titleFilter ( \"Oz\" ) . build (), ShowsProjectionRoot (). title ()) val titles = dgsQueryExecutor . executeAndExtractJsonPath < List < String >> ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ) assertThat ( titles ). containsExactly ( \"Ozark\" ) } The GraphQLQueryRequest is available as part of the graphql-client module and is used to build the query string, and wrap the response respectively. You can also refer to the GraphQLClient JavaDoc for more details on the list of supported methods.","title":"Building GraphQL Queries for Tests"},{"location":"query-execution-testing/#mocking-external-service-calls-in-tests","text":"It\u2019s not uncommon for a data fetcher to talk to external systems such as a database or a gRPC service. If it does so within a test, this adds two problems: It adds latency; your tests are going to run slower when they make a lot of external calls. It adds flakiness: Did your code introduce a bug, or did something go wrong in the external system? In many cases it\u2019s better to mock these external services. Spring already has good support for doing so with the @Mockbean annotation, which you can leverage in your DGS tests.","title":"Mocking External Service Calls in Tests"},{"location":"query-execution-testing/#example_1","text":"Let's update the Shows example to load shows from an external data source, instead of just returning a fixed list. For the sake of the example we'll just move the fixed list of shows to a new class that we'll annotate @Service . The data fetcher is updated to use the injected ShowsService . Java public interface ShowsService { List < Show > shows (); } @Service public class ShowsServiceImpl implements ShowsService { @Override public List < Show > shows () { return List . of ( new Show ( \"Stranger Things\" , 2016 ), new Show ( \"Ozark\" , 2017 ), new Show ( \"The Crown\" , 2016 ), new Show ( \"Dead to Me\" , 2019 ), new Show ( \"Orange is the New Black\" , 2013 ) ); } } Kotlin interface ShowsService { fun shows (): List < ShowsDataFetcher . Show > } @Service class BasicShowsService : ShowsService { override fun shows (): List < ShowsDataFetcher . Show > { return listOf ( ShowsDataFetcher . Show ( \"Stranger Things\" , 2016 ), ShowsDataFetcher . Show ( \"Ozark\" , 2017 ), ShowsDataFetcher . Show ( \"The Crown\" , 2016 ), ShowsDataFetcher . Show ( \"Dead to Me\" , 2019 ), ShowsDataFetcher . Show ( \"Orange is the New Black\" , 2013 ) ) } } @DgsComponent class ShowsDataFetcher { @Autowired lateinit var showsService : ShowsService @DgsData ( parentType = \"Query\" , field = \"shows\" ) fun shows ( @InputArgument ( \"titleFilter\" ) titleFilter : String? ): List < Show > { return if ( titleFilter != null ) { showsService . shows (). filter { it . title . contains ( titleFilter ) } } else { showsService . shows () } } } For the sake of the example the shows are still in-memory, imagine that the service would actually call out to an external data store. Let's try to mock this service in the test! Java @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDataFetcher . class }) public class ShowsDataFetcherTests { @Autowired DgsQueryExecutor dgsQueryExecutor ; @MockBean ShowsService showsService ; @BeforeEach public void before () { Mockito . when ( showsService . shows ()). thenAnswer ( invocation -> List . of ( new Show ( \"mock title\" , 2020 ))); } @Test public void showsWithQueryApi () { GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ShowsGraphQLQuery . Builder (). build (), new ShowsProjectionRoot (). title () ); List < String > titles = dgsQueryExecutor . executeAndExtractJsonPath ( graphQLQueryRequest . serialize (), \"data.shows[*].title\" ); assertThat ( titles ). containsExactly ( \"mock title\" ); } } Kotlin @SpringBootTest ( classes = [ DgsAutoConfiguration :: class , ShowsDataFetcher :: class ] ) class ShowsDataFetcherTest { @Autowired lateinit var dgsQueryExecutor : DgsQueryExecutor @MockBean lateinit var showsService : ShowsService @BeforeEach fun before () { Mockito . `when` ( showsService . shows ()). thenAnswer { listOf ( ShowsDataFetcher . Show ( \"mock title\" , 2020 )) } } @Test fun shows () { val titles : List < String > = dgsQueryExecutor . executeAndExtractJsonPath ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent (), \"data.shows[*].title\" ) assertThat ( titles ). contains ( \"mock title\" ) } }","title":"Example"},{"location":"query-execution-testing/#testing-exceptions","text":"The tests you wrote so far are mostly happy paths. Failure scenarios are also easy to test. We use the mocked example from above to force an exception. Java @Test void showsWithException () { Mockito . when ( showsService . shows ()). thenThrow ( new RuntimeException ( \"nothing to see here\" )); ExecutionResult result = dgsQueryExecutor . execute ( \" { shows { title releaseYear }}\" ); assertThat ( result . getErrors ()). isNotEmpty (); assertThat ( result . getErrors (). get ( 0 ). getMessage ()). isEqualTo ( \"java.lang.RuntimeException: nothing to see here\" ); } Kotlin @Test fun showsWithException () { Mockito . `when` ( showsService . shows ()). thenThrow ( RuntimeException ( \"nothing to see here\" )) val result = dgsQueryExecutor . execute ( \"\"\" { shows { title releaseYear } } \"\"\" . trimIndent ()) assertThat ( result . errors ). isNotEmpty assertThat ( result . errors [ 0 ] . message ). isEqualTo ( \"java.lang.RuntimeException: nothing to see here\" ) } When an error happens while executing the query, the errors are wrapped in a QueryException . This allows you to easily inspect the error. The message of the QueryException is the concatenation of all the errors. The getErrors() method gives access to the individual errors for further inspection.","title":"Testing Exceptions"},{"location":"query-execution-testing/#testing-with-client","text":"If you are interested in testing the web layer as well, you can use the Java GraphQL Client . Following is a simple example: Java import static org.junit.jupiter.api.Assertions.assertTrue ; import java.util.HashMap ; import java.util.List ; import com.netflix.graphql.dgs.autoconfig.DgsAutoConfiguration ; import com.netflix.graphql.dgs.client.DefaultGraphQLClient ; import com.netflix.graphql.dgs.client.GraphQLClient ; import com.netflix.graphql.dgs.client.GraphQLResponse ; import com.netflix.graphql.dgs.client.HttpResponse ; import org.junit.jupiter.api.Test ; import org.springframework.boot.test.context.SpringBootTest ; import org.springframework.boot.test.context.SpringBootTest.WebEnvironment ; import org.springframework.boot.web.server.LocalServerPort ; import org.springframework.http.HttpEntity ; import org.springframework.http.HttpHeaders ; import org.springframework.http.HttpMethod ; import org.springframework.http.ResponseEntity ; import org.springframework.web.client.RestTemplate ; @SpringBootTest ( classes = { DgsAutoConfiguration . class , ShowsDatafetcher . class }, webEnvironment = WebEnvironment . RANDOM_PORT ) class ShowsDatafetcherTest { private GraphQLClient client ; private RestTemplate restTemplate ; public ShowsDatafetcherTest ( @LocalServerPort Integer port ) { this . client = new DefaultGraphQLClient ( \"http://localhost:\" + port . toString () + \"/graphql\" ); this . restTemplate = new RestTemplate (); } @Test void shows () { String query = \"{ shows { title releaseYear }}\" ; // Read more about executeQuery() at https://netflix.github.io/dgs/advanced/java-client/ GraphQLResponse response = this . client . executeQuery ( query , new HashMap <> (), ( url , headers , body ) -> { HttpHeaders requestHeaders = new HttpHeaders (); headers . forEach ( requestHeaders :: put ); ResponseEntity < String > exchange = this . restTemplate . exchange ( url , HttpMethod . POST , new HttpEntity < String > ( body , requestHeaders ), String . class ); return new HttpResponse ( exchange . getStatusCodeValue (), exchange . getBody ()); }); List <?> titles = response . extractValueAsObject ( \"shows[*].title\" , List . class ); assertTrue ( titles . contains ( \"Ozark\" )); } }","title":"Testing with Client"},{"location":"scalars/","text":"It is easy to add a custom scalar type in the DGS framework: Create a class that implements the graphql.schema.Coercing interface and annotate it with the @DgsScalar annotation. Also make sure the scalar type is defined in your [GraphQL] schema! For example, this is a simple LocalDateTime implementation: @DgsScalar ( name = \"DateTime\" ) public class DateTimeScalar implements Coercing < LocalDateTime , String > { @Override public String serialize ( Object dataFetcherResult ) throws CoercingSerializeException { if ( dataFetcherResult instanceof LocalDateTime ) { return (( LocalDateTime ) dataFetcherResult ). format ( DateTimeFormatter . ISO_DATE_TIME ); } else { throw new CoercingSerializeException ( \"Not a valid DateTime\" ); } } @Override public LocalDateTime parseValue ( Object input ) throws CoercingParseValueException { return LocalDateTime . parse ( input . toString (), DateTimeFormatter . ISO_DATE_TIME ); } @Override public LocalDateTime parseLiteral ( Object input ) throws CoercingParseLiteralException { if ( input instanceof StringValue ) { return LocalDateTime . parse ((( StringValue ) input ). getValue (), DateTimeFormatter . ISO_DATE_TIME ); } throw new CoercingParseLiteralException ( \"Value is not a valid ISO date time\" ); } } Schema: scalar DateTime Important If you are Building GraphQL Queries for Tests , make sure to pass your custom scalars in GraphQLQueryRequest constructor as descibred in Scalars in DGS Client Registering Custom Scalars \u00b6 In more recent versions of graphql-java (>v15.0) some scalars , most notably the Long scalar, are no longer available by default. These are non standard scalar that are difficult for clients (e.g. JavaScript) to handle reliably. As a result of the deprecation, you will need to add them explicitly, and to do this you have a few options. Tip Go to the graphql-java-extended-scalars project page to see the full list of scalars supported by this library. There you will also find examples of the scalars used in both schemas as well as example queries. Automatically Register Scalar Extensions via graphql-dgs-extended-scalars \u00b6 The DGS Framework, as of version 3.9.2, has the graphql-dgs-extended-scalars module. This module provides an auto-configuration that will register automatically the scalar extensions defined in the com.graphql-java:graphql-java-extended-scalars library. To use it you need to... Add the com.netflix.graphql.dgs:graphql-dgs-extended-scalars dependency to your build. If you are using the DGS BOM you don't need to specify a version for it, the BOM will recommend one. Define the scalar in your schema Other mapping available on extended scalars doc The graphql-java-extended-scalars module offers a few knobs you can use to turn off registration. Property Description dgs.graphql.extensions.scalars.time-dates.enabled If set to false , it will not register the DateTime, Date, and Time scalar extensions. dgs.graphql.extensions.scalars.objects.enabled If set to false , it will not register the Object, Json, Url, and Locale scalar extensions. dgs.graphql.extensions.scalars.numbers.enabled If set to false , it will not register all numeric scalar extensions such as PositiveInt, NegativeInt, etc. dgs.graphql.extensions.scalars.chars.enabled If set to false , it will not register the GraphQLChar extension. dgs.graphql.extensions.scalars.enabled If set to false , it will disable automatic registration of all of the above. Important Are you using the code generation Gradle Plugin ? The graphql-java-extended-scalars module doesn't modify the behavior of such plugin, you will need to explicit define the type mappings . For example, let's say we want to use both the Url and PositiveInt Scalars. You will have to add the mapping below to your build file. Gradle generateJava { typeMapping = [ \"Url\" : \"java.net.URL\" , \"PositiveInt\" : \"java.lang.Integer\" ] } Gradle Kotlin generateJava { typeMapping = mutableMapOf ( \"Url\" to \"java.net.URL\" , \"PositiveInt\" to \"java.lang.Integer\" ) } Register Scalar Extensions via DgsRuntimeWiring \u00b6 You can also register the Scalar Extensions manually. To do so you need to... Add the com.graphql-java:graphql-java-extended-scalars dependency to your build. If you are using the DGS BOM you don't need to specify a version for it, the BOM will recommend one. Define the scalar in your schema Register the scalar. Here is an example of how you would set that up: Schema: scalar Long You can register the Long scalar manually with the DGS Framework as shown here: @DgsComponent public class LongScalarRegistration { @DgsRuntimeWiring public RuntimeWiring . Builder addScalar ( RuntimeWiring . Builder builder ) { return builder . scalar ( Scalars . GraphQLLong ); } }","title":"Adding Custom Scalars"},{"location":"scalars/#registering-custom-scalars","text":"In more recent versions of graphql-java (>v15.0) some scalars , most notably the Long scalar, are no longer available by default. These are non standard scalar that are difficult for clients (e.g. JavaScript) to handle reliably. As a result of the deprecation, you will need to add them explicitly, and to do this you have a few options. Tip Go to the graphql-java-extended-scalars project page to see the full list of scalars supported by this library. There you will also find examples of the scalars used in both schemas as well as example queries.","title":"Registering Custom Scalars"},{"location":"scalars/#automatically-register-scalar-extensions-via-graphql-dgs-extended-scalars","text":"The DGS Framework, as of version 3.9.2, has the graphql-dgs-extended-scalars module. This module provides an auto-configuration that will register automatically the scalar extensions defined in the com.graphql-java:graphql-java-extended-scalars library. To use it you need to... Add the com.netflix.graphql.dgs:graphql-dgs-extended-scalars dependency to your build. If you are using the DGS BOM you don't need to specify a version for it, the BOM will recommend one. Define the scalar in your schema Other mapping available on extended scalars doc The graphql-java-extended-scalars module offers a few knobs you can use to turn off registration. Property Description dgs.graphql.extensions.scalars.time-dates.enabled If set to false , it will not register the DateTime, Date, and Time scalar extensions. dgs.graphql.extensions.scalars.objects.enabled If set to false , it will not register the Object, Json, Url, and Locale scalar extensions. dgs.graphql.extensions.scalars.numbers.enabled If set to false , it will not register all numeric scalar extensions such as PositiveInt, NegativeInt, etc. dgs.graphql.extensions.scalars.chars.enabled If set to false , it will not register the GraphQLChar extension. dgs.graphql.extensions.scalars.enabled If set to false , it will disable automatic registration of all of the above. Important Are you using the code generation Gradle Plugin ? The graphql-java-extended-scalars module doesn't modify the behavior of such plugin, you will need to explicit define the type mappings . For example, let's say we want to use both the Url and PositiveInt Scalars. You will have to add the mapping below to your build file. Gradle generateJava { typeMapping = [ \"Url\" : \"java.net.URL\" , \"PositiveInt\" : \"java.lang.Integer\" ] } Gradle Kotlin generateJava { typeMapping = mutableMapOf ( \"Url\" to \"java.net.URL\" , \"PositiveInt\" to \"java.lang.Integer\" ) }","title":"Automatically Register Scalar Extensions via graphql-dgs-extended-scalars"},{"location":"scalars/#register-scalar-extensions-via-dgsruntimewiring","text":"You can also register the Scalar Extensions manually. To do so you need to... Add the com.graphql-java:graphql-java-extended-scalars dependency to your build. If you are using the DGS BOM you don't need to specify a version for it, the BOM will recommend one. Define the scalar in your schema Register the scalar. Here is an example of how you would set that up: Schema: scalar Long You can register the Long scalar manually with the DGS Framework as shown here: @DgsComponent public class LongScalarRegistration { @DgsRuntimeWiring public RuntimeWiring . Builder addScalar ( RuntimeWiring . Builder builder ) { return builder . scalar ( Scalars . GraphQLLong ); } }","title":"Register Scalar Extensions via DgsRuntimeWiring"},{"location":"videos/","text":"Videos \u00b6 DGS Framework - GraphQL for Spring Boot @ OpenValue \u00b6 DGS new features and tips - March 2021 \u00b6 Tech tips (short videos about a specific feature) \u00b6 Tech Tip #1 - Input Arguments \u00b6 Tech Tip #2 - Code generation \u00b6 Tech Tip #3 - Building federated queries for tests using codegen \u00b6 Tech Tip #4 - Type Resolvers \u00b6","title":"Videos"},{"location":"videos/#videos","text":"","title":"Videos"},{"location":"videos/#dgs-framework-graphql-for-spring-boot-openvalue","text":"","title":"DGS Framework - GraphQL for Spring Boot @ OpenValue"},{"location":"videos/#dgs-new-features-and-tips-march-2021","text":"","title":"DGS new features and tips - March 2021"},{"location":"videos/#tech-tips-short-videos-about-a-specific-feature","text":"","title":"Tech tips (short videos about a specific feature)"},{"location":"videos/#tech-tip-1-input-arguments","text":"","title":"Tech Tip #1 - Input Arguments"},{"location":"videos/#tech-tip-2-code-generation","text":"","title":"Tech Tip #2 - Code generation"},{"location":"videos/#tech-tip-3-building-federated-queries-for-tests-using-codegen","text":"","title":"Tech Tip #3 - Building federated queries for tests using codegen"},{"location":"videos/#tech-tip-4-type-resolvers","text":"","title":"Tech Tip #4 - Type Resolvers"},{"location":"advanced/context-passing/","text":"Commonly, the datafetcher for a nested field requires properties from its parent object to load its data. Take the following schema example. type Query { shows: [Show] } type Show { # The showId may or may not be there, depending on the scenario. showId: ID title: String reviews: [Review] } type Review { starRating: Int } Let's assume our backend already has methods available to Shows and Reviews from a datastore. Note that for this example, the getShows method does not return reviews. The getReviewsForShow method loads reviews for a show, given the show id. interface ShowsService { List < Show > getShows (); //Does not include reviews List < Review > getReviewsForShow ( int showId ); } For this scenario, you likely want to have two datafetchers, one for shows and one for reviews. There are different options for implementing the datafetcher, which each has pros and cons depending on the scenario. We'll go over the different scenarios and options. The easy case - Using getSource \u00b6 In the example schema the Show type has a showId . Having the showId available makes loading reviews in a separate datafetcher very easy. The DataFetcherEnvironment has a getSource() method that returns the parent loaded for a field. @DgsData ( parentType = \"Query\" , field = \"shows\" ) List < Show > shows () { return showsService . getShows (); } @DgsData ( parentType = \"Show\" , field = \"reviews\" ) List < Review > reviews ( DgsDataFetchingEnvironment dfe ) { Show show = dfe . getSource (); return showsService . getReviewsForShow ( show . getShowId ()); } This example is the easiest and most common scenario, but only possible if the showId field is available on the Show type. No showId - Use an internal type \u00b6 Sometimes you don't want to expose the showId field in the schema, or our types are not set up to carry this field for other reasons. For example, for 1:1 and N:1 it's not that common to model the relationship as a key in the Java model. Whatever the reason is, the scenario we look at here is that we don't have the showId available on Show . If we remove showId from the schema and use codegen, the generated Show type will not have showId field either. Not having the showId field makes loading reviews a bit more complicated, because now we can't get the showId from the Show type using getSource() . The getShowsForService(int showId) method indicates that internally (probably in the datastore), a show does have an id. In such a scenario, we likely have a different internal representation of Show than exposed in the API. For the remainder of the example, we'll call this the InternalShow type which the ShowsService returns. interface ShowsService { List < InternalShow > getShows (); //Does not include reviews List < Review > getReviewsForShow ( int showId ); } class InternalShow { int showId ; Sring title ; // getters and setters } However, the Show type in the GraphQL schema does not have a showId . type Show { title: String reviews: [Review] } The good news is that you can have fields set on your internal instances either not in the schema, or not queried. The framework drops this extra data while creating a response. We could create an extra ShowWithId wrapper class that either extends or composes the (generated) Show type, and adds a showId field. class ShowWithId { String showId ; Show show ; //Delegate all show fields String getTitle () { return show . getTitle (); } static ShowWithId fromInternalShow ( InternalShow internal ) { //Create Show instance and store id. } .... } The shows datafetcher should return the wrapper type instead of just Show . @DgsData ( parentType = \"Query\" , field = \"shows\" ) List < Show > shows () { return showsService . getShows (). stream () . map ( ShowWithId :: fromInternalShow ) . collect ( Collectors . toList ()); } As said, the extra field doesn't affect the response to the client at all. No showId - Use local context \u00b6 Using wrapper types works well when the schema type and internal type are mostly similar. An alternative way is to use \"local context\". A datafetcher can return a DataFetcherResult<T> , which contains data , errors and localContext . The data and errors fields are the data and errors you would normally return directly from your datafetcher. The localContext field can hold any data you want to pass down to child datafetchers. The localContext can be retrieved in the child datafetcher from the DataFetchingEnvironment and is passed down to the next level child datafetchers if not overwritten. In the following example the shows datafetcher creates a DataFetcherResult that holds the list of Show instances (not the internal type). The localContext is set to a map with each show as key, and the showId as value. @DgsData ( parentType = \"Query\" , field = \"shows\" ) public DataFetcherResult < List < Show >> shows ( @InputArgument ( \"titleFilter\" ) String titleFilter ) { List < InternalShow > internalShows = getShows ( titleFilter ); List < Show > shows = internalShows . stream () . map ( s -> Show . newBuilder (). title ( s . getTitle ()). build ()) . collect ( Collectors . toList ()); return DataFetcherResult . < List < Show >> newResult () . data ( shows ) . localContext ( internalShows . stream () . collect ( Collectors . toMap ( s -> Show . newBuilder (). title ( s . getTitle ()). build (), InternalShow :: getId ))) . build (); } private List < InternalShow > getShows ( String titleFilter ) { if ( titleFilter == null ) { return showsService . shows (); } return showsService . shows (). stream (). filter ( s -> s . getTitle (). contains ( titleFilter )). collect ( Collectors . toList ()); } The reviews datafetcher can now use a combination of the getSource and getLocalContext methods to get the showId for a show. @DgsData ( parentType = \"Show\" , field = \"reviews\" ) public CompletableFuture < List < Review >> reviews ( DgsDataFetchingEnvironment dfe ) { Map < Show , Integer > shows = dfe . getLocalContext (); Show show = dfe . getSource (); return showsService . getReviewsForShow ( shows . get ( show )); } A benefit of this approach is that in contrast with getSource , the localContext gets passed down to the next level of child datafechers as well. Pre-loading \u00b6 Suppose our internal datastore allows us to load shows and reviews together efficiently, for example using a SQL join query. In that case, it can be more efficient to pre-load reviews in the shows datafetcher. In the shows datafetcher we can check if the reviews field was included in the query, and only if it is, load the reviews. Depending on the Java/Kotlin types we use, the Show type may or may not have a reviews field. If we use DGS codegen it will, and we can just set the reviews field when creating the Show instances in the shows datafetcher. If the type returned by the shows datafetcher does not have a reviews field, we can again use the localContext to pass on the review data to a reviews datafetcher. Below is an example of pre-loading and using localContext . @DgsData ( parentType = \"Query\" , field = \"shows\" ) public DataFetcherResult < List < Show >> shows ( DataFetchingEnvironment dfe ) { List < Show > shows = showsService . shows (); if ( dfe . getSelectionSet (). contains ( \"reviews\" )) { Map < Integer , List < Review >> reviewsForShows = reviewsService . reviewsForShows ( shows . stream (). map ( Show :: getId ). collect ( Collectors . toList ())); return DataFetcherResult . < List < Show >> newResult () . data ( shows ) . localContext ( reviewsForShows ) . build (); } else { return DataFetcherResult . < List < Show >> newResult (). data ( shows ). build (); } } @DgsData ( parentType = \"Show\" , field = \"reviews\" ) public List < Review > reviews ( DgsDataFetchingEnvironment dfe ) { Show show = dfe . getSource (); //Load the reviews from the pre-loaded localContext. Map < Integer , List < Review >> reviewsForShows = dfe . getLocalContext (); return reviewsForShows . get ( show . getId ()); }","title":"Nested data fetchers"},{"location":"advanced/context-passing/#the-easy-case-using-getsource","text":"In the example schema the Show type has a showId . Having the showId available makes loading reviews in a separate datafetcher very easy. The DataFetcherEnvironment has a getSource() method that returns the parent loaded for a field. @DgsData ( parentType = \"Query\" , field = \"shows\" ) List < Show > shows () { return showsService . getShows (); } @DgsData ( parentType = \"Show\" , field = \"reviews\" ) List < Review > reviews ( DgsDataFetchingEnvironment dfe ) { Show show = dfe . getSource (); return showsService . getReviewsForShow ( show . getShowId ()); } This example is the easiest and most common scenario, but only possible if the showId field is available on the Show type.","title":"The easy case - Using getSource"},{"location":"advanced/context-passing/#no-showid-use-an-internal-type","text":"Sometimes you don't want to expose the showId field in the schema, or our types are not set up to carry this field for other reasons. For example, for 1:1 and N:1 it's not that common to model the relationship as a key in the Java model. Whatever the reason is, the scenario we look at here is that we don't have the showId available on Show . If we remove showId from the schema and use codegen, the generated Show type will not have showId field either. Not having the showId field makes loading reviews a bit more complicated, because now we can't get the showId from the Show type using getSource() . The getShowsForService(int showId) method indicates that internally (probably in the datastore), a show does have an id. In such a scenario, we likely have a different internal representation of Show than exposed in the API. For the remainder of the example, we'll call this the InternalShow type which the ShowsService returns. interface ShowsService { List < InternalShow > getShows (); //Does not include reviews List < Review > getReviewsForShow ( int showId ); } class InternalShow { int showId ; Sring title ; // getters and setters } However, the Show type in the GraphQL schema does not have a showId . type Show { title: String reviews: [Review] } The good news is that you can have fields set on your internal instances either not in the schema, or not queried. The framework drops this extra data while creating a response. We could create an extra ShowWithId wrapper class that either extends or composes the (generated) Show type, and adds a showId field. class ShowWithId { String showId ; Show show ; //Delegate all show fields String getTitle () { return show . getTitle (); } static ShowWithId fromInternalShow ( InternalShow internal ) { //Create Show instance and store id. } .... } The shows datafetcher should return the wrapper type instead of just Show . @DgsData ( parentType = \"Query\" , field = \"shows\" ) List < Show > shows () { return showsService . getShows (). stream () . map ( ShowWithId :: fromInternalShow ) . collect ( Collectors . toList ()); } As said, the extra field doesn't affect the response to the client at all.","title":"No showId - Use an internal type"},{"location":"advanced/context-passing/#no-showid-use-local-context","text":"Using wrapper types works well when the schema type and internal type are mostly similar. An alternative way is to use \"local context\". A datafetcher can return a DataFetcherResult<T> , which contains data , errors and localContext . The data and errors fields are the data and errors you would normally return directly from your datafetcher. The localContext field can hold any data you want to pass down to child datafetchers. The localContext can be retrieved in the child datafetcher from the DataFetchingEnvironment and is passed down to the next level child datafetchers if not overwritten. In the following example the shows datafetcher creates a DataFetcherResult that holds the list of Show instances (not the internal type). The localContext is set to a map with each show as key, and the showId as value. @DgsData ( parentType = \"Query\" , field = \"shows\" ) public DataFetcherResult < List < Show >> shows ( @InputArgument ( \"titleFilter\" ) String titleFilter ) { List < InternalShow > internalShows = getShows ( titleFilter ); List < Show > shows = internalShows . stream () . map ( s -> Show . newBuilder (). title ( s . getTitle ()). build ()) . collect ( Collectors . toList ()); return DataFetcherResult . < List < Show >> newResult () . data ( shows ) . localContext ( internalShows . stream () . collect ( Collectors . toMap ( s -> Show . newBuilder (). title ( s . getTitle ()). build (), InternalShow :: getId ))) . build (); } private List < InternalShow > getShows ( String titleFilter ) { if ( titleFilter == null ) { return showsService . shows (); } return showsService . shows (). stream (). filter ( s -> s . getTitle (). contains ( titleFilter )). collect ( Collectors . toList ()); } The reviews datafetcher can now use a combination of the getSource and getLocalContext methods to get the showId for a show. @DgsData ( parentType = \"Show\" , field = \"reviews\" ) public CompletableFuture < List < Review >> reviews ( DgsDataFetchingEnvironment dfe ) { Map < Show , Integer > shows = dfe . getLocalContext (); Show show = dfe . getSource (); return showsService . getReviewsForShow ( shows . get ( show )); } A benefit of this approach is that in contrast with getSource , the localContext gets passed down to the next level of child datafechers as well.","title":"No showId - Use local context"},{"location":"advanced/context-passing/#pre-loading","text":"Suppose our internal datastore allows us to load shows and reviews together efficiently, for example using a SQL join query. In that case, it can be more efficient to pre-load reviews in the shows datafetcher. In the shows datafetcher we can check if the reviews field was included in the query, and only if it is, load the reviews. Depending on the Java/Kotlin types we use, the Show type may or may not have a reviews field. If we use DGS codegen it will, and we can just set the reviews field when creating the Show instances in the shows datafetcher. If the type returned by the shows datafetcher does not have a reviews field, we can again use the localContext to pass on the review data to a reviews datafetcher. Below is an example of pre-loading and using localContext . @DgsData ( parentType = \"Query\" , field = \"shows\" ) public DataFetcherResult < List < Show >> shows ( DataFetchingEnvironment dfe ) { List < Show > shows = showsService . shows (); if ( dfe . getSelectionSet (). contains ( \"reviews\" )) { Map < Integer , List < Review >> reviewsForShows = reviewsService . reviewsForShows ( shows . stream (). map ( Show :: getId ). collect ( Collectors . toList ())); return DataFetcherResult . < List < Show >> newResult () . data ( shows ) . localContext ( reviewsForShows ) . build (); } else { return DataFetcherResult . < List < Show >> newResult (). data ( shows ). build (); } } @DgsData ( parentType = \"Show\" , field = \"reviews\" ) public List < Review > reviews ( DgsDataFetchingEnvironment dfe ) { Show show = dfe . getSource (); //Load the reviews from the pre-loaded localContext. Map < Integer , List < Review >> reviewsForShows = dfe . getLocalContext (); return reviewsForShows . get ( show . getId ()); }","title":"Pre-loading"},{"location":"advanced/custom-datafetcher-context/","text":"Each data fetcher in [GraphQL] Java has a context. A data fetcher gets access to its context by calling DataFetchingEnvironment.getContext() . This is a common mechanism to pass request context to data fetchers and data loaders. The DGS framework has its own DgsContext implementation, which is used for log instrumentation among other things. It is designed in such a way that you can extend it with your own custom context. To create a custom context, implement a Spring bean of type DgsCustomContextBuilder . Write the build() method so that it creates an instance of the type that represents your custom context object: @Component public class MyContextBuilder implements DgsCustomContextBuilder < MyContext > { @Override public MyContext build () { return new MyContext (); } } public class MyContext { private final String customState = \"Custom state!\" ; public String getCustomState () { return customState ; } } A data fetcher can now retrieve the context by calling the getCustomContext() method: @DgsData ( parentType = \"Query\" , field = \"withContext\" ) public String withContext ( DataFetchingEnvironment dfe ) { MyContext customContext = DgsContext . getCustomContext ( dfe ); return customContext . getCustomState (); } Similarly, custom context can be used in a DataLoader. @DgsDataLoader ( name = \"exampleLoaderWithContext\" ) public class ExampleLoaderWithContext implements BatchLoaderWithContext < String , String > { @Override public CompletionStage < List < String >> load ( List < String > keys , BatchLoaderEnvironment environment ) { MyContext context = DgsContext . getCustomContext ( environment ); return CompletableFuture . supplyAsync (() -> keys . stream (). map ( key -> context . getCustomState () + \" \" + key ). collect ( Collectors . toList ())); } }","title":"Data Fetching Context"},{"location":"advanced/custom-object-mapper/","text":"In certain applications, you may want to customize the serialization of the GraphQL response, for e.g., by adding additional modules. This can be done by setting up a custom object mapper that overrides the default provided by the DGS framework in your application's configuration class. Note that this mapper is only used for serialization of outgoing GraphQL responses. Also, this mechanism will NOT affect how custom scalars are serialized - those would rely on your scalar implementation's serialization logic handled by graphql-java To create a custom object mapper, implement a Spring bean of type ObjectMapper with @Qualifier(\"dgsObjectMapper\") . @Bean @Qualifier ( \"dgsObjectMapper\" ) public ObjectMapper dgsObjectMapper () { ObjectMapper customMapper = new ObjectMapper () customMapper . registerModule ( JavaTimeModule ()); return customMapper ; }","title":"Custom Object Mapper"},{"location":"advanced/dynamic-schemas/","text":"We strongly recommend primarily using schema-first development. Most DGSs have a schema file and use the declarative, annotation-based programming model to create data fetchers and such. That said, there are scenarios where generating the schema from another source, possibly dynamically, is required. Creating a schema from code \u00b6 Create a schema from code by using the @DgsTypeDefinitionRegistry annotation. Use the @DgsTypeDefinitionRegistry on methods inside a @DgsComponent class to provide a TypeDefinitionRegistry . The TypeDefinitionRegistry is part of the graphql-java API. You use a TypeDefinitionRegistry to programmatically define a schema. Note that you can mix static schema files with one or more DgsTypeDefinitionRegistry methods. The result is a schema with all the registered types merged. This way, you can primarily use a schema-first workflow while falling back to @DgsTypeDefinitionRegistry to add some dynamic parts to the schema. The following is an example of a DgsTypeDefinitionRegistry . @DgsComponent public class DynamicTypeDefinitions { @DgsTypeDefinitionRegistry public TypeDefinitionRegistry registry () { TypeDefinitionRegistry typeDefinitionRegistry = new TypeDefinitionRegistry (); ObjectTypeExtensionDefinition query = ObjectTypeExtensionDefinition . newObjectTypeExtensionDefinition (). name ( \"Query\" ). fieldDefinition ( FieldDefinition . newFieldDefinition (). name ( \"randomNumber\" ). type ( new TypeName ( \"Int\" )). build () ). build (); typeDefinitionRegistry . add ( query ); return typeDefinitionRegistry ; } } This TypeDefinitionRegistry creates a field randomNumber on the Query object type. Creating datafetchers programmatically \u00b6 If you're creating schema elements dynamically, it's likely you also need to create datafetchers dynamically. You can use the @DgsCodeRegistry annotation to add datafetchers programmatically. A method annotated @DgsCodeRegistry takes two arguments: GraphQLCodeRegistry.Builder codeRegistryBuilder TypeDefinitionRegistry registry The method must return the modified GraphQLCodeRegistry.Builder. The following is an example of a programmatically created datafetcher for the field created in the previous example. @DgsComponent public class DynamicDataFetcher { @DgsCodeRegistry public GraphQLCodeRegistry . Builder registry ( GraphQLCodeRegistry . Builder codeRegistryBuilder , TypeDefinitionRegistry registry ) { DataFetcher < Integer > df = ( dfe ) -> new Random (). nextInt (); FieldCoordinates coordinates = FieldCoordinates . coordinates ( \"Query\" , \"randomNumber\" ); return codeRegistryBuilder . dataFetcher ( coordinates , df ); } } Changing schemas at runtime \u00b6 It's helpful to combine creating schemas/datafetchers at runtime with dynamically re-loading the schema in some very rare use-cases. You can achieve this by implementing your own ReloadSchemaIndicator . You can use an external signal (e.g., reading from a message queue) to have the framework recreate the schema by executing the @DgsTypeDefinitionRegistry and @DgsCodeRegistry again. If these methods create the schema based on external input, you have a system that can dynamically rewire its API. For obvious reasons, this isn't an approach that you should use for typical APIs; stable APIs are generally the thing to aim for!","title":"Dynamic schemas"},{"location":"advanced/dynamic-schemas/#creating-a-schema-from-code","text":"Create a schema from code by using the @DgsTypeDefinitionRegistry annotation. Use the @DgsTypeDefinitionRegistry on methods inside a @DgsComponent class to provide a TypeDefinitionRegistry . The TypeDefinitionRegistry is part of the graphql-java API. You use a TypeDefinitionRegistry to programmatically define a schema. Note that you can mix static schema files with one or more DgsTypeDefinitionRegistry methods. The result is a schema with all the registered types merged. This way, you can primarily use a schema-first workflow while falling back to @DgsTypeDefinitionRegistry to add some dynamic parts to the schema. The following is an example of a DgsTypeDefinitionRegistry . @DgsComponent public class DynamicTypeDefinitions { @DgsTypeDefinitionRegistry public TypeDefinitionRegistry registry () { TypeDefinitionRegistry typeDefinitionRegistry = new TypeDefinitionRegistry (); ObjectTypeExtensionDefinition query = ObjectTypeExtensionDefinition . newObjectTypeExtensionDefinition (). name ( \"Query\" ). fieldDefinition ( FieldDefinition . newFieldDefinition (). name ( \"randomNumber\" ). type ( new TypeName ( \"Int\" )). build () ). build (); typeDefinitionRegistry . add ( query ); return typeDefinitionRegistry ; } } This TypeDefinitionRegistry creates a field randomNumber on the Query object type.","title":"Creating a schema from code"},{"location":"advanced/dynamic-schemas/#creating-datafetchers-programmatically","text":"If you're creating schema elements dynamically, it's likely you also need to create datafetchers dynamically. You can use the @DgsCodeRegistry annotation to add datafetchers programmatically. A method annotated @DgsCodeRegistry takes two arguments: GraphQLCodeRegistry.Builder codeRegistryBuilder TypeDefinitionRegistry registry The method must return the modified GraphQLCodeRegistry.Builder. The following is an example of a programmatically created datafetcher for the field created in the previous example. @DgsComponent public class DynamicDataFetcher { @DgsCodeRegistry public GraphQLCodeRegistry . Builder registry ( GraphQLCodeRegistry . Builder codeRegistryBuilder , TypeDefinitionRegistry registry ) { DataFetcher < Integer > df = ( dfe ) -> new Random (). nextInt (); FieldCoordinates coordinates = FieldCoordinates . coordinates ( \"Query\" , \"randomNumber\" ); return codeRegistryBuilder . dataFetcher ( coordinates , df ); } }","title":"Creating datafetchers programmatically"},{"location":"advanced/dynamic-schemas/#changing-schemas-at-runtime","text":"It's helpful to combine creating schemas/datafetchers at runtime with dynamically re-loading the schema in some very rare use-cases. You can achieve this by implementing your own ReloadSchemaIndicator . You can use an external signal (e.g., reading from a message queue) to have the framework recreate the schema by executing the @DgsTypeDefinitionRegistry and @DgsCodeRegistry again. If these methods create the schema based on external input, you have a system that can dynamically rewire its API. For obvious reasons, this isn't an approach that you should use for typical APIs; stable APIs are generally the thing to aim for!","title":"Changing schemas at runtime"},{"location":"advanced/federated-testing/","text":"Federation allows you to extend or reference existing types in a graph. Your DGS fulfills a part of the query based on the schema that is owned by your DGS, while the gateway is responsible for fetching data from other DGSs. Testing Federated Queries without the Gateway \u00b6 You can test federated queries for your DGS in isolation by replicating the format of the query that the gateway would send to your DGS. This does not involve the gateway, and thus the parts of the query response that your DGS is not responsible for will not be hydrated. This technique is useful if you want to verify that your DGS is able to return the appropriate data, in response to a federated query. Let's look at an example of a schema that extends the Movie type that is already defined by another DGS. type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } Now you want to verify that your DGS is able to fulfill the Movie query by hydrating the script field based on the movieId field. Normally, the gateway would send an _entities query in the following format: query ( $representations : [ _Any !]! ) { _entities ( representations : $representations ) { ... on Movie { movieId script { title } }}} The representations input is a variable map containing the __typename field set to Movie and movieId set to a value, e.g., 12345 . You can now set up a Query Executor test by either manually constructing the query, or you can generate the federated query using the Entities Query Builder API available through client code generation . Here is an example of a test that uses a manually constructed _entities query for Movie : @Test void federatedMovieQuery () throws IOException { String query = \"query ($representations: [_Any!]!) {\" + \"_entities(representations: $representations) {\" + \"... on Movie {\" + \"movieId \" + \"script { title }\" + \"}}}\" ; Map < String , Object > variables = new HashMap <> (); Map < String , Object > representation = new HashMap <> (); representation . put ( \"__typename\" , \"Movie\" ); representation . put ( \"movieId\" , 1 ); variables . put ( \"representations\" , List . of ( representation )); DocumentContext context = queryExecutor . executeAndGetDocumentContext ( query , variables ); GraphQLResponse response = new GraphQLResponse ( context . jsonString ()); Movie movie = response . extractValueAsObject ( \"data._entities[0]\" , Movie . class ); assertThat ( movie . getScript (). getTitle ()). isEqualTo ( \"Top Secret\" ); } Using the Entities Query Builder API \u00b6 Alternatively, you can generate the federated query by using EntitiesGraphQLQuery to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . You will also need to add com.netflix.graphql.dgs:graphql-dgs-client:latest.release dependency to build.gradle. Now we can write a test that uses EntitiesGraphQLQuery along with GraphQLQueryRequest and EntitiesProjectionRoot to build the query. Finally, you can also extract the response using GraphQLResponse . This set up is shown here: @Test void federatedMovieQueryAPI () throws IOException { // constructs the _entities query with variable $representations containing a // movie representation that represents { __typename: \"Movie\" movieId: 12345 } EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder () . addRepresentationAsVariable ( MovieRepresentation . newBuilder (). movieId ( 1122 ). build () ) . build (); // sets up the query and the field selection set using the EntitiesProjectionRoot GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onMovie (). movieId (). script (). title ()); String query = request . serialize (); // pass in the constructed _entities query with the variable map containing representations DocumentContext context = queryExecutor . executeAndGetDocumentContext ( query , entitiesQuery . getVariables ()); GraphQLResponse response = new GraphQLResponse ( context . jsonString ()); Movie movie = response . extractValueAsObject ( \"data._entities[0]\" , Movie . class ); assertThat ( movie . getScript (). getTitle ()). isEqualTo ( \"Top Secret\" ); } Check out this video for a demo on how to configure and write the above test.","title":"Federated Testing"},{"location":"advanced/federated-testing/#testing-federated-queries-without-the-gateway","text":"You can test federated queries for your DGS in isolation by replicating the format of the query that the gateway would send to your DGS. This does not involve the gateway, and thus the parts of the query response that your DGS is not responsible for will not be hydrated. This technique is useful if you want to verify that your DGS is able to return the appropriate data, in response to a federated query. Let's look at an example of a schema that extends the Movie type that is already defined by another DGS. type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } Now you want to verify that your DGS is able to fulfill the Movie query by hydrating the script field based on the movieId field. Normally, the gateway would send an _entities query in the following format: query ( $representations : [ _Any !]! ) { _entities ( representations : $representations ) { ... on Movie { movieId script { title } }}} The representations input is a variable map containing the __typename field set to Movie and movieId set to a value, e.g., 12345 . You can now set up a Query Executor test by either manually constructing the query, or you can generate the federated query using the Entities Query Builder API available through client code generation . Here is an example of a test that uses a manually constructed _entities query for Movie : @Test void federatedMovieQuery () throws IOException { String query = \"query ($representations: [_Any!]!) {\" + \"_entities(representations: $representations) {\" + \"... on Movie {\" + \"movieId \" + \"script { title }\" + \"}}}\" ; Map < String , Object > variables = new HashMap <> (); Map < String , Object > representation = new HashMap <> (); representation . put ( \"__typename\" , \"Movie\" ); representation . put ( \"movieId\" , 1 ); variables . put ( \"representations\" , List . of ( representation )); DocumentContext context = queryExecutor . executeAndGetDocumentContext ( query , variables ); GraphQLResponse response = new GraphQLResponse ( context . jsonString ()); Movie movie = response . extractValueAsObject ( \"data._entities[0]\" , Movie . class ); assertThat ( movie . getScript (). getTitle ()). isEqualTo ( \"Top Secret\" ); }","title":"Testing Federated Queries without the Gateway"},{"location":"advanced/federated-testing/#using-the-entities-query-builder-api","text":"Alternatively, you can generate the federated query by using EntitiesGraphQLQuery to build the graphql request in combination with the code generation plugin to generate the classes needed to use the request builder. This provides a convenient type-safe way to build your queries. To set up code generation to generate the required classes to use for building your queries, follow the instructions here . You will also need to add com.netflix.graphql.dgs:graphql-dgs-client:latest.release dependency to build.gradle. Now we can write a test that uses EntitiesGraphQLQuery along with GraphQLQueryRequest and EntitiesProjectionRoot to build the query. Finally, you can also extract the response using GraphQLResponse . This set up is shown here: @Test void federatedMovieQueryAPI () throws IOException { // constructs the _entities query with variable $representations containing a // movie representation that represents { __typename: \"Movie\" movieId: 12345 } EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder () . addRepresentationAsVariable ( MovieRepresentation . newBuilder (). movieId ( 1122 ). build () ) . build (); // sets up the query and the field selection set using the EntitiesProjectionRoot GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onMovie (). movieId (). script (). title ()); String query = request . serialize (); // pass in the constructed _entities query with the variable map containing representations DocumentContext context = queryExecutor . executeAndGetDocumentContext ( query , entitiesQuery . getVariables ()); GraphQLResponse response = new GraphQLResponse ( context . jsonString ()); Movie movie = response . extractValueAsObject ( \"data._entities[0]\" , Movie . class ); assertThat ( movie . getScript (). getTitle ()). isEqualTo ( \"Top Secret\" ); } Check out this video for a demo on how to configure and write the above test.","title":"Using the Entities Query Builder API"},{"location":"advanced/file-uploads/","text":"In GraphQL, you model a file upload operation as a GraphQL mutation request from a client to your DGS. The following sections describe how you implement file uploads and downloads using a Multipart POST request. For more context on file uploads and best practices, see Apollo Server File Upload Best Practices by Khalil Stemmler from Apollo Blog . Multipart File Upload \u00b6 A multipart request is an HTTP request that contains multiple parts in a single request: the mutation query, file data, JSON objects, and whatever else you like. You can use Apollo\u2019s upload client, or even a simple cURL, to send along a stream of file data using a multipart request that you model in your schema as a Mutation. See GraphQL multipart request specification for the specification of a multipart POST request for uploading files using GraphQL mutations. The DGS framework supports the Upload scalar with which you can specify files in your mutation query as a MultipartFile . When you send a multipart request for file upload, the framework processes each part and assembles the final GraphQL query that it hands to your data fetcher for further processing. Here is an example of a Mutation query that uploads a file to your DGS: scalar Upload extend type Mutation { uploadScriptWithMultipartPOST(input: Upload!): Boolean } Note that you need to declare the Upload scalar in your schema, although the implementation is provided by the DGS framework. In your DGS, add a data fetcher to handle this as a MultipartFile as shown here: @DgsData ( parentType = DgsConstants . MUTATION . TYPE_NAME , field = \"uploadScriptWithMultipartPOST\" ) public boolean uploadScript ( DataFetchingEnvironment dfe ) throws IOException { // NOTE: Cannot use @InputArgument or Object Mapper to convert to class, because MultipartFile cannot be // deserialized MultipartFile file = dfe . getArgument ( \"input\" ); String content = new String ( file . getBytes ()); return ! content . isEmpty (); } Note that you will not be able to use a Jackson object mapper to deserialize a type that contains a MultipartFile , so you will need to explicitly get the file argument from your input. On your client, you can use apollo-upload-client to send your Mutation query as a multipart POST request with file data. Here\u2019s how you configure your link: import { createUploadLink } from 'apollo-upload-client' const uploadLink = createUploadLink ({ uri : uri }) const authedClient = authLink && new ApolloClient ({ link : uploadLink )), cache : new InMemoryCache () }) Once you set this up, set up your Mutation query and the pass the file that the user selected as a variable: // query for file uploads using multipart post const UploadScriptMultipartMutation_gql = gql ` mutation uploadScriptWithMultipartPOST($input: Upload!) { uploadScriptWithMultipartPOST(input: $input) } ` ; function MultipartScriptUpload () { const [ uploadScriptMultipartMutation , { loading : mutationLoading , error : mutationError , data : mutationData , }, ] = useMutation ( UploadScriptMultipartMutation_gql ); const [ scriptMultipartInput , setScriptMultipartInput ] = useState < any > (); const onSubmitScriptMultipart = () => { const fileInput = scriptMultipartInput . files [ 0 ]; uploadScriptMultipartMutation ({ variables : { input : fileInput }, }); }; return ( < div > < h3 > Upload script using multipart HTTP POST < /h3> < form onSubmit = { e => { e . preventDefault (); onSubmitScriptMultipart (); }} > < label > < input type = \"file\" ref = { ref => { setScriptMultipartInput ( ref ! ); }} /> < /label> < br /> < br /> < button type = \"submit\" > Submit < /button> < /form> < /div> ); }","title":"File Uploads"},{"location":"advanced/file-uploads/#multipart-file-upload","text":"A multipart request is an HTTP request that contains multiple parts in a single request: the mutation query, file data, JSON objects, and whatever else you like. You can use Apollo\u2019s upload client, or even a simple cURL, to send along a stream of file data using a multipart request that you model in your schema as a Mutation. See GraphQL multipart request specification for the specification of a multipart POST request for uploading files using GraphQL mutations. The DGS framework supports the Upload scalar with which you can specify files in your mutation query as a MultipartFile . When you send a multipart request for file upload, the framework processes each part and assembles the final GraphQL query that it hands to your data fetcher for further processing. Here is an example of a Mutation query that uploads a file to your DGS: scalar Upload extend type Mutation { uploadScriptWithMultipartPOST(input: Upload!): Boolean } Note that you need to declare the Upload scalar in your schema, although the implementation is provided by the DGS framework. In your DGS, add a data fetcher to handle this as a MultipartFile as shown here: @DgsData ( parentType = DgsConstants . MUTATION . TYPE_NAME , field = \"uploadScriptWithMultipartPOST\" ) public boolean uploadScript ( DataFetchingEnvironment dfe ) throws IOException { // NOTE: Cannot use @InputArgument or Object Mapper to convert to class, because MultipartFile cannot be // deserialized MultipartFile file = dfe . getArgument ( \"input\" ); String content = new String ( file . getBytes ()); return ! content . isEmpty (); } Note that you will not be able to use a Jackson object mapper to deserialize a type that contains a MultipartFile , so you will need to explicitly get the file argument from your input. On your client, you can use apollo-upload-client to send your Mutation query as a multipart POST request with file data. Here\u2019s how you configure your link: import { createUploadLink } from 'apollo-upload-client' const uploadLink = createUploadLink ({ uri : uri }) const authedClient = authLink && new ApolloClient ({ link : uploadLink )), cache : new InMemoryCache () }) Once you set this up, set up your Mutation query and the pass the file that the user selected as a variable: // query for file uploads using multipart post const UploadScriptMultipartMutation_gql = gql ` mutation uploadScriptWithMultipartPOST($input: Upload!) { uploadScriptWithMultipartPOST(input: $input) } ` ; function MultipartScriptUpload () { const [ uploadScriptMultipartMutation , { loading : mutationLoading , error : mutationError , data : mutationData , }, ] = useMutation ( UploadScriptMultipartMutation_gql ); const [ scriptMultipartInput , setScriptMultipartInput ] = useState < any > (); const onSubmitScriptMultipart = () => { const fileInput = scriptMultipartInput . files [ 0 ]; uploadScriptMultipartMutation ({ variables : { input : fileInput }, }); }; return ( < div > < h3 > Upload script using multipart HTTP POST < /h3> < form onSubmit = { e => { e . preventDefault (); onSubmitScriptMultipart (); }} > < label > < input type = \"file\" ref = { ref => { setScriptMultipartInput ( ref ! ); }} /> < /label> < br /> < br /> < button type = \"submit\" > Submit < /button> < /form> < /div> ); }","title":"Multipart File Upload"},{"location":"advanced/instrumentation/","text":"Adding instrumentation for tracing and logging \u00b6 It can be extremely valuable to add tracing, metrics and logging to your GraphQL API. At Netflix we publish tracing spans and metrics for each datafetcher to our distributed tracing/metrics backends, and log queries and query results to our logging backend. The implementations we use at Netflix are highly specific for our infrastructure, but it's easy to add your own to the framework! Internally the DGS framework uses GraphQL Java . GraphQL Java supports the concept of instrumentation . In the DGS framework we can easily add one or more instrumentation classes by implementing the graphql.execution.instrumentation.Instrumentation interface and register the class as @Component . The easiest way to implement the Instrumentation interface is to extend graphql.execution.instrumentation.SimpleInstrumentation . The following is an example of an implementation that outputs the execution time for each data fetcher, and the total query execution time, to the logs. Most likely you would want to replace the log output with writing to your tracing/metrics backend. Note that the code example accounts for async data fetchers. If we wouldn't do this, the result for an async data fetcher would always be 0, because the actual processing happens later. Java @Component public class ExampleTracingInstrumentation extends SimpleInstrumentation { private final static Logger LOGGER = LoggerFactory . getLogger ( ExampleTracingInstrumentation . class ); @Override public InstrumentationState createState () { return new TracingState (); } @Override public InstrumentationContext < ExecutionResult > beginExecution ( InstrumentationExecutionParameters parameters ) { TracingState tracingState = parameters . getInstrumentationState (); tracingState . startTime = System . currentTimeMillis (); return super . beginExecution ( parameters ); } @Override public DataFetcher <?> instrumentDataFetcher ( DataFetcher <?> dataFetcher , InstrumentationFieldFetchParameters parameters ) { // We only care about user code if ( parameters . isTrivialDataFetcher ()) { return dataFetcher ; } return environment -> { long startTime = System . currentTimeMillis (); Object result = dataFetcher . get ( environment ); if ( result instanceof CompletableFuture ) { (( CompletableFuture <?> ) result ). whenComplete (( r , ex ) -> { long totalTime = System . currentTimeMillis () - startTime ; LOGGER . info ( \"Async datafetcher {} took {}ms\" , findDatafetcherTag ( parameters ), totalTime ); }); } else { long totalTime = System . currentTimeMillis () - startTime ; LOGGER . info ( \"Datafetcher {} took {}ms\" , findDatafetcherTag ( parameters ), totalTime ); } return result ; }; } @Override public CompletableFuture < ExecutionResult > instrumentExecutionResult ( ExecutionResult executionResult , InstrumentationExecutionParameters parameters ) { TracingState tracingState = parameters . getInstrumentationState (); long totalTime = System . currentTimeMillis () - tracingState . startTime ; LOGGER . info ( \"Total execution time: {}ms\" , totalTime ); return super . instrumentExecutionResult ( executionResult , parameters ); } private String findDatafetcherTag ( InstrumentationFieldFetchParameters parameters ) { GraphQLOutputType type = parameters . getExecutionStepInfo (). getParent (). getType (); GraphQLObjectType parent ; if ( type instanceof GraphQLNonNull ) { parent = ( GraphQLObjectType ) (( GraphQLNonNull ) type ). getWrappedType (); } else { parent = ( GraphQLObjectType ) type ; } return parent . getName () + \".\" + parameters . getExecutionStepInfo (). getPath (). getSegmentName (); } static class TracingState implements InstrumentationState { long startTime ; } } Kotlin @Component class ExampleTracingInstrumentation : SimpleInstrumentation () { val logger : Logger = LoggerFactory . getLogger ( ExampleTracingInstrumentation :: class . java ) override fun createState (): InstrumentationState { return TraceState () } override fun beginExecution ( parameters : InstrumentationExecutionParameters ): InstrumentationContext < ExecutionResult > { val state : TraceState = parameters . getInstrumentationState () state . traceStartTime = System . currentTimeMillis () return super . beginExecution ( parameters ) } override fun instrumentDataFetcher ( dataFetcher : DataFetcher <*> , parameters : InstrumentationFieldFetchParameters ): DataFetcher <*> { // We only care about user code if ( parameters . isTrivialDataFetcher ) { return dataFetcher } val dataFetcherName = findDatafetcherTag ( parameters ) return DataFetcher { environment -> val startTime = System . currentTimeMillis () val result = dataFetcher . get ( environment ) if ( result is CompletableFuture <*> ) { result . whenComplete { _ , _ -> val totalTime = System . currentTimeMillis () - startTime logger . info ( \"Async datafetcher ' $ dataFetcherName ' took ${ totalTime } ms\" ) } } else { val totalTime = System . currentTimeMillis () - startTime logger . info ( \"Datafetcher ' $ dataFetcherName ': ${ totalTime } ms\" ) } result } } override fun instrumentExecutionResult ( executionResult : ExecutionResult , parameters : InstrumentationExecutionParameters ): CompletableFuture < ExecutionResult > { val state : TraceState = parameters . getInstrumentationState () val totalTime = System . currentTimeMillis () - state . traceStartTime logger . info ( \"Total execution time: ${ totalTime } ms\" ) return super . instrumentExecutionResult ( executionResult , parameters ) } private fun findDatafetcherTag ( parameters : InstrumentationFieldFetchParameters ): String { val type = parameters . executionStepInfo . parent . type val parentType = if ( type is GraphQLNonNull ) { type . wrappedType as GraphQLObjectType } else { type as GraphQLObjectType } return \" ${ parentType . name } . ${ parameters . executionStepInfo . path . segmentName } \" } data class TraceState ( var traceStartTime : Long = 0 ): InstrumentationState } Datafetcher 'Query.shows': 0ms Total execution time: 3ms Enabling Apollo Tracing \u00b6 If you want to leverage Apollo Tracing , as supported by graphql-java , you can create a bean of type {@link TracingInstrumentation}. In this example, we added a conditional property on the bean to enable/disable the Apollo Tracing. This property is enabled by default, but you can turn it off by setting graphql.tracing.enabled=false in your application properties. import graphql.execution.instrumentation.tracing.TracingInstrumentation ; @SpringBootApplication public class ReviewsDgs { @Bean @ConditionalOnProperty ( prefix = \"graphql.tracing\" , name = \"enabled\" , matchIfMissing = true ) public Instrumentation tracingInstrumentation (){ return new TracingInstrumentation (); } } For federated tracing, you will need to use the instrumentation provided by Apollo's jvm federation library import com.apollographql.federation.graphqljava.tracing.FederatedTracingInstrumentation ; @SpringBootApplication public class ReviewsDgs { @Bean @ConditionalOnProperty ( prefix = \"graphql.tracing\" , name = \"enabled\" , matchIfMissing = true ) public Instrumentation tracingInstrumentation (){ return new FederatedTracingInstrumentation (); } } Metrics Out of The Box \u00b6 tl;dr Supported via the opt-in graphql-dgs-spring-boot-micrometer module. Provides specific GraphQL metrics such as gql.query , gql.error , and gql.dataLoader . Supports several backend implementations since its implemented via Micrometer . Gradle Groovy dependencies { implementation 'com.netflix.graphql.dgs:graphql-dgs-spring-boot-micrometer' } Gradle Kotlin dependencies { implementation ( \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-micrometer\" ) } Maven <dependencies> <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-spring-boot-micrometer </artifactId> </dependency> </dependencies> Hint Note that the version is missing since we assume you are using the latest BOM. We recommend you use the DGS Platform BOM to handle such versions. Shared Tags \u00b6 The following are tags shared across most of the meters. Tags: tag name values description gql.operation QUERY, MUTATION, SUBSCRIPTION are the possible values. These represent the GraphQL operation that is executed. gql.operation.name GraphQL operation name if any, else anonymous . Since the cardinality of the value is high it will be limited . gql.query.complexity one in [5, 10, 20, 50, 100, 200, 500, 1000] The total number of nodes in the query. Refer to Query Complexity section . for additional information. gql.query.sig.hash Query Signature Hash of the query that was executed. Since the cardinality of the value is high it will be limited . GraphQL Query Complexity \u00b6 The gql.query.complexity is typically calculated as 1 + Child's Complexity. The query complexity is valuable to calculate the cost of a query as this can vary based on input arguments to the query. The computed value is represented as one of the bucketed values to reduce the cardinality of the metric. Example Query: query { viewer { repositories(first: 50) { edges { repository:node { name issues(first: 10) { totalCount edges { node { title bodyHTML } } } } } } } } Example Calculation: 50 = 50 repositories + 50 x 10 = 500 repository issues = 550 total nodes GraphQL Query Signature Hash \u00b6 The Query Signature is defined as the tuple of the GraphQL AST Signature of the GraphQL Document and the GraphQL AST Signature Hash . The GraphQL AST Signature of a GraphQL Document is defined as follows: A canonical AST which removes excess operations, removes any field aliases, hides literal values and sorts the result into a canonical query Ref graphql-java The GraphQL AST Signature Hash is the Hex 256 SHA string produced by encoding the AST Signature . While we can't tag a metric by its signature, due its length, we can use the hash , as now expressed by the gql.query.sig.hash tag. There are a few configuration parameters that can change the behavior of the gql.query.sig.hash tag. management.metrics.dgs-graphql.query-signature.enabled : Defaulting to true , it enables the calculation of the GQL Query Signature . The gql.query.sig.hash will express the GQL Query Signature Hash . management.metrics.dgs-graphql.query-signature.caching.enabled : Defaulting to true , it will cache the GQL Query Signature . If set to false it will just disable the cache but will not turn the calculation of the signature off. If you want to turn such calculation off use the management.metrics.dgs-graphql.query-signature.enabled property. Cardinality Limiter \u00b6 The cardinality of a given tag, the number of different values that a tag can express, can be problematic to servers supporting metrics. In order to prevent the cardinality of some of the tags supported out of the box there are some limiters by default. The limited tag values will only see the first 100 different values by default, from there new values will be expressed as --others-- . You can change the limiter via the following configuration: management.metrics.dgs-graphql.tags.limiter.limit : Defaults to 100 , sets the number of different values expressed per limited tag. Not all tags are limited, currently, only following are: gql.operation.name gql.query.sig.hash Query Timer: gql.query \u00b6 Captures the elapsed time that a given GraphQL query, or mutation, takes. Name: gql.query Tags: tag name values description outcome success or failure Result of the operation, as defined by the ExecutionResult . Error Counter: gql.error \u00b6 Captures the number of GraphQL errors encountered during query execution of a query or mutation. Remember that one graphql request can have multiple errors. Name: gql.error Tags: tag name description gql.errorCode The GraphQL error code, such as VALIDATION , INTERNAL , etc. gql.path The sanitized query path that resulted in the error. gql.errorDetail Optional flag containing additional details, if present. Data Loader Timer: gql.dataLoader \u00b6 Captures the elapsed time for a data loader invocation for a batch of queries. This is useful if you want to find data loaders that might be responsible for poor query performance. Name: gql.dataLoader Tags: tag name description gql.loaderName The name of the data loader, may or may not be the same as the type of entity. gql.loaderBatchSize The number of queries executed in the batch. Data Fetcher Timer: gql.resolver \u00b6 Captures the elapsed time of each data fetcher invocation. This is useful if you want to find data fetchers that might be responsible for poor query performance. That said, there might be times where you want to remove a data fetcher from being measured/included in this meter. You can do so by annotating the method with @DgsEnableDataFetcherInstrumentation(false) . Info This metric is not available if: The data is resolved via a Batch Loader. The method is annotated with @DgsEnableDataFetcherInstrumentation(false) . The DataFetcher is TrivialDataFetcher . A trivial DataFetcher is one that simply maps data from an object to a field. This is defined directly in graphql-java . Name: gql.resolver Tags: tag name description gql.field Name of the data fetcher. This has the ${parentType}.${field} format as specified in the @DgsData annotation. Data Fetcher Timer as a Counter \u00b6 The data fetcher, or resolver, timer can be used as a counter as well. If used in this manner it will reflect the number of invocations of each data fetcher. This is useful if you want to find out which data fetchers are used often. Further Tag Customization \u00b6 You can customize the tags applied to the metrics above by providing beans that implement the following functional interfaces. Interface Description DgsContextualTagCustomizer Used to add common, contextual tags. Example of these could be used to describe the deployment environment, application profile, application version, etc DgsExecutionTagCustomizer Used to add tags specific to the ExecutionResult of the query. The SimpleGqlOutcomeTagCustomizer is an example of this. DgsFieldFetchTagCustomizer Used to add tags specific to the execution of data fetchers . The SimpleGqlOutcomeTagCustomizer is an example of this as well. Additional Metrics Configuration \u00b6 management.metrics.dgs-graphql.enabled : Enables the metrics provided out of the box; defaults to true . management.metrics.dgs-graphql.tag-customizers.outcome.enabled : Enables the tag customizer that will label the gql.query and gql.resolver timers with an outcome reflecting the result of the GraphQL outcome, either success or failure ; defaults to true . management.metrics.dgs-graphql.data-loader-instrumentation.enabled : Enables instrumentation of data loaders ; defaults to true .","title":"Instrumentation (Tracing, Metrics)"},{"location":"advanced/instrumentation/#adding-instrumentation-for-tracing-and-logging","text":"It can be extremely valuable to add tracing, metrics and logging to your GraphQL API. At Netflix we publish tracing spans and metrics for each datafetcher to our distributed tracing/metrics backends, and log queries and query results to our logging backend. The implementations we use at Netflix are highly specific for our infrastructure, but it's easy to add your own to the framework! Internally the DGS framework uses GraphQL Java . GraphQL Java supports the concept of instrumentation . In the DGS framework we can easily add one or more instrumentation classes by implementing the graphql.execution.instrumentation.Instrumentation interface and register the class as @Component . The easiest way to implement the Instrumentation interface is to extend graphql.execution.instrumentation.SimpleInstrumentation . The following is an example of an implementation that outputs the execution time for each data fetcher, and the total query execution time, to the logs. Most likely you would want to replace the log output with writing to your tracing/metrics backend. Note that the code example accounts for async data fetchers. If we wouldn't do this, the result for an async data fetcher would always be 0, because the actual processing happens later. Java @Component public class ExampleTracingInstrumentation extends SimpleInstrumentation { private final static Logger LOGGER = LoggerFactory . getLogger ( ExampleTracingInstrumentation . class ); @Override public InstrumentationState createState () { return new TracingState (); } @Override public InstrumentationContext < ExecutionResult > beginExecution ( InstrumentationExecutionParameters parameters ) { TracingState tracingState = parameters . getInstrumentationState (); tracingState . startTime = System . currentTimeMillis (); return super . beginExecution ( parameters ); } @Override public DataFetcher <?> instrumentDataFetcher ( DataFetcher <?> dataFetcher , InstrumentationFieldFetchParameters parameters ) { // We only care about user code if ( parameters . isTrivialDataFetcher ()) { return dataFetcher ; } return environment -> { long startTime = System . currentTimeMillis (); Object result = dataFetcher . get ( environment ); if ( result instanceof CompletableFuture ) { (( CompletableFuture <?> ) result ). whenComplete (( r , ex ) -> { long totalTime = System . currentTimeMillis () - startTime ; LOGGER . info ( \"Async datafetcher {} took {}ms\" , findDatafetcherTag ( parameters ), totalTime ); }); } else { long totalTime = System . currentTimeMillis () - startTime ; LOGGER . info ( \"Datafetcher {} took {}ms\" , findDatafetcherTag ( parameters ), totalTime ); } return result ; }; } @Override public CompletableFuture < ExecutionResult > instrumentExecutionResult ( ExecutionResult executionResult , InstrumentationExecutionParameters parameters ) { TracingState tracingState = parameters . getInstrumentationState (); long totalTime = System . currentTimeMillis () - tracingState . startTime ; LOGGER . info ( \"Total execution time: {}ms\" , totalTime ); return super . instrumentExecutionResult ( executionResult , parameters ); } private String findDatafetcherTag ( InstrumentationFieldFetchParameters parameters ) { GraphQLOutputType type = parameters . getExecutionStepInfo (). getParent (). getType (); GraphQLObjectType parent ; if ( type instanceof GraphQLNonNull ) { parent = ( GraphQLObjectType ) (( GraphQLNonNull ) type ). getWrappedType (); } else { parent = ( GraphQLObjectType ) type ; } return parent . getName () + \".\" + parameters . getExecutionStepInfo (). getPath (). getSegmentName (); } static class TracingState implements InstrumentationState { long startTime ; } } Kotlin @Component class ExampleTracingInstrumentation : SimpleInstrumentation () { val logger : Logger = LoggerFactory . getLogger ( ExampleTracingInstrumentation :: class . java ) override fun createState (): InstrumentationState { return TraceState () } override fun beginExecution ( parameters : InstrumentationExecutionParameters ): InstrumentationContext < ExecutionResult > { val state : TraceState = parameters . getInstrumentationState () state . traceStartTime = System . currentTimeMillis () return super . beginExecution ( parameters ) } override fun instrumentDataFetcher ( dataFetcher : DataFetcher <*> , parameters : InstrumentationFieldFetchParameters ): DataFetcher <*> { // We only care about user code if ( parameters . isTrivialDataFetcher ) { return dataFetcher } val dataFetcherName = findDatafetcherTag ( parameters ) return DataFetcher { environment -> val startTime = System . currentTimeMillis () val result = dataFetcher . get ( environment ) if ( result is CompletableFuture <*> ) { result . whenComplete { _ , _ -> val totalTime = System . currentTimeMillis () - startTime logger . info ( \"Async datafetcher ' $ dataFetcherName ' took ${ totalTime } ms\" ) } } else { val totalTime = System . currentTimeMillis () - startTime logger . info ( \"Datafetcher ' $ dataFetcherName ': ${ totalTime } ms\" ) } result } } override fun instrumentExecutionResult ( executionResult : ExecutionResult , parameters : InstrumentationExecutionParameters ): CompletableFuture < ExecutionResult > { val state : TraceState = parameters . getInstrumentationState () val totalTime = System . currentTimeMillis () - state . traceStartTime logger . info ( \"Total execution time: ${ totalTime } ms\" ) return super . instrumentExecutionResult ( executionResult , parameters ) } private fun findDatafetcherTag ( parameters : InstrumentationFieldFetchParameters ): String { val type = parameters . executionStepInfo . parent . type val parentType = if ( type is GraphQLNonNull ) { type . wrappedType as GraphQLObjectType } else { type as GraphQLObjectType } return \" ${ parentType . name } . ${ parameters . executionStepInfo . path . segmentName } \" } data class TraceState ( var traceStartTime : Long = 0 ): InstrumentationState } Datafetcher 'Query.shows': 0ms Total execution time: 3ms","title":"Adding instrumentation for tracing and logging"},{"location":"advanced/instrumentation/#enabling-apollo-tracing","text":"If you want to leverage Apollo Tracing , as supported by graphql-java , you can create a bean of type {@link TracingInstrumentation}. In this example, we added a conditional property on the bean to enable/disable the Apollo Tracing. This property is enabled by default, but you can turn it off by setting graphql.tracing.enabled=false in your application properties. import graphql.execution.instrumentation.tracing.TracingInstrumentation ; @SpringBootApplication public class ReviewsDgs { @Bean @ConditionalOnProperty ( prefix = \"graphql.tracing\" , name = \"enabled\" , matchIfMissing = true ) public Instrumentation tracingInstrumentation (){ return new TracingInstrumentation (); } } For federated tracing, you will need to use the instrumentation provided by Apollo's jvm federation library import com.apollographql.federation.graphqljava.tracing.FederatedTracingInstrumentation ; @SpringBootApplication public class ReviewsDgs { @Bean @ConditionalOnProperty ( prefix = \"graphql.tracing\" , name = \"enabled\" , matchIfMissing = true ) public Instrumentation tracingInstrumentation (){ return new FederatedTracingInstrumentation (); } }","title":"Enabling Apollo Tracing"},{"location":"advanced/instrumentation/#metrics-out-of-the-box","text":"tl;dr Supported via the opt-in graphql-dgs-spring-boot-micrometer module. Provides specific GraphQL metrics such as gql.query , gql.error , and gql.dataLoader . Supports several backend implementations since its implemented via Micrometer . Gradle Groovy dependencies { implementation 'com.netflix.graphql.dgs:graphql-dgs-spring-boot-micrometer' } Gradle Kotlin dependencies { implementation ( \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-micrometer\" ) } Maven <dependencies> <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-spring-boot-micrometer </artifactId> </dependency> </dependencies> Hint Note that the version is missing since we assume you are using the latest BOM. We recommend you use the DGS Platform BOM to handle such versions.","title":"Metrics Out of The Box"},{"location":"advanced/instrumentation/#shared-tags","text":"The following are tags shared across most of the meters. Tags: tag name values description gql.operation QUERY, MUTATION, SUBSCRIPTION are the possible values. These represent the GraphQL operation that is executed. gql.operation.name GraphQL operation name if any, else anonymous . Since the cardinality of the value is high it will be limited . gql.query.complexity one in [5, 10, 20, 50, 100, 200, 500, 1000] The total number of nodes in the query. Refer to Query Complexity section . for additional information. gql.query.sig.hash Query Signature Hash of the query that was executed. Since the cardinality of the value is high it will be limited .","title":"Shared Tags"},{"location":"advanced/instrumentation/#graphql-query-complexity","text":"The gql.query.complexity is typically calculated as 1 + Child's Complexity. The query complexity is valuable to calculate the cost of a query as this can vary based on input arguments to the query. The computed value is represented as one of the bucketed values to reduce the cardinality of the metric. Example Query: query { viewer { repositories(first: 50) { edges { repository:node { name issues(first: 10) { totalCount edges { node { title bodyHTML } } } } } } } } Example Calculation: 50 = 50 repositories + 50 x 10 = 500 repository issues = 550 total nodes","title":"GraphQL Query Complexity"},{"location":"advanced/instrumentation/#graphql-query-signature-hash","text":"The Query Signature is defined as the tuple of the GraphQL AST Signature of the GraphQL Document and the GraphQL AST Signature Hash . The GraphQL AST Signature of a GraphQL Document is defined as follows: A canonical AST which removes excess operations, removes any field aliases, hides literal values and sorts the result into a canonical query Ref graphql-java The GraphQL AST Signature Hash is the Hex 256 SHA string produced by encoding the AST Signature . While we can't tag a metric by its signature, due its length, we can use the hash , as now expressed by the gql.query.sig.hash tag. There are a few configuration parameters that can change the behavior of the gql.query.sig.hash tag. management.metrics.dgs-graphql.query-signature.enabled : Defaulting to true , it enables the calculation of the GQL Query Signature . The gql.query.sig.hash will express the GQL Query Signature Hash . management.metrics.dgs-graphql.query-signature.caching.enabled : Defaulting to true , it will cache the GQL Query Signature . If set to false it will just disable the cache but will not turn the calculation of the signature off. If you want to turn such calculation off use the management.metrics.dgs-graphql.query-signature.enabled property.","title":"GraphQL Query Signature Hash"},{"location":"advanced/instrumentation/#cardinality-limiter","text":"The cardinality of a given tag, the number of different values that a tag can express, can be problematic to servers supporting metrics. In order to prevent the cardinality of some of the tags supported out of the box there are some limiters by default. The limited tag values will only see the first 100 different values by default, from there new values will be expressed as --others-- . You can change the limiter via the following configuration: management.metrics.dgs-graphql.tags.limiter.limit : Defaults to 100 , sets the number of different values expressed per limited tag. Not all tags are limited, currently, only following are: gql.operation.name gql.query.sig.hash","title":"Cardinality Limiter"},{"location":"advanced/instrumentation/#query-timer-gqlquery","text":"Captures the elapsed time that a given GraphQL query, or mutation, takes. Name: gql.query Tags: tag name values description outcome success or failure Result of the operation, as defined by the ExecutionResult .","title":"Query Timer: gql.query"},{"location":"advanced/instrumentation/#error-counter-gqlerror","text":"Captures the number of GraphQL errors encountered during query execution of a query or mutation. Remember that one graphql request can have multiple errors. Name: gql.error Tags: tag name description gql.errorCode The GraphQL error code, such as VALIDATION , INTERNAL , etc. gql.path The sanitized query path that resulted in the error. gql.errorDetail Optional flag containing additional details, if present.","title":"Error Counter: gql.error"},{"location":"advanced/instrumentation/#data-loader-timer-gqldataloader","text":"Captures the elapsed time for a data loader invocation for a batch of queries. This is useful if you want to find data loaders that might be responsible for poor query performance. Name: gql.dataLoader Tags: tag name description gql.loaderName The name of the data loader, may or may not be the same as the type of entity. gql.loaderBatchSize The number of queries executed in the batch.","title":"Data Loader Timer: gql.dataLoader"},{"location":"advanced/instrumentation/#data-fetcher-timer-gqlresolver","text":"Captures the elapsed time of each data fetcher invocation. This is useful if you want to find data fetchers that might be responsible for poor query performance. That said, there might be times where you want to remove a data fetcher from being measured/included in this meter. You can do so by annotating the method with @DgsEnableDataFetcherInstrumentation(false) . Info This metric is not available if: The data is resolved via a Batch Loader. The method is annotated with @DgsEnableDataFetcherInstrumentation(false) . The DataFetcher is TrivialDataFetcher . A trivial DataFetcher is one that simply maps data from an object to a field. This is defined directly in graphql-java . Name: gql.resolver Tags: tag name description gql.field Name of the data fetcher. This has the ${parentType}.${field} format as specified in the @DgsData annotation.","title":"Data Fetcher Timer: gql.resolver"},{"location":"advanced/instrumentation/#data-fetcher-timer-as-a-counter","text":"The data fetcher, or resolver, timer can be used as a counter as well. If used in this manner it will reflect the number of invocations of each data fetcher. This is useful if you want to find out which data fetchers are used often.","title":"Data Fetcher Timer as a Counter"},{"location":"advanced/instrumentation/#further-tag-customization","text":"You can customize the tags applied to the metrics above by providing beans that implement the following functional interfaces. Interface Description DgsContextualTagCustomizer Used to add common, contextual tags. Example of these could be used to describe the deployment environment, application profile, application version, etc DgsExecutionTagCustomizer Used to add tags specific to the ExecutionResult of the query. The SimpleGqlOutcomeTagCustomizer is an example of this. DgsFieldFetchTagCustomizer Used to add tags specific to the execution of data fetchers . The SimpleGqlOutcomeTagCustomizer is an example of this as well.","title":"Further Tag Customization"},{"location":"advanced/instrumentation/#additional-metrics-configuration","text":"management.metrics.dgs-graphql.enabled : Enables the metrics provided out of the box; defaults to true . management.metrics.dgs-graphql.tag-customizers.outcome.enabled : Enables the tag customizer that will label the gql.query and gql.resolver timers with an outcome reflecting the result of the GraphQL outcome, either success or failure ; defaults to true . management.metrics.dgs-graphql.data-loader-instrumentation.enabled : Enables instrumentation of data loaders ; defaults to true .","title":"Additional Metrics Configuration"},{"location":"advanced/java-client/","text":"Usage \u00b6 The DGS framework provides a GraphQL client that can be used to retrieve data from a GraphQL endpoint. The client is also useful for integration testing a DGS. The client has two components, each usable by itself, or in combination together. GraphQL Client - A HTTP client wrapper that provides easy parsing of GraphQL responses Query API codegen - Generate type-safe Query builders The client has multiple interfaces and implementations for different needs. The interfaces are the following: GraphQLClient - Client interface for blocking implementations. Only recommended when using a blocking HTTP client. MonoGraphQLClient - The same as GraphQLClient, but based on the reactive Mono interface meant for non-blocking implementations. Comes with an out-of-the-box WebClient implementation: MonoGraphQLClient.createWithWebClient(...) . ReactiveGraphQLClient - Client interface for streaming responses such as Subscriptions, where multiple results are expected. Based on the reactive Flux interface. Implemented by SSESubscriptionGraphQLClient and WebSocketGraphQLClient . GraphQL Client with WebClient \u00b6 The easiest way to use the DGS GraphQL Client is to use the WebClient implementation. WebClient is the recommended HTTP client in Spring, and is the best choice for most use cases, unless you have a specific reason to use a different HTTP client. Because WebClient is Reactive, the client returns a Mono for all operations. Java //Configure a WebClient for your needs, e.g. including authentication headers and TLS. WebClient webClient = WebClient . create ( \"http://localhost:8080/graphql\" ); WebClientGraphQLClient client = MonoGraphQLClient . createWithWebClient ( webClient ); //The GraphQLResponse contains data and errors. Mono < GraphQLResponse > graphQLResponseMono = graphQLClient . reactiveExecuteQuery ( query ); //GraphQLResponse has convenience methods to extract fields using JsonPath. Mono < String > somefield = graphQLResponseMono . map ( r -> r . extractValue ( \"somefield\" )); //Don't forget to subscribe! The request won't be executed otherwise. somefield . subscribe (); Kotlin //Configure a WebClient for your needs, e.g. including authentication headers and TLS. val client = MonoGraphQLClient . createWithWebClient ( WebClient . create ( \"http://localhost:8080/graphql\" )) //Executing the query returns a Mono of GraphQLResponse. val result = client . reactiveExecuteQuery ( \"{hello}\" ). map { r -> r . extractValue < String > ( \"hello\" ) } //Don't forget to subscribe! The request won't be executed otherwise. somefield . subscribe (); The reactiveExecuteQuery method takes a query String as input, and optionally a Map of variables and an operation name. Instead of using a query String, you can use code generation to create a type-safe query builder API. The GraphQLResponse provides methods to parse and retrieve data and errors in a variety of ways. Refer to the GraphQLResponse JavaDoc for the complete list of supported methods. method description example getData Get the data as a Map Map<String,Object> data = response.getData() dataAsObject Parse data as the provided class, using the Jackson Object Mapper TickResponse data = response.dataAsObject(TicksResponse.class) extractValue Extract values given a JsonPath . The return type will be whatever type you expect, but depends on the JSON shape. For JSON objects, a Map is returned. Although this looks type safe, it really isn't. It's mostly useful for \"simple\" types like String, Int etc., and Lists of those types. List<String> name = response.extractValue(\"movies[*].originalTitle\") extractValueAsObject Extract values given a JsonPath and deserialize into the given class Ticks ticks = response.extractValueAsObject(\"ticks\", Ticks.class) extractValueAsObject Extract values given a JsonPath and deserialize into the given TypeRef. Useful for Maps and Lists of a certain class. List<Route> routes = response.extractValueAsObject(\"ticks.edges[*].node.route\", new TypeRef<List<Route>>(){}) getRequestDetails Extract a RequestDetails object. This only works if requestDetails was requested in the query, and against the Gateway. RequestDetails requestDetails = response.getRequestDetails() getParsed Get the parsed DocumentContext for further JsonPath processing response.getDocumentContext() The client can be used against any GraphQL endpoint (it doesn't have to be implemented with the DGS framework), but provides extra conveniences for parsing Gateway and DGS responses. This includes support for the Errors Spec . Headers \u00b6 HTTP headers can easily be added to the request. WebClientGraphQLClient client = MonoGraphQLClient . createWithWebClient ( webClient , headers -> headers . add ( \"myheader\" , \"test\" )); By default, the client already sets the Content-type and Accept headers. Errors \u00b6 The GraphQLClient checks both for HTTP level errors (based on the response status code) and the errors block in a GraphQL response. The GraphQLClient is compatible with the Errors Spec used by the Gateway and DGS, and makes it easy to extract error information such as the ErrorType. For example, for following GraphQL response the GraphQLClient lets you easily get the ErrorType and ErrorDetail fields. Note that the ErrorType is an enum as specified by the Errors Spec . { \"errors\": [ { \"message\": \"java.lang.RuntimeException: test\", \"locations\": [], \"path\": [ \"hello\" ], \"extensions\": { \"errorType\": \"BAD_REQUEST\", \"errorDetail\": \"FIELD_NOT_FOUND\" } } ], \"data\": { \"hello\": null } } assertThat ( graphQLResponse . errors . get ( 0 ). extensions . errorType ). isEqualTo ( ErrorType . BAD_REQUEST ) assertThat ( graphQLResponse . errors . get ( 0 ). extensions . errorDetail ). isEqualTo ( \"FIELD_NOT_FOUND\" ) Plug in your own HTTP client \u00b6 Instead of using WebClient, you can also plug in your own HTTP client. This is useful if you already have a configured client for your backend with authN/authZ, TLS, etc. In this case you are responsible for making the actual request, but the GraphQL client wraps the HTTP client and provides easy parsing of GraphQL responses. There are two interfaces that you can pick from: GraphQLClient: For blocking HTTP clients MonoGraphQLClient: For non-blocking HTTP clients Both interfaces return a GraphQLResponse for each query execution, but MonoGraphQLClient wraps the result in a Mono , making it a better fit for non-blocking clients. Create an instance by using the factory method on the interface. This returns an instance of CustomGraphQLClient or CustomMonoGraphQLClient . The implementations are named Custom* to indicate you need to provide handling of the actual HTTP request. Java CustomGraphQLClient client = GraphQLClient . createCustom ( url , ( url , headers , body ) -> { HttpHeaders httpHeaders = new HttpHeaders (); headers . forEach ( httpHeaders :: addAll ); ResponseEntity < String > exchange = restTemplate . exchange ( url , HttpMethod . POST , new HttpEntity <> ( body , httpHeaders ), String . class ); return new HttpResponse ( exchange . getStatusCodeValue (), exchange . getBody ()); }); GraphQLResponse graphQLResponse = client . executeQuery ( query , emptyMap (), \"SubmitReview\" ); String submittedBy = graphQLResponse . extractValueAsObject ( \"submitReview.submittedBy\" , String . class ); Kotlin //Configure your HTTP client val restTemplate = RestTemplate (); val client = GraphQLClient . createCustom ( \"http://localhost:8080/graphql\" ) { url , headers , body -> //Prepare the request, e.g. set up headers. val httpHeaders = HttpHeaders () headers . forEach { httpHeaders . addAll ( it . key , it . value ) } //Use your HTTP client to send the request to the server. val exchange = restTemplate . exchange ( url , HttpMethod . POST , HttpEntity ( body , httpHeaders ), String :: class . java ) //Transform the response into a HttpResponse HttpResponse ( exchange . statusCodeValue , exchange . body ) } //Send a query and extract a value out of the result. val result = client . executeQuery ( \"{hello}\" ). extractValue < String > ( \"hello\" ) Alternatively, use MonoGraphQLClient.createCustomReactive(...) to create the reactive equivalent. The provided RequestExecutor must now return Mono<HttpResponse> . CustomMonoGraphQLClient client = MonoGraphQLClient . createCustomReactive ( url , ( requestUrl , headers , body ) -> { HttpHeaders httpHeaders = new HttpHeaders (); headers . forEach ( httpHeaders :: addAll ); ResponseEntity < String > exchange = restTemplate . exchange ( url , HttpMethod . POST , new HttpEntity <> ( body , httpHeaders ), String . class ); return Mono . just ( new HttpResponse ( exchange . getStatusCodeValue (), exchange . getBody (), exchange . getHeaders ())); }); Mono < GraphQLResponse > graphQLResponse = client . reactiveExecuteQuery ( query , emptyMap (), \"SubmitReview\" ); String submittedBy = graphQLResponse . map ( r -> r . extractValueAsObject ( \"submitReview.submittedBy\" , String . class )). block (); Note that in this example we just use Mono.just to create a Mono. This doesn't make the call non-blocking. Migrating from DefaultGraphQLClient \u00b6 In previous versions of the framework we provided the DefaultGraphQLClient class. This has been deprecated for the following reasons: The \"Default\" in the name suggested that it should be the implementation for most use cases. However, the new WebClient implementation is a much better option now. Naming things is hard. The API required you to pass in the RequestExecutor for each query execution. This wasn't ergonomic for the new WebClient implementation, because no RequestExecutor is required. If you want to migrate existing usage of DefaultGraphQLClient you can either use the WebClient implementation and get rid of your RequestExecutor entirely, or alternatively use CustomGraphQLClient / CustomMonoGraphQLClient which has almost the same API. To migrate to CustomGraphQLClient you pass in your existing RequestExecutor to the GraphQLClient.createCustom(url, requestExecutor) factory method, and remove it from the executeQuery methods. We plan to eventually remove the DefaultGraphQLClient , because its API is confusing. Type safe Query API \u00b6 Based on a GraphQL schema a type safe query API can be generated for Java/Kotlin. The generated API is a builder style API that lets you build a GraphQL query, and it's projection (field selection). Because the code gets re-generated when the schema changes, it helps catch errors in the query. It's arguably also more readable, although multiline String support in Java and Kotlin do mitigate that issue as well. If you own a DGS and want to generate a client for this DGS (e.g. for testing purposes) the client generation is just an extra property on the Codegen configuration . Specify the following in your build.gradle . // Using plugins DSL plugins { id \"com.netflix.dgs.codegen\" version \"[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]\" } generateJava { packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true } Code will be generated on build, and the generated code will be under builder/generated , which is added to the classpath by the plugin. With codegen configured correctly, a builder style API will be generated when building the project. Using the same query example as above, the query can be build using the generated builder API. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new TicksGraphQLQuery.Builder() .first(first) .after(after) .build(), new TicksConnectionProjectionRoot() .edges() .node() .date() .route() .name() .votes() .starRating() .parent() .grade()); String query = graphQLQueryRequest.serialize(); The GraphQLQueryRequest is a class from graphql-dgs-client . The TicksGraphQLQuery and TicksConnectionProjectionRoot are generated. After building the query, it can be serialized to a String, and executed using the GraphQLClient. Note that the edges and node fields are because the example schema is using Relay pagination. Scalars in DGS Client \u00b6 Custom scalars can be used in input types in GraphQL. Let's take the example of a DateTimeScalar (created in Adding Custom Scalars ). In Java, we want to represent this as a LocalDateTime class. When sending the query, we somehow have to serialize this. There are many ways to represent a date, so how do we make sure that we use the same representation as the server expects? In this release we added an optional scalars argument to the GraphQLQueryRequest constructor. This is a Map<Class<?>, Coercing<?,?>> that maps the Java class representing the input to an actual Scalar implementation. We will generate the query API with DateTimeScalar as follows: Map < Class <?> , Coercing <? , ?>> scalars = new HashMap <> (); scalars . put ( java . time . LocalDateTime . class , new DateTimeScalar ()); new GraphQLQueryRequest ( ReviewsGraphQLQuery . newRequest (). dateRange ( new DateRange ( LocalDate . of ( 2020 , 1 , 1 ), LocalDate . now ())). build (), new ReviewsProjectionRoot (). submittedDate (). starScore (), scalars ); This way you can re-use exactly the same serialization code that you already have for your scalar implementation or one of the existing ones from - for example - the graphql-dgs-extended-scalars module. Interface projections \u00b6 When a field returns an interface, fields on the concrete types are specified using a fragment. type Query @extends { script(name: String): Script } interface Script { title: String director: String actors: [Actor] } type MovieScript implements Script { title: String director: String length: Int } type ShowScript implements Script { title: String director: String episodes: Int } query { script(name: \"Top Secret\") { title ... on MovieScript { length } } } This syntax is supported by the Query builder as well. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ScriptGraphQLQuery . Builder () . name ( \"Top Secret\" ) . build (), new ScriptProjectionRoot () . title () . onMovieScript () . length (); ); Building Federated Queries \u00b6 You can use GraphQLQueryRequest along with EntitiesGraphQLQuery to generated federated queries. The API provides a type-safe way to construct the _entities query with the associated representations based on the input schema. The representations are passed in as a map of variables. Each representation class is generated based on the key fields defined on the entity in your schema, along with the __typename . The EntitiesProjectionRoot is used to select query fields on the specified type. For example, let us look at a schema that extends a Movie type: type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } With client code generation, you will now have a MovieRepresentation containing the key field, i.e., movieId , and the __typename field already set to type Movie . Now you can add each representation to the EntitiesGraphQLQuery as a representations variable. You will also have a EntitiesProjectionRoot with onMovie() to select fields on Movie from. Finally, you put them all together as a GraphQLQueryRequest , which you serialize into the final query string. The map of representations variables is available via getVariables on the EntitiesGraphQLQuery . Here is an example for the schema shown earlier: EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder () . addRepresentationAsVariable ( MovieRepresentation . newBuilder (). movieId ( 1122 ). build () ) . build (); GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onMovie (). movieId (). script (). title () ); String query = request . serialize (); Map < String , Object > representations = entitiesQuery . getVariables (); Subscriptions \u00b6 Subscriptions are supported through the ReactiveGraphQLClient interface. The interface has two implementations: WebSocketGraphQLClient : For subscriptions over WebSockets SSESubscriptionGraphQLClient : For subscriptions over Server Sent Events (SSE) Both implementations require the use of WebClient, and cannot be used with other HTTP clients (in contrast to the \"normal\" DGS client). The clients return a Flux of GraphQLResponse . Each GraphQLResponse represents a message pushed from the subscription, and contains data and errors . It also offers convenience methods to parse data using JsonPath. Java WebClient webClient = WebClient . create ( \"http://localhost:\" + port ); SSESubscriptionGraphQLClient client = new SSESubscriptionGraphQLClient ( \"/subscriptions\" , webClient ); Flux < GraphQLResponse > numbers = client . reactiveExecuteQuery ( \"subscription {numbers}\" , Collections . emptyMap ()); numbers . mapNotNull ( r -> r . extractValue ( \"data.numbers\" )) . log () . subscribe (); Kotlin val webClient = WebClient . create ( \"http://localhost: $ port \" ) val client = SSESubscriptionGraphQLClient ( \"/subscriptions\" , webClient ) val reactiveExecuteQuery = client . reactiveExecuteQuery ( \"subscription {numbers}\" , emptyMap ()) reactiveExecuteQuery . mapNotNull { r -> r . data [ \"numbers\" ] } . log () . subscribe () In case the connection fails to set up, either because of a connection error, or because of an invalid query, a WebClientResponseException will be thrown. Errors later on in the process will be errors in the stream. Don't forget to subscribe() to the stream, otherwise the connection doesn't get started! An example of using the client to write subscription integration tests is available here .","title":"Java GraphQL Client"},{"location":"advanced/java-client/#usage","text":"The DGS framework provides a GraphQL client that can be used to retrieve data from a GraphQL endpoint. The client is also useful for integration testing a DGS. The client has two components, each usable by itself, or in combination together. GraphQL Client - A HTTP client wrapper that provides easy parsing of GraphQL responses Query API codegen - Generate type-safe Query builders The client has multiple interfaces and implementations for different needs. The interfaces are the following: GraphQLClient - Client interface for blocking implementations. Only recommended when using a blocking HTTP client. MonoGraphQLClient - The same as GraphQLClient, but based on the reactive Mono interface meant for non-blocking implementations. Comes with an out-of-the-box WebClient implementation: MonoGraphQLClient.createWithWebClient(...) . ReactiveGraphQLClient - Client interface for streaming responses such as Subscriptions, where multiple results are expected. Based on the reactive Flux interface. Implemented by SSESubscriptionGraphQLClient and WebSocketGraphQLClient .","title":"Usage"},{"location":"advanced/java-client/#graphql-client-with-webclient","text":"The easiest way to use the DGS GraphQL Client is to use the WebClient implementation. WebClient is the recommended HTTP client in Spring, and is the best choice for most use cases, unless you have a specific reason to use a different HTTP client. Because WebClient is Reactive, the client returns a Mono for all operations. Java //Configure a WebClient for your needs, e.g. including authentication headers and TLS. WebClient webClient = WebClient . create ( \"http://localhost:8080/graphql\" ); WebClientGraphQLClient client = MonoGraphQLClient . createWithWebClient ( webClient ); //The GraphQLResponse contains data and errors. Mono < GraphQLResponse > graphQLResponseMono = graphQLClient . reactiveExecuteQuery ( query ); //GraphQLResponse has convenience methods to extract fields using JsonPath. Mono < String > somefield = graphQLResponseMono . map ( r -> r . extractValue ( \"somefield\" )); //Don't forget to subscribe! The request won't be executed otherwise. somefield . subscribe (); Kotlin //Configure a WebClient for your needs, e.g. including authentication headers and TLS. val client = MonoGraphQLClient . createWithWebClient ( WebClient . create ( \"http://localhost:8080/graphql\" )) //Executing the query returns a Mono of GraphQLResponse. val result = client . reactiveExecuteQuery ( \"{hello}\" ). map { r -> r . extractValue < String > ( \"hello\" ) } //Don't forget to subscribe! The request won't be executed otherwise. somefield . subscribe (); The reactiveExecuteQuery method takes a query String as input, and optionally a Map of variables and an operation name. Instead of using a query String, you can use code generation to create a type-safe query builder API. The GraphQLResponse provides methods to parse and retrieve data and errors in a variety of ways. Refer to the GraphQLResponse JavaDoc for the complete list of supported methods. method description example getData Get the data as a Map Map<String,Object> data = response.getData() dataAsObject Parse data as the provided class, using the Jackson Object Mapper TickResponse data = response.dataAsObject(TicksResponse.class) extractValue Extract values given a JsonPath . The return type will be whatever type you expect, but depends on the JSON shape. For JSON objects, a Map is returned. Although this looks type safe, it really isn't. It's mostly useful for \"simple\" types like String, Int etc., and Lists of those types. List<String> name = response.extractValue(\"movies[*].originalTitle\") extractValueAsObject Extract values given a JsonPath and deserialize into the given class Ticks ticks = response.extractValueAsObject(\"ticks\", Ticks.class) extractValueAsObject Extract values given a JsonPath and deserialize into the given TypeRef. Useful for Maps and Lists of a certain class. List<Route> routes = response.extractValueAsObject(\"ticks.edges[*].node.route\", new TypeRef<List<Route>>(){}) getRequestDetails Extract a RequestDetails object. This only works if requestDetails was requested in the query, and against the Gateway. RequestDetails requestDetails = response.getRequestDetails() getParsed Get the parsed DocumentContext for further JsonPath processing response.getDocumentContext() The client can be used against any GraphQL endpoint (it doesn't have to be implemented with the DGS framework), but provides extra conveniences for parsing Gateway and DGS responses. This includes support for the Errors Spec .","title":"GraphQL Client with WebClient"},{"location":"advanced/java-client/#headers","text":"HTTP headers can easily be added to the request. WebClientGraphQLClient client = MonoGraphQLClient . createWithWebClient ( webClient , headers -> headers . add ( \"myheader\" , \"test\" )); By default, the client already sets the Content-type and Accept headers.","title":"Headers"},{"location":"advanced/java-client/#errors","text":"The GraphQLClient checks both for HTTP level errors (based on the response status code) and the errors block in a GraphQL response. The GraphQLClient is compatible with the Errors Spec used by the Gateway and DGS, and makes it easy to extract error information such as the ErrorType. For example, for following GraphQL response the GraphQLClient lets you easily get the ErrorType and ErrorDetail fields. Note that the ErrorType is an enum as specified by the Errors Spec . { \"errors\": [ { \"message\": \"java.lang.RuntimeException: test\", \"locations\": [], \"path\": [ \"hello\" ], \"extensions\": { \"errorType\": \"BAD_REQUEST\", \"errorDetail\": \"FIELD_NOT_FOUND\" } } ], \"data\": { \"hello\": null } } assertThat ( graphQLResponse . errors . get ( 0 ). extensions . errorType ). isEqualTo ( ErrorType . BAD_REQUEST ) assertThat ( graphQLResponse . errors . get ( 0 ). extensions . errorDetail ). isEqualTo ( \"FIELD_NOT_FOUND\" )","title":"Errors"},{"location":"advanced/java-client/#plug-in-your-own-http-client","text":"Instead of using WebClient, you can also plug in your own HTTP client. This is useful if you already have a configured client for your backend with authN/authZ, TLS, etc. In this case you are responsible for making the actual request, but the GraphQL client wraps the HTTP client and provides easy parsing of GraphQL responses. There are two interfaces that you can pick from: GraphQLClient: For blocking HTTP clients MonoGraphQLClient: For non-blocking HTTP clients Both interfaces return a GraphQLResponse for each query execution, but MonoGraphQLClient wraps the result in a Mono , making it a better fit for non-blocking clients. Create an instance by using the factory method on the interface. This returns an instance of CustomGraphQLClient or CustomMonoGraphQLClient . The implementations are named Custom* to indicate you need to provide handling of the actual HTTP request. Java CustomGraphQLClient client = GraphQLClient . createCustom ( url , ( url , headers , body ) -> { HttpHeaders httpHeaders = new HttpHeaders (); headers . forEach ( httpHeaders :: addAll ); ResponseEntity < String > exchange = restTemplate . exchange ( url , HttpMethod . POST , new HttpEntity <> ( body , httpHeaders ), String . class ); return new HttpResponse ( exchange . getStatusCodeValue (), exchange . getBody ()); }); GraphQLResponse graphQLResponse = client . executeQuery ( query , emptyMap (), \"SubmitReview\" ); String submittedBy = graphQLResponse . extractValueAsObject ( \"submitReview.submittedBy\" , String . class ); Kotlin //Configure your HTTP client val restTemplate = RestTemplate (); val client = GraphQLClient . createCustom ( \"http://localhost:8080/graphql\" ) { url , headers , body -> //Prepare the request, e.g. set up headers. val httpHeaders = HttpHeaders () headers . forEach { httpHeaders . addAll ( it . key , it . value ) } //Use your HTTP client to send the request to the server. val exchange = restTemplate . exchange ( url , HttpMethod . POST , HttpEntity ( body , httpHeaders ), String :: class . java ) //Transform the response into a HttpResponse HttpResponse ( exchange . statusCodeValue , exchange . body ) } //Send a query and extract a value out of the result. val result = client . executeQuery ( \"{hello}\" ). extractValue < String > ( \"hello\" ) Alternatively, use MonoGraphQLClient.createCustomReactive(...) to create the reactive equivalent. The provided RequestExecutor must now return Mono<HttpResponse> . CustomMonoGraphQLClient client = MonoGraphQLClient . createCustomReactive ( url , ( requestUrl , headers , body ) -> { HttpHeaders httpHeaders = new HttpHeaders (); headers . forEach ( httpHeaders :: addAll ); ResponseEntity < String > exchange = restTemplate . exchange ( url , HttpMethod . POST , new HttpEntity <> ( body , httpHeaders ), String . class ); return Mono . just ( new HttpResponse ( exchange . getStatusCodeValue (), exchange . getBody (), exchange . getHeaders ())); }); Mono < GraphQLResponse > graphQLResponse = client . reactiveExecuteQuery ( query , emptyMap (), \"SubmitReview\" ); String submittedBy = graphQLResponse . map ( r -> r . extractValueAsObject ( \"submitReview.submittedBy\" , String . class )). block (); Note that in this example we just use Mono.just to create a Mono. This doesn't make the call non-blocking.","title":"Plug in your own HTTP client"},{"location":"advanced/java-client/#migrating-from-defaultgraphqlclient","text":"In previous versions of the framework we provided the DefaultGraphQLClient class. This has been deprecated for the following reasons: The \"Default\" in the name suggested that it should be the implementation for most use cases. However, the new WebClient implementation is a much better option now. Naming things is hard. The API required you to pass in the RequestExecutor for each query execution. This wasn't ergonomic for the new WebClient implementation, because no RequestExecutor is required. If you want to migrate existing usage of DefaultGraphQLClient you can either use the WebClient implementation and get rid of your RequestExecutor entirely, or alternatively use CustomGraphQLClient / CustomMonoGraphQLClient which has almost the same API. To migrate to CustomGraphQLClient you pass in your existing RequestExecutor to the GraphQLClient.createCustom(url, requestExecutor) factory method, and remove it from the executeQuery methods. We plan to eventually remove the DefaultGraphQLClient , because its API is confusing.","title":"Migrating from DefaultGraphQLClient"},{"location":"advanced/java-client/#type-safe-query-api","text":"Based on a GraphQL schema a type safe query API can be generated for Java/Kotlin. The generated API is a builder style API that lets you build a GraphQL query, and it's projection (field selection). Because the code gets re-generated when the schema changes, it helps catch errors in the query. It's arguably also more readable, although multiline String support in Java and Kotlin do mitigate that issue as well. If you own a DGS and want to generate a client for this DGS (e.g. for testing purposes) the client generation is just an extra property on the Codegen configuration . Specify the following in your build.gradle . // Using plugins DSL plugins { id \"com.netflix.dgs.codegen\" version \"[REPLACE_WITH_CODEGEN_PLUGIN_VERSION]\" } generateJava { packageName = 'com.example.packagename' // The package name to use to generate sources generateClient = true } Code will be generated on build, and the generated code will be under builder/generated , which is added to the classpath by the plugin. With codegen configured correctly, a builder style API will be generated when building the project. Using the same query example as above, the query can be build using the generated builder API. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest( new TicksGraphQLQuery.Builder() .first(first) .after(after) .build(), new TicksConnectionProjectionRoot() .edges() .node() .date() .route() .name() .votes() .starRating() .parent() .grade()); String query = graphQLQueryRequest.serialize(); The GraphQLQueryRequest is a class from graphql-dgs-client . The TicksGraphQLQuery and TicksConnectionProjectionRoot are generated. After building the query, it can be serialized to a String, and executed using the GraphQLClient. Note that the edges and node fields are because the example schema is using Relay pagination.","title":"Type safe Query API"},{"location":"advanced/java-client/#scalars-in-dgs-client","text":"Custom scalars can be used in input types in GraphQL. Let's take the example of a DateTimeScalar (created in Adding Custom Scalars ). In Java, we want to represent this as a LocalDateTime class. When sending the query, we somehow have to serialize this. There are many ways to represent a date, so how do we make sure that we use the same representation as the server expects? In this release we added an optional scalars argument to the GraphQLQueryRequest constructor. This is a Map<Class<?>, Coercing<?,?>> that maps the Java class representing the input to an actual Scalar implementation. We will generate the query API with DateTimeScalar as follows: Map < Class <?> , Coercing <? , ?>> scalars = new HashMap <> (); scalars . put ( java . time . LocalDateTime . class , new DateTimeScalar ()); new GraphQLQueryRequest ( ReviewsGraphQLQuery . newRequest (). dateRange ( new DateRange ( LocalDate . of ( 2020 , 1 , 1 ), LocalDate . now ())). build (), new ReviewsProjectionRoot (). submittedDate (). starScore (), scalars ); This way you can re-use exactly the same serialization code that you already have for your scalar implementation or one of the existing ones from - for example - the graphql-dgs-extended-scalars module.","title":"Scalars in DGS Client"},{"location":"advanced/java-client/#interface-projections","text":"When a field returns an interface, fields on the concrete types are specified using a fragment. type Query @extends { script(name: String): Script } interface Script { title: String director: String actors: [Actor] } type MovieScript implements Script { title: String director: String length: Int } type ShowScript implements Script { title: String director: String episodes: Int } query { script(name: \"Top Secret\") { title ... on MovieScript { length } } } This syntax is supported by the Query builder as well. GraphQLQueryRequest graphQLQueryRequest = new GraphQLQueryRequest ( new ScriptGraphQLQuery . Builder () . name ( \"Top Secret\" ) . build (), new ScriptProjectionRoot () . title () . onMovieScript () . length (); );","title":"Interface projections"},{"location":"advanced/java-client/#building-federated-queries","text":"You can use GraphQLQueryRequest along with EntitiesGraphQLQuery to generated federated queries. The API provides a type-safe way to construct the _entities query with the associated representations based on the input schema. The representations are passed in as a map of variables. Each representation class is generated based on the key fields defined on the entity in your schema, along with the __typename . The EntitiesProjectionRoot is used to select query fields on the specified type. For example, let us look at a schema that extends a Movie type: type Movie @key(fields: \"movieId\") @extends { movieId: Int @external script: MovieScript } type MovieScript { title: String director: String actors: [Actor] } type Actor { name: String gender: String age: Int } With client code generation, you will now have a MovieRepresentation containing the key field, i.e., movieId , and the __typename field already set to type Movie . Now you can add each representation to the EntitiesGraphQLQuery as a representations variable. You will also have a EntitiesProjectionRoot with onMovie() to select fields on Movie from. Finally, you put them all together as a GraphQLQueryRequest , which you serialize into the final query string. The map of representations variables is available via getVariables on the EntitiesGraphQLQuery . Here is an example for the schema shown earlier: EntitiesGraphQLQuery entitiesQuery = new EntitiesGraphQLQuery . Builder () . addRepresentationAsVariable ( MovieRepresentation . newBuilder (). movieId ( 1122 ). build () ) . build (); GraphQLQueryRequest request = new GraphQLQueryRequest ( entitiesQuery , new EntitiesProjectionRoot (). onMovie (). movieId (). script (). title () ); String query = request . serialize (); Map < String , Object > representations = entitiesQuery . getVariables ();","title":"Building Federated Queries"},{"location":"advanced/java-client/#subscriptions","text":"Subscriptions are supported through the ReactiveGraphQLClient interface. The interface has two implementations: WebSocketGraphQLClient : For subscriptions over WebSockets SSESubscriptionGraphQLClient : For subscriptions over Server Sent Events (SSE) Both implementations require the use of WebClient, and cannot be used with other HTTP clients (in contrast to the \"normal\" DGS client). The clients return a Flux of GraphQLResponse . Each GraphQLResponse represents a message pushed from the subscription, and contains data and errors . It also offers convenience methods to parse data using JsonPath. Java WebClient webClient = WebClient . create ( \"http://localhost:\" + port ); SSESubscriptionGraphQLClient client = new SSESubscriptionGraphQLClient ( \"/subscriptions\" , webClient ); Flux < GraphQLResponse > numbers = client . reactiveExecuteQuery ( \"subscription {numbers}\" , Collections . emptyMap ()); numbers . mapNotNull ( r -> r . extractValue ( \"data.numbers\" )) . log () . subscribe (); Kotlin val webClient = WebClient . create ( \"http://localhost: $ port \" ) val client = SSESubscriptionGraphQLClient ( \"/subscriptions\" , webClient ) val reactiveExecuteQuery = client . reactiveExecuteQuery ( \"subscription {numbers}\" , emptyMap ()) reactiveExecuteQuery . mapNotNull { r -> r . data [ \"numbers\" ] } . log () . subscribe () In case the connection fails to set up, either because of a connection error, or because of an invalid query, a WebClientResponseException will be thrown. Errors later on in the process will be errors in the stream. Don't forget to subscribe() to the stream, otherwise the connection doesn't get started! An example of using the client to write subscription integration tests is available here .","title":"Subscriptions"},{"location":"advanced/mocking/","text":"This guide is about how to provide mock data for data fetchers. There are two primary reasons to do so: Provide example data that UI teams can use while the data fetcher is under development. This is useful during schema design. Provide stable test data for UI teams to write their tests against. An argument can be made that this type of mock data should live in the UI code. It\u2019s for their tests after all. However, by pulling it into the DGS, the owners of the data can provide test data that can be used by many teams. The two approaches are also not mutually exclusive. [GraphQL] Mocking \u00b6 The library in the DGS framework supports: returning static data from mocks returning generated data for simple types (like String fields) The library is modular, so you can use it for a variety of workflows and use cases. The mocking framework is already part of the DGS framework. All you need to provide is one or more MockProvider implementations. MockProvider is an interface with a Map<String, Object> provide() method. Each key in the Map is a field in the [GraphQL] schema, which can be several levels deep. The value in the Map is whatever mock data you want to return for this key. Example \u00b6 Create a MockProvider that provides mock data for the hello field you created in the getting started tutorial : @Component public class HelloMockProvider implements MockProvider { @NotNull @Override public Map < String , Object > provide () { Map < String , Object > mock = new HashMap <> (); mock . put ( \"hello\" , \"Mocked hello response\" ); return mock ; } } If you run the application again and test the hello query, you will see that it now returns the mock data.","title":"Mocking"},{"location":"advanced/mocking/#graphql-mocking","text":"The library in the DGS framework supports: returning static data from mocks returning generated data for simple types (like String fields) The library is modular, so you can use it for a variety of workflows and use cases. The mocking framework is already part of the DGS framework. All you need to provide is one or more MockProvider implementations. MockProvider is an interface with a Map<String, Object> provide() method. Each key in the Map is a field in the [GraphQL] schema, which can be several levels deep. The value in the Map is whatever mock data you want to return for this key.","title":"[GraphQL] Mocking"},{"location":"advanced/mocking/#example","text":"Create a MockProvider that provides mock data for the hello field you created in the getting started tutorial : @Component public class HelloMockProvider implements MockProvider { @NotNull @Override public Map < String , Object > provide () { Map < String , Object > mock = new HashMap <> (); mock . put ( \"hello\" , \"Mocked hello response\" ); return mock ; } } If you run the application again and test the hello query, you will see that it now returns the mock data.","title":"Example"},{"location":"advanced/operation-caching/","text":"Before operations (queries, mutations, and subscriptions) can be executed, their request string needs to be parsed and validated . Performing these two steps can be expensive. The GraphQL Java library opens up a special PreparsedDocumentProvider interface which intercepts these two steps and allows library consumers to cache, or modify the resulting operation. The DGS Framework supports injecting a PreparsedDocumentProvider by defining a bean of the same type. The following example uses Caffeine to cache the 2500 most recent operations for a maximum of 1 hour. @Component // Resolved by Spring public class CachingPreparsedDocumentProvider implements PreparsedDocumentProvider { private final Cache < String , PreparsedDocumentEntry > cache = Caffeine . newBuilder () . maximumSize ( 2500 ) . expireAfterAccess ( Duration . ofHours ( 1 )) . build (); @Override public PreparsedDocumentEntry getDocument ( ExecutionInput executionInput , Function < ExecutionInput , PreparsedDocumentEntry > parseAndValidateFunction ) { return cache . get ( executionInput . getQuery (), operationString -> parseAndValidateFunction . apply ( executionInput )); } } The bean can also be injected using an annotated @Bean method: @Configuration public class MyDgsConfiguration { @Bean public public PreparsedDocumentEntry getDocument ( ExecutionInput executionInput , Function < ExecutionInput , PreparsedDocumentEntry > parseAndValidateFunction ) { return new CachingPreparsedDocumentProvider (); } } Using operation variables \u00b6 When using PreparsedDocumentProvider this way, it is important that you use operation variables in your operation. Otherwise, your cache may fill up with operations that are used only once, or contain personal information. This means that operations like the following: query DgsPersonQuery { person(id: \"123\") { id firstName } } Should be written as: query DgsPersonQuery($personId: String!) { person(id: $personId) { id firstName } } With the personId variable set to \"123\" in your specific client implementation.","title":"Operation Caching"},{"location":"advanced/operation-caching/#using-operation-variables","text":"When using PreparsedDocumentProvider this way, it is important that you use operation variables in your operation. Otherwise, your cache may fill up with operations that are used only once, or contain personal information. This means that operations like the following: query DgsPersonQuery { person(id: \"123\") { id firstName } } Should be written as: query DgsPersonQuery($personId: String!) { person(id: $personId) { id firstName } } With the personId variable set to \"123\" in your specific client implementation.","title":"Using operation variables"},{"location":"advanced/platform-bom/","text":"Using the Platform Bill of Materials (BOM) Both Gradle 1 and Maven 2 define a mechanism that developers can leverage to align the versions of dependencies that belong to the same framework, or an umbrella of dependencies that need to be aligned to work well together. Using them will prevent version conflicts and aide you figure out which dependency versions work well with each other. Let's go through a scenario, and assume you are using both the graphql-dgs-spring-boot-starter and the graphql-dgs-subscriptions-websockets-autoconfigure . Without using the platform/BOM you will have to define a version for each; unless the versions are explicitly maintained there is a chance that in the future they diverge. Manually aligning the versions of the dependencies becomes harder if your have a multi-module project where each module is using different dependencies of the DGS Framework, for example, the graphql-dgs-client . If you are using the platform/BOM you define the version of the DGS Framework in one place only , it will make sure that all other DGS Framework dependencies are using the same version. In the case of the DGS Framework we have two different BOM definitions, the graphql-dgs-platform-dependencies and the graphql-dgs-platform . The latter only defines version alignment for the DGS modules while the first also defines versions for the dependencies of the DGS framework, such as Spring, Jackson, and Kotlin. Using the Platform/BOM? \u00b6 Let's go through an example and assume that we want to use the DGS Framework 3.10.2... Gradle repositories { mavenCentral () } dependencies { // DGS BOM/platform dependency. This is the only place you set version of DGS implementation ( platform ( 'com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:3.10.2' )) // DGS dependencies. We don't have to specify a version here! implementation 'com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter' implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure' //Additional Jackson dependency. We don't need to specify a version, because Jackson is part of the BOM/platform definition. implementation 'com.fasterxml.jackson.datatype:jackson-datatype-joda' //Other dependencies... } Gradle Kotlin repositories { mavenCentral () } dependencies { //DGS BOM/platform dependency. This is the only place you set version of DGS implementation ( platform ( \"com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:3.10.2\" )) //DGS dependencies. We don't have to specify a version here! implementation ( \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter\" ) implementation ( \"com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure\" ) //Additional Jackson dependency. We don't need to specify a version, because Jackson is part of the BOM/platform definition. implementation ( \"com.fasterxml.jackson.datatype:jackson-datatype-joda\" ) //Other dependencies... } Maven <dependencyManagement> <dependencies> <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-platform-dependencies </artifactId> <!-- The DGS BOM/platform dependency. This is the only place you set version of DGS --> <version> 3.10.2 </version> <type> pom </type> <scope> import </scope> </dependency> </dependencies> </dependencyManagement> <dependencies> <!-- DGS dependencies. We don't have to specify a version here! --> <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-spring-boot-starter </artifactId> </dependency> <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-subscriptions-websockets-autoconfigure </artifactId> </dependency> <!-- Additional Jackson dependency. We don't need to specify a version, because Jackson is part of the BOM/platform definition. --> <dependency> <groupId> com.fasterxml.jackson.datatype </groupId> <artifactId> jackson-datatype-joda </artifactId> </dependency> <!-- Other dependencies --> </dependencies> Notice that the version is only specified on the platform dependency , and not on the graphql-dgs-spring-boot-starter and graphql-dgs-subscriptions-websockets-autoconfigure . The BOM will make sure that all DGS dependencies are aligned, in other words, using the same version. In addition, since we are using the graphql-dgs-platform-dependencies , we can use the DGS chosen version of some dependencies as well, such as Jackson. Note Versions in the platform are recommendations. The versions can be overridden by the user, or by other platforms you might be using (such as the Spring dependency-management plugin). Gradle supports this via the Java Platform , checkout the section that describes how to consume a Java platform . \u21a9 Maven supports this via the BOM . Note that the BOM will be consumed via the dependencyManagement block. \u21a9","title":"Using the Platform BOM"},{"location":"advanced/platform-bom/#using-the-platformbom","text":"Let's go through an example and assume that we want to use the DGS Framework 3.10.2... Gradle repositories { mavenCentral () } dependencies { // DGS BOM/platform dependency. This is the only place you set version of DGS implementation ( platform ( 'com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:3.10.2' )) // DGS dependencies. We don't have to specify a version here! implementation 'com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter' implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure' //Additional Jackson dependency. We don't need to specify a version, because Jackson is part of the BOM/platform definition. implementation 'com.fasterxml.jackson.datatype:jackson-datatype-joda' //Other dependencies... } Gradle Kotlin repositories { mavenCentral () } dependencies { //DGS BOM/platform dependency. This is the only place you set version of DGS implementation ( platform ( \"com.netflix.graphql.dgs:graphql-dgs-platform-dependencies:3.10.2\" )) //DGS dependencies. We don't have to specify a version here! implementation ( \"com.netflix.graphql.dgs:graphql-dgs-spring-boot-starter\" ) implementation ( \"com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure\" ) //Additional Jackson dependency. We don't need to specify a version, because Jackson is part of the BOM/platform definition. implementation ( \"com.fasterxml.jackson.datatype:jackson-datatype-joda\" ) //Other dependencies... } Maven <dependencyManagement> <dependencies> <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-platform-dependencies </artifactId> <!-- The DGS BOM/platform dependency. This is the only place you set version of DGS --> <version> 3.10.2 </version> <type> pom </type> <scope> import </scope> </dependency> </dependencies> </dependencyManagement> <dependencies> <!-- DGS dependencies. We don't have to specify a version here! --> <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-spring-boot-starter </artifactId> </dependency> <dependency> <groupId> com.netflix.graphql.dgs </groupId> <artifactId> graphql-dgs-subscriptions-websockets-autoconfigure </artifactId> </dependency> <!-- Additional Jackson dependency. We don't need to specify a version, because Jackson is part of the BOM/platform definition. --> <dependency> <groupId> com.fasterxml.jackson.datatype </groupId> <artifactId> jackson-datatype-joda </artifactId> </dependency> <!-- Other dependencies --> </dependencies> Notice that the version is only specified on the platform dependency , and not on the graphql-dgs-spring-boot-starter and graphql-dgs-subscriptions-websockets-autoconfigure . The BOM will make sure that all DGS dependencies are aligned, in other words, using the same version. In addition, since we are using the graphql-dgs-platform-dependencies , we can use the DGS chosen version of some dependencies as well, such as Jackson. Note Versions in the platform are recommendations. The versions can be overridden by the user, or by other platforms you might be using (such as the Spring dependency-management plugin). Gradle supports this via the Java Platform , checkout the section that describes how to consume a Java platform . \u21a9 Maven supports this via the BOM . Note that the BOM will be consumed via the dependencyManagement block. \u21a9","title":"Using the Platform/BOM?"},{"location":"advanced/relay-pagination/","text":"Relay Pagination \u00b6 The DGS framework supports dynamic generation of schema types for cursor based pagination based on the relay spec . When a type in the graphql schema is annotated with the @connection directive, the framework generates the corresponding Connection and Edge types, along with the common PageInfo . This avoids boilerplate code around defining related Connection and Edge types in the schema for every type that needs to be paginated. Note The @connection directive only works for DGSs that are not required to register the static schema file with an external service (since the relay types are dynamically generated). For example, in a federated architecture involving a gateway, some gateway implementations may or may not recognize the @connection directive when working with a static schema file. Set up \u00b6 To enable the use of @connection directive for generating the schema for pagination, add the following module to dependencies in your build.gradle: dependencies { implementation 'com.netflix.graphql.dgs:graphql-dgs-pagination' } Next, add the directive on the type you want to paginate. type Query { hello: MessageConnection } type Message @connection { name: String } Note that the @connection directive is defined automatically by the framework, so there is no need to add it to your schema file. This results in the following relay types dynamically generated and added to the schema: type MessageConnection { edges: [MessageEdge] pageInfo: PageInfo } type MessageEdge { node: Message cursor: String } type PageInfo { hasPreviousPage: Boolean! hasNextPage: Boolean! startCursor: String endCursor: String } You can now use the corresponding graphql.relay types for Connection<T> , Edge<T> and PageInfo to set up your datafetcher as shown here: @DgsData ( parentType = \"Query\" , field = \"hello\" ) public Connection < Message > hello ( DataFetchingEnvironment env ) { return new SimpleListConnection <> ( Collections . singletonList ( new Message ( \"This is a generated connection\" ))). get ( env ); } If your schema references a pagination type in a nested type, and you are using the code generation plugin, you will need some additional configuration, as described in the next section. Configuring Code Generation \u00b6 If you are using the DGS Codegen Plugin for generating your data model, you will need to also add a type mapping for the relay types. The code generation plugin does not process the @connection directive and therefore needs to be configured so the generated classes can refer to the mapped type. For example, generateJava{ ... typeMapping = [\"MessageConnection\": \"graphql.relay.SimpleListConnection<Message>\"] }","title":"Relay Pagination"},{"location":"advanced/relay-pagination/#relay-pagination","text":"The DGS framework supports dynamic generation of schema types for cursor based pagination based on the relay spec . When a type in the graphql schema is annotated with the @connection directive, the framework generates the corresponding Connection and Edge types, along with the common PageInfo . This avoids boilerplate code around defining related Connection and Edge types in the schema for every type that needs to be paginated. Note The @connection directive only works for DGSs that are not required to register the static schema file with an external service (since the relay types are dynamically generated). For example, in a federated architecture involving a gateway, some gateway implementations may or may not recognize the @connection directive when working with a static schema file.","title":"Relay Pagination"},{"location":"advanced/relay-pagination/#set-up","text":"To enable the use of @connection directive for generating the schema for pagination, add the following module to dependencies in your build.gradle: dependencies { implementation 'com.netflix.graphql.dgs:graphql-dgs-pagination' } Next, add the directive on the type you want to paginate. type Query { hello: MessageConnection } type Message @connection { name: String } Note that the @connection directive is defined automatically by the framework, so there is no need to add it to your schema file. This results in the following relay types dynamically generated and added to the schema: type MessageConnection { edges: [MessageEdge] pageInfo: PageInfo } type MessageEdge { node: Message cursor: String } type PageInfo { hasPreviousPage: Boolean! hasNextPage: Boolean! startCursor: String endCursor: String } You can now use the corresponding graphql.relay types for Connection<T> , Edge<T> and PageInfo to set up your datafetcher as shown here: @DgsData ( parentType = \"Query\" , field = \"hello\" ) public Connection < Message > hello ( DataFetchingEnvironment env ) { return new SimpleListConnection <> ( Collections . singletonList ( new Message ( \"This is a generated connection\" ))). get ( env ); } If your schema references a pagination type in a nested type, and you are using the code generation plugin, you will need some additional configuration, as described in the next section.","title":"Set up"},{"location":"advanced/relay-pagination/#configuring-code-generation","text":"If you are using the DGS Codegen Plugin for generating your data model, you will need to also add a type mapping for the relay types. The code generation plugin does not process the @connection directive and therefore needs to be configured so the generated classes can refer to the mapped type. For example, generateJava{ ... typeMapping = [\"MessageConnection\": \"graphql.relay.SimpleListConnection<Message>\"] }","title":"Configuring Code Generation"},{"location":"advanced/schema-reloading/","text":"The DGS framework is designed to work well with tools such as JRebel. In large Spring Boot codebases with many dependencies, it can take some time to restart the application during development. Waiting for the application to start can be disruptive to the development workflow. Enabling development mode for hot reloading \u00b6 Tools like JRebel allow for hot-reloading code. You make code changes compile, and without restarting the application, see the changes in the running application. Actively developing a DGS often includes making schema changes and wiring datafetchers. Some initialization needs to happen to pick up such changes. Out-of-the-box the DGS framework caches this initialization to be as efficient as possible in production, so the initialization only happens during startup. We can configure the DGS framework to run in development mode during development, which re-initializes the schema on each request. You can enable development mode in three ways: Set the dgs.reload configuration property to true (e.g. in application.yml ) Enable the laptop profile Implement your own ReloadIndicator bean to be fully in control over when to reload. This is useful when working with fully dynamic schemas .","title":"Hot reloading schemas"},{"location":"advanced/schema-reloading/#enabling-development-mode-for-hot-reloading","text":"Tools like JRebel allow for hot-reloading code. You make code changes compile, and without restarting the application, see the changes in the running application. Actively developing a DGS often includes making schema changes and wiring datafetchers. Some initialization needs to happen to pick up such changes. Out-of-the-box the DGS framework caches this initialization to be as efficient as possible in production, so the initialization only happens during startup. We can configure the DGS framework to run in development mode during development, which re-initializes the schema on each request. You can enable development mode in three ways: Set the dgs.reload configuration property to true (e.g. in application.yml ) Enable the laptop profile Implement your own ReloadIndicator bean to be fully in control over when to reload. This is useful when working with fully dynamic schemas .","title":"Enabling development mode for hot reloading"},{"location":"advanced/security/","text":"Fine-grained Access Control with @Secured \u00b6 The DGS Framework integrates with Spring Security using the well known @Secured annotation. Spring Security itself can be configured in many ways, which goes beyond the scope of this documentation. Once Spring Security is set up however, you can apply @Secured to your data fetchers, very similarly to how you apply it to a REST Controller in Spring MVC. @DgsComponent public class SecurityExampleFetchers { @DgsData ( parentType = \"Query\" , field = \"hello\" ) public String hello () { return \"Hello to everyone\" ; } @Secured ( \"admin\" ) @DgsData ( parentType = \"Query\" , field = \"secureGroup\" ) public String secureGroup () { return \"Hello to admins only\" ; } }","title":"Security"},{"location":"advanced/security/#fine-grained-access-control-with-secured","text":"The DGS Framework integrates with Spring Security using the well known @Secured annotation. Spring Security itself can be configured in many ways, which goes beyond the scope of this documentation. Once Spring Security is set up however, you can apply @Secured to your data fetchers, very similarly to how you apply it to a REST Controller in Spring MVC. @DgsComponent public class SecurityExampleFetchers { @DgsData ( parentType = \"Query\" , field = \"hello\" ) public String hello () { return \"Hello to everyone\" ; } @Secured ( \"admin\" ) @DgsData ( parentType = \"Query\" , field = \"secureGroup\" ) public String secureGroup () { return \"Hello to admins only\" ; } }","title":"Fine-grained Access Control with @Secured"},{"location":"advanced/subscriptions/","text":"GraphQL Subscriptions enable a client to receive updates for a query from the server over time. Pushing update notifications from the server is a good example. The DGS framework supports subscriptions out of the box. The Server Side Programming Model \u00b6 In the DGS framework a Subscription is implemented as a data fetcher with the @DgsSubscription annotation. The @DgsSubscription is just short-hand for @DgsData(parentType = \"Subscription\") . The difference with a normal data fetcher is that a subscription must return a org.reactivestreams.Publisher . import reactor.core.publisher.Flux ; import org.reactivestreams.Publisher ; @DgsSubscription public Publisher < Stock > stocks () { //Create a never-ending Flux that emits an item every second return Flux . interval ( Duration . ofSeconds ( 1 )). map ({ t -> Stock ( \"NFLX\" , 500 + t ) }) } The Publisher interface is from Reactive Streams. The Spring Framework comes with the Reactor library to work with Reactive Streams. A complete example can be found in SubscriptionDatafetcher.java . WebSockets \u00b6 The GraphQL specification doesn't specify a transport protocol. WebSockets are the most popular transport protocol however, and are supported by the DGS Framework. Apollo defines a sub-protocol , which is supported by client libraries and implemented by the DGS framework. To enable WebSockets support, add the following module to your build.gradle : implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure:latest.release' The subscription endpoint is on /subscriptions . Normal GraphQL queries can be sent to /graphql , while subscription requests go to /subscriptions . Apollo client supports WebSockets through a link . Typically, you want to configure Apollo Client with both an HTTP link and a WS link, and split between them based on the query type. A simple example of using the Apollo client can be found in the example project of the DGS Framework repository. Unit Testing Subscriptions \u00b6 Similar to a \"normal\" data fetcher test, you use the DgsQueryExecutor to execute a query. Just like a normal query, this results in a ExecutionResult . Instead of returning a result directly in the getData() method, a subscription query returns a Publisher . A Publisher can be asserted using the testing capabilities from Reactor. Each onNext of the Publisher is another ExecutionResult . This ExecutionResult contains the actual data! It might take a minute to wrap your head around the concept of this nested ExecutionResult , but it gives an excellent way to test Subscriptions, including corner cases. The following is a simple example of such a test. The example tests the stocks subscription from above. The stocks subscription produces a result every second, so the test uses VirtualTime to fast-forward time, without needing to wait in the test. Also note that the emitted ExecutionResult returns a Map<String, Object> , and not the Java type that your data fetcher returns. Use the Jackson Objectmapper to convert the map to a Java object. @SpringBootTest(classes = {DgsAutoConfiguration.class, SubscriptionDataFetcher.class}) class SubscriptionDataFetcherTest { @Autowired DgsQueryExecutor queryExecutor; ObjectMapper objectMapper = new ObjectMapper(); @Test void stocks() { ExecutionResult executionResult = queryExecutor.execute(\"subscription Stocks { stocks { name, price } }\"); Publisher<ExecutionResult> publisher = executionResult.getData(); VirtualTimeScheduler virtualTimeScheduler = VirtualTimeScheduler.create(); StepVerifier.withVirtualTime(() -> publisher, 3) .expectSubscription() .thenRequest(3) .assertNext(result -> assertThat(toStock(result).getPrice()).isEqualTo(500)) .assertNext(result -> assertThat(toStock(result).getPrice()).isEqualTo(501)) .assertNext(result -> assertThat(toStock(result).getPrice()).isEqualTo(502)) .thenCancel() .verify(); } private Stock toStock(ExecutionResult result) { Map<String, Object> data = result.getData(); return objectMapper.convertValue(data.get(\"stocks\"), Stock.class); } } In this example the subscription works in isolation; it just emits a result every second. In other scenarios a subscription could depend on something else happening in the system, such as the processing of a mutation. Such scenarios are easy to set up in a unit test, simply run multiple queries/mutations in your test to see it all work together. Notice that the unit tests really only test your code. It doesn't care about transport protocols. This is exactly what you need for your tests, because your tests should focus on testing your code, not the framework code. Integration testing subscriptions \u00b6 Although most subscription logic should be tested in unit tests, it can be useful to test end-to-end with a client. This can be achieved with the DGS client, and works well in a @SpringBootTest with a random port. The example below starts a subscription, and sends to mutations that should result in updates on the subscription. The example uses Websockets, but the same can be done for SSE. The code for this example can be found in the example project . @SpringBootTest ( webEnvironment = SpringBootTest . WebEnvironment . RANDOM_PORT ) public class ReviewSubscriptionIntegrationTest { @LocalServerPort private Integer port ; private WebSocketGraphQLClient webSocketGraphQLClient ; private MonoGraphQLClient graphQLClient ; private MonoRequestExecutor requestExecutor = ( url , headers , body ) -> WebClient . create ( url ) . post () . bodyValue ( body ) . headers ( consumer -> headers . forEach ( consumer :: addAll )) . exchangeToMono ( r -> r . bodyToMono ( String . class ). map ( responseBody -> new HttpResponse ( r . rawStatusCode (), responseBody , r . headers (). asHttpHeaders ()))); @BeforeEach public void setup () { webSocketGraphQLClient = new WebSocketGraphQLClient ( \"ws://localhost:\" + port + \"/subscriptions\" , new ReactorNettyWebSocketClient ()); graphQLClient = new DefaultGraphQLClient ( \"http://localhost:\" + port + \"/graphql\" ); } @Test public void testWebSocketSubscription () { GraphQLQueryRequest subscriptionRequest = new GraphQLQueryRequest ( ReviewAddedGraphQLQuery . newRequest (). showId ( 1 ). build (), new ReviewAddedProjectionRoot (). starScore () ); GraphQLQueryRequest addReviewMutation1 = new GraphQLQueryRequest ( AddReviewGraphQLQuery . newRequest (). review ( SubmittedReview . newBuilder (). showId ( 1 ). starScore ( 5 ). username ( \"DGS User\" ). build ()). build (), new AddReviewProjectionRoot (). starScore () ); GraphQLQueryRequest addReviewMutation2 = new GraphQLQueryRequest ( AddReviewGraphQLQuery . newRequest (). review ( SubmittedReview . newBuilder (). showId ( 1 ). starScore ( 3 ). username ( \"DGS User\" ). build ()). build (), new AddReviewProjectionRoot (). starScore () ); Flux < Integer > starScore = webSocketGraphQLClient . reactiveExecuteQuery ( subscriptionRequest . serialize (), Collections . emptyMap ()). map ( r -> r . extractValue ( \"reviewAdded.starScore\" )); StepVerifier . create ( starScore ) . thenAwait ( Duration . ofSeconds ( 1 )) //This await is necessary because of issue [#657](https://github.com/Netflix/dgs-framework/issues/657) . then (() -> { graphQLClient . reactiveExecuteQuery ( addReviewMutation1 . serialize (), Collections . emptyMap (), requestExecutor ). block (); }) . then (() -> graphQLClient . reactiveExecuteQuery ( addReviewMutation2 . serialize (), Collections . emptyMap (), requestExecutor ). block ()) . expectNext ( 5 ) . expectNext ( 3 ) . thenCancel () . verify (); } }","title":"Subscriptions"},{"location":"advanced/subscriptions/#the-server-side-programming-model","text":"In the DGS framework a Subscription is implemented as a data fetcher with the @DgsSubscription annotation. The @DgsSubscription is just short-hand for @DgsData(parentType = \"Subscription\") . The difference with a normal data fetcher is that a subscription must return a org.reactivestreams.Publisher . import reactor.core.publisher.Flux ; import org.reactivestreams.Publisher ; @DgsSubscription public Publisher < Stock > stocks () { //Create a never-ending Flux that emits an item every second return Flux . interval ( Duration . ofSeconds ( 1 )). map ({ t -> Stock ( \"NFLX\" , 500 + t ) }) } The Publisher interface is from Reactive Streams. The Spring Framework comes with the Reactor library to work with Reactive Streams. A complete example can be found in SubscriptionDatafetcher.java .","title":"The Server Side Programming Model"},{"location":"advanced/subscriptions/#websockets","text":"The GraphQL specification doesn't specify a transport protocol. WebSockets are the most popular transport protocol however, and are supported by the DGS Framework. Apollo defines a sub-protocol , which is supported by client libraries and implemented by the DGS framework. To enable WebSockets support, add the following module to your build.gradle : implementation 'com.netflix.graphql.dgs:graphql-dgs-subscriptions-websockets-autoconfigure:latest.release' The subscription endpoint is on /subscriptions . Normal GraphQL queries can be sent to /graphql , while subscription requests go to /subscriptions . Apollo client supports WebSockets through a link . Typically, you want to configure Apollo Client with both an HTTP link and a WS link, and split between them based on the query type. A simple example of using the Apollo client can be found in the example project of the DGS Framework repository.","title":"WebSockets"},{"location":"advanced/subscriptions/#unit-testing-subscriptions","text":"Similar to a \"normal\" data fetcher test, you use the DgsQueryExecutor to execute a query. Just like a normal query, this results in a ExecutionResult . Instead of returning a result directly in the getData() method, a subscription query returns a Publisher . A Publisher can be asserted using the testing capabilities from Reactor. Each onNext of the Publisher is another ExecutionResult . This ExecutionResult contains the actual data! It might take a minute to wrap your head around the concept of this nested ExecutionResult , but it gives an excellent way to test Subscriptions, including corner cases. The following is a simple example of such a test. The example tests the stocks subscription from above. The stocks subscription produces a result every second, so the test uses VirtualTime to fast-forward time, without needing to wait in the test. Also note that the emitted ExecutionResult returns a Map<String, Object> , and not the Java type that your data fetcher returns. Use the Jackson Objectmapper to convert the map to a Java object. @SpringBootTest(classes = {DgsAutoConfiguration.class, SubscriptionDataFetcher.class}) class SubscriptionDataFetcherTest { @Autowired DgsQueryExecutor queryExecutor; ObjectMapper objectMapper = new ObjectMapper(); @Test void stocks() { ExecutionResult executionResult = queryExecutor.execute(\"subscription Stocks { stocks { name, price } }\"); Publisher<ExecutionResult> publisher = executionResult.getData(); VirtualTimeScheduler virtualTimeScheduler = VirtualTimeScheduler.create(); StepVerifier.withVirtualTime(() -> publisher, 3) .expectSubscription() .thenRequest(3) .assertNext(result -> assertThat(toStock(result).getPrice()).isEqualTo(500)) .assertNext(result -> assertThat(toStock(result).getPrice()).isEqualTo(501)) .assertNext(result -> assertThat(toStock(result).getPrice()).isEqualTo(502)) .thenCancel() .verify(); } private Stock toStock(ExecutionResult result) { Map<String, Object> data = result.getData(); return objectMapper.convertValue(data.get(\"stocks\"), Stock.class); } } In this example the subscription works in isolation; it just emits a result every second. In other scenarios a subscription could depend on something else happening in the system, such as the processing of a mutation. Such scenarios are easy to set up in a unit test, simply run multiple queries/mutations in your test to see it all work together. Notice that the unit tests really only test your code. It doesn't care about transport protocols. This is exactly what you need for your tests, because your tests should focus on testing your code, not the framework code.","title":"Unit Testing Subscriptions"},{"location":"advanced/subscriptions/#integration-testing-subscriptions","text":"Although most subscription logic should be tested in unit tests, it can be useful to test end-to-end with a client. This can be achieved with the DGS client, and works well in a @SpringBootTest with a random port. The example below starts a subscription, and sends to mutations that should result in updates on the subscription. The example uses Websockets, but the same can be done for SSE. The code for this example can be found in the example project . @SpringBootTest ( webEnvironment = SpringBootTest . WebEnvironment . RANDOM_PORT ) public class ReviewSubscriptionIntegrationTest { @LocalServerPort private Integer port ; private WebSocketGraphQLClient webSocketGraphQLClient ; private MonoGraphQLClient graphQLClient ; private MonoRequestExecutor requestExecutor = ( url , headers , body ) -> WebClient . create ( url ) . post () . bodyValue ( body ) . headers ( consumer -> headers . forEach ( consumer :: addAll )) . exchangeToMono ( r -> r . bodyToMono ( String . class ). map ( responseBody -> new HttpResponse ( r . rawStatusCode (), responseBody , r . headers (). asHttpHeaders ()))); @BeforeEach public void setup () { webSocketGraphQLClient = new WebSocketGraphQLClient ( \"ws://localhost:\" + port + \"/subscriptions\" , new ReactorNettyWebSocketClient ()); graphQLClient = new DefaultGraphQLClient ( \"http://localhost:\" + port + \"/graphql\" ); } @Test public void testWebSocketSubscription () { GraphQLQueryRequest subscriptionRequest = new GraphQLQueryRequest ( ReviewAddedGraphQLQuery . newRequest (). showId ( 1 ). build (), new ReviewAddedProjectionRoot (). starScore () ); GraphQLQueryRequest addReviewMutation1 = new GraphQLQueryRequest ( AddReviewGraphQLQuery . newRequest (). review ( SubmittedReview . newBuilder (). showId ( 1 ). starScore ( 5 ). username ( \"DGS User\" ). build ()). build (), new AddReviewProjectionRoot (). starScore () ); GraphQLQueryRequest addReviewMutation2 = new GraphQLQueryRequest ( AddReviewGraphQLQuery . newRequest (). review ( SubmittedReview . newBuilder (). showId ( 1 ). starScore ( 3 ). username ( \"DGS User\" ). build ()). build (), new AddReviewProjectionRoot (). starScore () ); Flux < Integer > starScore = webSocketGraphQLClient . reactiveExecuteQuery ( subscriptionRequest . serialize (), Collections . emptyMap ()). map ( r -> r . extractValue ( \"reviewAdded.starScore\" )); StepVerifier . create ( starScore ) . thenAwait ( Duration . ofSeconds ( 1 )) //This await is necessary because of issue [#657](https://github.com/Netflix/dgs-framework/issues/657) . then (() -> { graphQLClient . reactiveExecuteQuery ( addReviewMutation1 . serialize (), Collections . emptyMap (), requestExecutor ). block (); }) . then (() -> graphQLClient . reactiveExecuteQuery ( addReviewMutation2 . serialize (), Collections . emptyMap (), requestExecutor ). block ()) . expectNext ( 5 ) . expectNext ( 3 ) . thenCancel () . verify (); } }","title":"Integration testing subscriptions"},{"location":"advanced/type-resolvers-for-abstract-types/","text":"You must register type resolvers whenever you use interface types or union types in your schema. Interface types and union types are explained in the GraphQL documentation . As an example, the following schema defines a Movie interface type with two different concrete object type implementations. type Query { movies: [Movie] } interface Movie { title: String } type ScaryMovie implements Movie { title: String gory: Boolean scareFactor: Int } type ActionMovie implements Movie { title: String nrOfExplosions: Int } The following data fetcher is registered to return a list of movies. The data fetcher returns a combination Movie types. @DgsComponent public class MovieDataFetcher { @DgsData ( parentType = \"Query\" , field = \"movies\" ) public List < Movie > movies () { return Lists . newArrayList ( new ActionMovie ( \"Crouching Tiger\" , 0 ), new ActionMovie ( \"Black hawk down\" , 10 ), new ScaryMovie ( \"American Horror Story\" , true , 10 ), new ScaryMovie ( \"Love Death + Robots\" , false , 4 ) ); } } The GraphQL runtime needs to know that a Java instance of ActionMovie represents the ActionMovie GraphQL type. This mapping is the responsibility of a TypeResolver . Tip: If your Java type names and GraphQL type names are the same, the DGS framework creates a `TypeResolver` automatically. No code needs to be added! Registering a Type Resolver \u00b6 If the name of your Java type and GraphQL type don't match, you need to provide a TypeResolver . A type resolver helps the framework map from concrete Java types to the correct object type in the schema. Use the @DgsTypeResolver annotation to register a type resolver. The annotation has a name property; set this to the name of the interface type or union type in the [GraphQL] schema. The resolver takes an object of the Java interface type, and returns a String which is the concrete object type of the instance as defined in the schema. The following is a type resolver for the Movie interface type introduced above: @DgsTypeResolver ( name = \"Movie\" ) public String resolveMovie ( Movie movie ) { if ( movie instanceof ScaryMovie ) { return \"ScaryMovie\" ; } else if ( movie instanceof ActionMovie ) { return \"ActionMovie\" ; } else { throw new RuntimeException ( \"Invalid type: \" + movie . getClass (). getName () + \" found in MovieTypeResolver\" ); } } You can add the @DgsTypeResolver annotation to any @DgsComponent class. This means you can either keep the type resolver in the same class as the data fetcher responsible for returning the data for this type, or you can create a separate class for it.","title":"Interfaces and Unions"},{"location":"advanced/type-resolvers-for-abstract-types/#registering-a-type-resolver","text":"If the name of your Java type and GraphQL type don't match, you need to provide a TypeResolver . A type resolver helps the framework map from concrete Java types to the correct object type in the schema. Use the @DgsTypeResolver annotation to register a type resolver. The annotation has a name property; set this to the name of the interface type or union type in the [GraphQL] schema. The resolver takes an object of the Java interface type, and returns a String which is the concrete object type of the instance as defined in the schema. The following is a type resolver for the Movie interface type introduced above: @DgsTypeResolver ( name = \"Movie\" ) public String resolveMovie ( Movie movie ) { if ( movie instanceof ScaryMovie ) { return \"ScaryMovie\" ; } else if ( movie instanceof ActionMovie ) { return \"ActionMovie\" ; } else { throw new RuntimeException ( \"Invalid type: \" + movie . getClass (). getName () + \" found in MovieTypeResolver\" ); } } You can add the @DgsTypeResolver annotation to any @DgsComponent class. This means you can either keep the type resolver in the same class as the data fetcher responsible for returning the data for this type, or you can create a separate class for it.","title":"Registering a Type Resolver"}]}